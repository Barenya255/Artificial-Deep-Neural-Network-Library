{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vlcWrCZGiPRM"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.datasets import fashion_mnist\n",
    "from tqdm import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OLGP4dlAQjyZ",
    "outputId": "5e331d30-d8a9-46fb-f77a-3cf210f38da3"
   },
   "outputs": [],
   "source": [
    "!pip install cupy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "TfO2B6v6BVWj"
   },
   "outputs": [],
   "source": [
    "class PreProc:\n",
    "    def __init__(self):\n",
    "        (self.trainx,self.trainy),(self.testx, self.testy) = fashion_mnist.load_data()\n",
    "    def visualize(self,n):\n",
    "        for i in range(n):\n",
    "            plt.subplot(330+1+i) # ask someone why??\n",
    "            plt.imshow(self.trainx[i], cmap = plt.get_cmap('gray'))\n",
    "        plt.show()\n",
    "    def flatten(self):\n",
    "        trainx_flattened = self.trainx\n",
    "        testx_flattened = self.testx\n",
    "        trainx_flattened.shape = (60000,784)\n",
    "        testx_flattened.shape = (10000,784)\n",
    "        return trainx_flattened,testx_flattened\n",
    "    def getLabels(self):\n",
    "        return self.trainy, self.testy\n",
    "    def getInputSize(self):\n",
    "        return len(self.trainx[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4HhzRi0xhOLc"
   },
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "7BwoVy0bPFA0"
   },
   "outputs": [],
   "source": [
    "class Functions:\n",
    "    @staticmethod\n",
    "    def sigmoid(input):\n",
    "        input = np.clip(input, -100,100)\n",
    "        return  1.0/(1.0+np.exp(-input))\n",
    "    @staticmethod\n",
    "    def softmax(input):\n",
    "        input = np.clip(input, -100,100)\n",
    "        return np.exp(input)/(np.sum(np.exp(input)))\n",
    "    @staticmethod\n",
    "    def onehot(input):\n",
    "        result = np.zeros(10)\n",
    "        result[input] = 1\n",
    "        return result\n",
    "    @staticmethod\n",
    "    def crossEntropyLoss(y,yHat):\n",
    "        loss = (-1/10.0) * np.sum(np.multiply(y, np.log(yHat+1e-10)) + np.multiply((1 - y), np.log(1 - (yHat+1e-10))))\n",
    "        return loss\n",
    "    @staticmethod\n",
    "    def derivative_sigmoid(input):\n",
    "        return Functions.sigmoid(input)*(1-Functions.sigmoid(input))\n",
    "    def plot(input):\n",
    "        plt.plot(input)\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Loss over iterations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Functions.sigmoid(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "Bu5XtsgmjyaH"
   },
   "outputs": [],
   "source": [
    "class Algorithms:\n",
    "    @staticmethod\n",
    "    def ForwardProp(net, activate, output, inputLayer):\n",
    "        L = len(net)-1\n",
    "        a = []\n",
    "        h = []\n",
    "        weights = net[0][:,:len(net[0][0])-1]\n",
    "        bias = net[0][:,len(net[0][0])-1]\n",
    "        temp = np.matmul(weights,inputLayer)+bias\n",
    "        a.append(temp)\n",
    "        h.append(activate(a[0]))\n",
    "        for k in range(1,L):\n",
    "            weights = net[k][:,:len(net[k][0])-1]\n",
    "            bias = net[k][:,len(net[k][0])-1]\n",
    "            temp = np.matmul(weights,h[k-1])+bias\n",
    "            a.append(bias + np.matmul(weights,h[k-1]))\n",
    "            h.append(activate(a[k]))\n",
    "        weights = net[L][:,:len(net[L][0])-1]\n",
    "        bias = net[L][:,len(net[L][0])-1]\n",
    "        temp = np.matmul(weights,h[L-1])+bias\n",
    "        a.append(temp)\n",
    "        h.append(output(a[L]))\n",
    "        return a,h\n",
    "    @staticmethod\n",
    "    def BackProp(net, a, h, dataPoint, dataLabel):\n",
    "        L = len(net)-1\n",
    "        gradaL = -(Functions.onehot(dataLabel)-h[len(h)-1])\n",
    "        gradient = np.zeros_like(net)\n",
    "        for k in range(L,0,-1):\n",
    "            gradW = np.outer(gradaL,h[k-1].T)\n",
    "            gradB = gradaL\n",
    "            gradB.resize((len(gradB),1))\n",
    "            grad = np.append(gradW,gradB,axis=1)\n",
    "            gradient[k] = grad\n",
    "\n",
    "            gradhL_1 = np.matmul(np.transpose(net[k][:,len(net[k])-1]),gradaL)\n",
    "            gradaL_1 = np.multiply(gradhL_1, Functions.derivative_sigmoid(a[k-1]))\n",
    "            gradaL = gradaL_1\n",
    "        gradW = np.outer(gradaL,dataPoint.T)\n",
    "        gradB = gradaL\n",
    "        gradB.resize((len(gradB),1))\n",
    "        grad = np.append(gradW,gradB,axis=1)\n",
    "        gradient[0] = grad\n",
    "        return gradient\n",
    "\n",
    "    @staticmethod\n",
    "    def mGD(net, learningRate, train, label, activate, output):\n",
    "        beta = 0.9\n",
    "        prevGrad = np.zeros_like(net)\n",
    "        for i in range(len(train)):\n",
    "            a,h = Algorithms.ForwardProp(net, activate, output, train[i])\n",
    "            gradient = Algorithms.BackProp(net, a, h, train[i], label[i])\n",
    "            gradient += beta*prevGrad + learningRate*gradient\n",
    "        gradient /= len(train)\n",
    "        net = net - gradient\n",
    "        return net\n",
    "\n",
    "    @staticmethod\n",
    "    def miniBatchMGD(net, batchSize, learningRate, dataPoints, dataLabels):\n",
    "        batchSize = 32\n",
    "        gradient = np.zeros_like(net)\n",
    "        lossTrack = []\n",
    "        beta = 0.9\n",
    "        prevGrad = np.zeros_like(net)\n",
    "        for epoch in tqdm(range(15)):\n",
    "            indices = np.arange(len(dataPoints))\n",
    "            np.random.shuffle(indices)\n",
    "            batchX = dataPoints[indices]\n",
    "            batchY = dataLabels[indices]\n",
    "            for i in range(math.ceil(len(dataPoints)/batchSize)):\n",
    "                trainer = batchX[i*batchSize:i*batchSize+batchSize]\n",
    "                labeler = batchY[i*batchSize:i*batchSize+batchSize]\n",
    "                batchLoss = 0.0\n",
    "                gradient = 0.0\n",
    "                for data in range(batchSize):\n",
    "                    a,h = Algorithms.ForwardProp(net, Functions.sigmoid, Functions.softmax, trainer[data])\n",
    "                    currGrad = Algorithms.BackProp(net, a, h, trainer[data], labeler[data])\n",
    "                    batchLoss += Functions.crossEntropyLoss(Functions.onehot(labeler[data]), h[-1])\n",
    "                    gradient += beta*prevGrad + (1-beta)*currGrad\n",
    "                batchLoss /= batchSize\n",
    "                gradient /= batchSize\n",
    "                prevGrad = gradient\n",
    "                net = net - learningRate*gradient \n",
    "                lossTrack.append(batchLoss)\n",
    "            print(\"The loss after this epoch is: \"+ str(batchLoss))\n",
    "        return net\n",
    "\n",
    "    @staticmethod\n",
    "    def miniBatchADAM(net, batchSize, learningRate, dataPoints, dataLabels):\n",
    "        batchSize = 32\n",
    "        epsilon = 1e-5\n",
    "        gradient = np.zeros_like(net)\n",
    "        lossTrack = []\n",
    "        beta2 = 0.9\n",
    "        beta1 = 0.9\n",
    "        prevGrad = np.zeros_like(net)\n",
    "        for epoch in tqdm(range(15)):\n",
    "            indices = np.arange(len(dataPoints))\n",
    "            np.random.shuffle(indices)\n",
    "            batchX = dataPoints[indices]\n",
    "            batchY = dataLabels[indices]\n",
    "            for i in range(math.ceil(len(dataPoints)/batchSize)):\n",
    "                trainer = batchX[i*batchSize:i*batchSize+batchSize]\n",
    "                labeler = batchY[i*batchSize:i*batchSize+batchSize]\n",
    "                batchLoss = 0.0\n",
    "                gradient = 0.0\n",
    "                moment1 = 0.0\n",
    "                moment2 = 0.0\n",
    "                for data in range(batchSize):\n",
    "                    a,h = Algorithms.ForwardProp(net, Functions.sigmoid, Functions.softmax, trainer[data])\n",
    "                    currGrad = Algorithms.BackProp(net, a, h, trainer[data], labeler[data])\n",
    "                    batchLoss += Functions.crossEntropyLoss(Functions.onehot(labeler[data]), h[-1])\n",
    "                    gradient += currGrad\n",
    "                moment1 = beta1*moment1 + (1-beta1)*gradient\n",
    "                moment2 = beta2*moment2 + (1-beta2)*gradient**2\n",
    "                moment1 = moment1/(1 - beta1**(epoch+1))\n",
    "                moment2 = moment2/(1 - beta2**(epoch+1))\n",
    "                denom = 0.0\n",
    "                for i in moment2:\n",
    "                    denom += np.linalg.norm(i)**2\n",
    "                temp = (learningRate/(np.sqrt(denom)+epsilon))*moment1\n",
    "                batchLoss /= batchSize\n",
    "                gradient /= batchSize\n",
    "                prevGrad = gradient\n",
    "                net = net - temp*gradient \n",
    "                lossTrack.append(batchLoss)\n",
    "            print(\"The loss after this epoch is: \"+ str(batchLoss))\n",
    "        return net\n",
    "\n",
    "    def miniBatchNAG(net, batchSize, learningRate, dataPoints, dataLabels):\n",
    "        batchSize = 32\n",
    "        gradient = np.zeros_like(net)\n",
    "        lossTrack = []\n",
    "        beta = 0.9\n",
    "        prevGrad = np.zeros_like(net)\n",
    "        for epoch in tqdm(range(15)):\n",
    "            indices = np.arange(len(dataPoints))\n",
    "            np.random.shuffle(indices)\n",
    "            batchX = dataPoints[indices]\n",
    "            batchY = dataLabels[indices]\n",
    "            for i in range(math.ceil(len(dataPoints)/batchSize)):\n",
    "                trainer = batchX[i*batchSize:i*batchSize+batchSize]\n",
    "                labeler = batchY[i*batchSize:i*batchSize+batchSize]\n",
    "                batchLoss = 0.0\n",
    "                gradient = 0.0\n",
    "                for data in range(batchSize):\n",
    "                    a,h = Algorithms.ForwardProp(net-prevGrad, Functions.sigmoid, Functions.softmax, trainer[data])\n",
    "                    currGrad = Algorithms.BackProp(net-prevGrad, a, h, trainer[data], labeler[data])\n",
    "                    batchLoss += Functions.crossEntropyLoss(Functions.onehot(labeler[data]), h[-1])\n",
    "                    gradient += beta*prevGrad + (1-beta)*currGrad\n",
    "                batchLoss /= batchSize\n",
    "                gradient /= batchSize\n",
    "                prevGrad = gradient\n",
    "                net = net - learningRate*gradient \n",
    "                lossTrack.append(batchLoss)\n",
    "            print(\"The loss after this epoch is: \"+ str(batchLoss))\n",
    "        return net\n",
    "    \n",
    "    @staticmethod\n",
    "    def RMSProp(net, batchSize, learningRate, dataPoints, dataLabels):\n",
    "        gradient = np.zeros_like(net, dtype = np.float32)\n",
    "        lossTrack = []\n",
    "        beta = 0.5\n",
    "        epsilon = 0.000001\n",
    "        learnerRate = np.full_like(net, learningRate)\n",
    "        prevGrad = np.zeros_like(net)\n",
    "        for epoch in tqdm(range(15)):\n",
    "            indices = np.arange(len(dataPoints))\n",
    "            np.random.shuffle(indices)\n",
    "            batchX = dataPoints[indices]\n",
    "            batchY = dataLabels[indices]\n",
    "            for i in range(math.ceil(len(dataPoints)/batchSize)):\n",
    "                trainer = batchX[i*batchSize:i*batchSize+batchSize]\n",
    "                labeler = batchY[i*batchSize:i*batchSize+batchSize]\n",
    "                batchLoss = 0.0\n",
    "                gradient = 0.0\n",
    "                v_t_1 = 0.0\n",
    "                for data in range(batchSize):\n",
    "                    a,h = Algorithms.ForwardProp(net, Functions.sigmoid, Functions.softmax, trainer[data])\n",
    "                    currGrad = Algorithms.BackProp(net, a, h, trainer[data], labeler[data])\n",
    "                    batchLoss += Functions.crossEntropyLoss(Functions.onehot(labeler[data]), h[-1])\n",
    "                    gradient += currGrad\n",
    "                batchLoss /= batchSize\n",
    "                gradient /= batchSize\n",
    "                prevGrad = gradient\n",
    "                v_t = v_t_1*beta + (1-beta)*gradient**2\n",
    "                v_t_1 = v_t\n",
    "                temp = np.zeros_like(v_t)\n",
    "                for i in range(len(v_t)):\n",
    "                    temp[i] = np.sqrt(v_t[i])\n",
    "                net = net - ((learnerRate)/(temp + epsilon))*gradient \n",
    "                lossTrack.append(batchLoss)\n",
    "            print(\"The loss after this epoch is: \"+ str(batchLoss))\n",
    "        return net\n",
    "\n",
    "    @staticmethod\n",
    "    def miniBatchGD(net, batchSize, learningRate, dataPoints, dataLabels):\n",
    "        batchSize = 32\n",
    "        gradient = np.zeros_like(net)\n",
    "        lossTrack = []\n",
    "        for epoch in tqdm(range(15)):\n",
    "            indices = np.arange(len(dataPoints))\n",
    "            np.random.shuffle(indices)\n",
    "            batchX = dataPoints[indices]\n",
    "            batchY = dataLabels[indices]\n",
    "            for i in range(math.ceil(len(dataPoints)/batchSize)):\n",
    "                trainer = batchX[i*batchSize:i*batchSize+batchSize]\n",
    "                labeler = batchY[i*batchSize:i*batchSize+batchSize]\n",
    "                batchLoss = 0.0\n",
    "                for data in range(batchSize):\n",
    "                    a,h = Algorithms.ForwardProp(net, Functions.sigmoid, Functions.softmax, trainer[data])\n",
    "                    currGrad = Algorithms.BackProp(net, a, h, trainer[data], labeler[data])\n",
    "                    batchLoss += Functions.crossEntropyLoss(Functions.onehot(labeler[data]), h[-1])\n",
    "                    gradient += currGrad\n",
    "                batchLoss /= batchSize\n",
    "                gradient /= batchSize\n",
    "                net = net - learningRate*gradient \n",
    "                lossTrack.append(batchLoss)\n",
    "            print(\"The loss after this epoch is: \"+ str(batchLoss))\n",
    "        return net\n",
    "    \n",
    "    @staticmethod\n",
    "    def adaGrad(net, batchSize, learningRate, dataPoints, dataLabels):\n",
    "        batchSize = 32\n",
    "        gradient = np.zeros_like(net)\n",
    "        lossTrack = []\n",
    "        for epoch in tqdm(range(15)):\n",
    "            indices = np.arange(len(dataPoints))\n",
    "            np.random.shuffle(indices)\n",
    "            batchX = dataPoints[indices]\n",
    "            batchY = dataLabels[indices]\n",
    "            for i in range(math.ceil(len(dataPoints)/batchSize)):\n",
    "                trainer = batchX[i*batchSize:i*batchSize+batchSize]\n",
    "                labeler = batchY[i*batchSize:i*batchSize+batchSize]\n",
    "                batchLoss = 0.0\n",
    "                v_t_1 = 0\n",
    "                for data in range(batchSize):\n",
    "                    a,h = Algorithms.ForwardProp(net, Functions.sigmoid, Functions.softmax, trainer[data])\n",
    "                    currGrad = Algorithms.BackProp(net, a, h, trainer[data], labeler[data])\n",
    "                    batchLoss += Functions.crossEntropyLoss(Functions.onehot(labeler[data]), h[-1])\n",
    "                    gradient += currGrad\n",
    "                batchLoss /= batchSize\n",
    "                gradient /= batchSize\n",
    "                v_t = v_t_1 + gradient**2\n",
    "                denom = 0.0\n",
    "                for val in v_t:\n",
    "                    denom += np.linalg.norm(val)\n",
    "                net = net - (learningRate)/(np.sqrt(denom)+0.00001)*gradient \n",
    "                lossTrack.append(batchLoss)\n",
    "            print(\"The loss after this epoch is: \"+ str(batchLoss))\n",
    "        return net\n",
    "    \n",
    "    @staticmethod\n",
    "    def evaluateNetwork(net,test_x, test_y):\n",
    "        num_acc = 0\n",
    "        for i in range(len(test_x)):\n",
    "            a,h = Algorithms.ForwardProp(net, Functions.sigmoid, Functions.softmax, test_x[i])\n",
    "            h = np.array(h, dtype = object)\n",
    "            predY =   np.argmax(h[len(h)-1])\n",
    "            if test_y[i] == predY:\n",
    "                num_acc+=1\n",
    "        print(num_acc/len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WsKHjO2hWrvW",
    "outputId": "3efc9ad6-da10-42df-bf76-4fb8dfc592d9"
   },
   "outputs": [],
   "source": [
    "len(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RYUFuUhfSjBG",
    "outputId": "f75413db-7810-4b53-d9a9-d230e43191c3"
   },
   "outputs": [],
   "source": [
    "for i in range(L,0,-1):\n",
    "    print(i, end = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qHfSx0jHSqtk",
    "outputId": "5e2dae53-035e-468c-98d7-d29b08d821f1"
   },
   "outputs": [],
   "source": [
    "#The class of FeedForwardNeuralNetwor\n",
    "\n",
    "class FFNet:\n",
    "    #constructor\n",
    "    def __init__(self,number_of_hidden_layers, number_of_inputs, number_of_outputs):\n",
    "        self.number_of_inputs = number_of_inputs\n",
    "        self.number_of_hidden_layers = number_of_hidden_layers\n",
    "        self.number_of_outputs = number_of_outputs\n",
    "        self.input = [0 for i in range(number_of_inputs)]\n",
    "        self.output = [0 for i in range(10)]\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        #self.hidden.append(np.random.random((number_of_inputs+1)))\n",
    "    \n",
    "    #Method for creating layers\n",
    "    def addHiddenLayer(self,number_of_neurons):\n",
    "        if(len(self.hidden) == 0):\n",
    "            temp_weights = np.random.random((number_of_neurons, self.number_of_inputs))\n",
    "            temp_biases = np.random.random((number_of_neurons))\n",
    "        else:\n",
    "            prev_neurons = len(self.hidden[len(self.hidden) - 1])\n",
    "            temp_weights = np.random.random((number_of_neurons, prev_neurons))\n",
    "            temp_biases = np.random.random((number_of_neurons))\n",
    "        temp_weights = temp_weights/np.linalg.norm(temp_weights)\n",
    "        temp_biases = temp_biases/np.linalg.norm(temp_biases)\n",
    "        self.weights.append(temp_weights)\n",
    "        self.biases.append(temp_biases)\n",
    "    \n",
    "    def addOutputLayer(self, number_of_outputs):\n",
    "        if(len(self.hidden) == 0):\n",
    "            #print(\"number of inputs: \"+str(self.number_of_inputs))\n",
    "            temp_weights = np.random.random((number_of_outputs, self.number_of_inputs))\n",
    "            temp_biases = np.random.random((number_of_outputs))\n",
    "        else:\n",
    "            prev_neurons = len(self.hidden[len(self.hidden) - 1])\n",
    "            temp_weights = temp_weights/np.linalg.norm(temp_weights)\n",
    "            temp_biases = np.random.random((number_of_outputs))\n",
    "        self.weights.append(temp_weights)\n",
    "        self.biases.append(temp_biases)\n",
    "\n",
    "    def solidify(self):\n",
    "        self.weights = np.array(self.weights, dtype = object)\n",
    "        self.biases = np.array(self.biases, dtype = object)\n",
    "\n",
    "    def getNetwork(self):\n",
    "        return self.network\n",
    "    \n",
    "    def ForwardProp(self, activate, output, inputLayer):\n",
    "        return Algorithms.ForwardProp(self.network, activate, output, inputLayer)\n",
    "    \n",
    "    def lossCalc(self, lossFunction, Y):\n",
    "        predY = self.historyA[(len(self.historyA)-1)]\n",
    "        return lossFunction(Y,self.predY)\n",
    "\n",
    "    def BackProp(self, a, h, dataPoint, dataLabel):\n",
    "        return Algorithms.BackProp(self.network, a, h, dataPoint, dataLabel)\n",
    "\n",
    "    def gradientDescent(self, learningRate, lossFunction, activate, output, dataPoints, dataLabels):\n",
    "        losses = []\n",
    "        for i in range(5):\n",
    "            gradient = 0.0\n",
    "            loss = 0\n",
    "            for index in tqdm(range(len(dataPoints))):\n",
    "                a,h = Algorithms.ForwardProp(self.network, activate, output, dataPoints[index])\n",
    "                predY = h[(len(h)-1)]\n",
    "\n",
    "                loss += Functions.crossEntropyLoss(dataLabels[index], predY)\n",
    "                gradient+= Algorithms.BackProp(self.network, a, h, dataPoints[index], dataLabels[index])\n",
    "            losses.append(loss)\n",
    "            #gradient /= 60000\n",
    "            self.network = self.network - learningRate * gradient\n",
    "        return self.network, losses\n",
    "    def stochGradientDescent(self, learningRate, lossFunction, activate, output, dataPoints, dataLabels):\n",
    "        #losses = []\n",
    "        net = self.network\n",
    "        for i in range(3):\n",
    "            for i in tqdm(range(len(dataPoints))):\n",
    "                a,h = Algorithms.ForwardProp(net,Functions.sigmoid, Functions.softmax, dataPoints[i])\n",
    "                gradient = Algorithms.BackProp(net, a,h,dataPoints[i], dataLabels[i])\n",
    "                net = net - 0.1*gradient\n",
    "        return self.network\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "FpxCgrhAinMc"
   },
   "outputs": [],
   "source": [
    "#The class of FeedForwardNeuralNetwor\n",
    "\n",
    "class FFNet:\n",
    "    #constructor\n",
    "    def __init__(self,number_of_hidden_layers, number_of_inputs, number_of_outputs):\n",
    "        self.number_of_inputs = number_of_inputs\n",
    "        self.number_of_hidden_layers = number_of_hidden_layers\n",
    "        self.number_of_outputs = number_of_outputs\n",
    "        self.input = [0 for i in range(number_of_inputs)]\n",
    "        self.output = [0 for i in range(10)]\n",
    "        self.hidden = []\n",
    "        #self.hidden.append(np.random.random((number_of_inputs+1)))\n",
    "    \n",
    "    #Method for creating layers\n",
    "    def addHiddenLayer(self,number_of_neurons):\n",
    "        if(len(self.hidden) == 0):\n",
    "            temp_weights = np.random.random((number_of_neurons, self.number_of_inputs+1)) #The +1 is for biases.\n",
    "        else:\n",
    "            prev_neurons = len(self.hidden[len(self.hidden) - 1])\n",
    "            temp_weights = np.random.random((number_of_neurons, prev_neurons + 1)) # The +1 is for biases.\n",
    "        temp_weights = temp_weights/np.linalg.norm(temp_weights)\n",
    "        self.hidden.append(temp_weights)\n",
    "    \n",
    "    def addOutputLayer(self, number_of_outputs):\n",
    "        if(len(self.hidden) == 0):\n",
    "            #print(\"number of inputs: \"+str(self.number_of_inputs))\n",
    "            temp_weights = np.random.random((number_of_outputs, self.number_of_inputs)) #Bias not needed for the output layer.\n",
    "        else:\n",
    "            prev_neurons = len(self.hidden[len(self.hidden) - 1])\n",
    "            temp_weights = np.random.random((number_of_outputs, prev_neurons + 1)) #Bias not needed for the output layer.\n",
    "        temp_weights = temp_weights/np.linalg.norm(temp_weights)\n",
    "        self.hidden.append(temp_weights)\n",
    "\n",
    "    def solidify(self):\n",
    "        self.network = np.array(self.hidden, dtype = object)\n",
    "\n",
    "    def getNetwork(self):\n",
    "        return self.network\n",
    "    \n",
    "    def ForwardProp(self, activate, output, inputLayer):\n",
    "        return Algorithms.ForwardProp(self.network, activate, output, inputLayer)\n",
    "    \n",
    "    def lossCalc(self, lossFunction, Y):\n",
    "        predY = self.historyA[(len(self.historyA)-1)]\n",
    "        return lossFunction(Y,self.predY)\n",
    "\n",
    "    def BackProp(self, a, h, dataPoint, dataLabel):\n",
    "        return Algorithms.BackProp(self.network, a, h, dataPoint, dataLabel)\n",
    "\n",
    "    def gradientDescent(self, learningRate, lossFunction, activate, output, dataPoints, dataLabels):\n",
    "        losses = []\n",
    "        for i in range(5):\n",
    "            gradient = 0.0\n",
    "            loss = 0\n",
    "            for index in tqdm(range(len(dataPoints))):\n",
    "                a,h = Algorithms.ForwardProp(self.network, activate, output, dataPoints[index])\n",
    "                predY = h[(len(h)-1)]\n",
    "\n",
    "                loss += Functions.crossEntropyLoss(dataLabels[index], predY)\n",
    "                gradient+= Algorithms.BackProp(self.network, a, h, dataPoints[index], dataLabels[index])\n",
    "            losses.append(loss)\n",
    "            #gradient /= 60000\n",
    "            self.network = self.network - learningRate * gradient\n",
    "        return self.network, losses\n",
    "    def stochGradientDescent(self, learningRate, lossFunction, activate, output, dataPoints, dataLabels):\n",
    "        #losses = []\n",
    "        net = self.network\n",
    "        for i in range(3):\n",
    "            for i in tqdm(range(len(dataPoints))):\n",
    "                a,h = Algorithms.ForwardProp(net,Functions.sigmoid, Functions.softmax, dataPoints[i])\n",
    "                gradient = Algorithms.BackProp(net, a,h,dataPoints[i], dataLabels[i])\n",
    "                net = net - 0.1*gradient\n",
    "        return self.network\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ew6hJr2qWgsd"
   },
   "outputs": [],
   "source": [
    "class Parallelizer:\n",
    "    @staticmethod\n",
    "    @cuda.jit\n",
    "    def entireDataSet(net, dataPoints, dataLabels, activate, output, learningRate, answer, losses):\n",
    "        index = cuda.grid()\n",
    "        a,h = ParallelAlgorithms.ForwardProp(net, activate, output, dataPoints[index])\n",
    "        predY = h[(len(h)-1)]\n",
    "\n",
    "        loss = Functions.crossEntropyLoss(dataLabels[index], predY)\n",
    "        losses[index] = (loss)\n",
    "        gradient = ParallelAlgorithms.BackProp(net, a, h, dataPoints[index], dataLabels[index])\n",
    "        cuda.atomic.add(answer,0,gradient,answer)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "wpFxAmhE9t2C",
    "outputId": "fdd02dd5-6147-4d7a-9d86-9cabebea2f81"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|██▊                                        | 1/15 [03:43<52:14, 223.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss after this epoch is: 0.32440024092462694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|█████▋                                     | 2/15 [04:14<23:50, 110.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss after this epoch is: 0.2904860150882165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|████████▊                                   | 3/15 [04:43<14:40, 73.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss after this epoch is: 0.26754650009257924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|███████████▋                                | 4/15 [05:14<10:21, 56.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss after this epoch is: 0.24244075090449865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|██████████████▋                             | 5/15 [05:44<07:48, 46.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss after this epoch is: 0.23071834449796919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|█████████████████▌                          | 6/15 [06:14<06:10, 41.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss after this epoch is: 0.21912279169502918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 47%|████████████████████▌                       | 7/15 [06:44<05:00, 37.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss after this epoch is: 0.21777234347610336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 53%|███████████████████████▍                    | 8/15 [07:15<04:07, 35.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss after this epoch is: 0.21668532485532507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████████████████████████▍                 | 9/15 [07:45<03:23, 33.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss after this epoch is: 0.19497692747464007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|████████████████████████████▋              | 10/15 [08:15<02:43, 32.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss after this epoch is: 0.18754285269982082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 73%|███████████████████████████████▌           | 11/15 [08:46<02:08, 32.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss after this epoch is: 0.19250813901429287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|██████████████████████████████████▍        | 12/15 [09:16<01:34, 31.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss after this epoch is: 0.1901191438153988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 87%|█████████████████████████████████████▎     | 13/15 [09:47<01:02, 31.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss after this epoch is: 0.17000313838231434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 93%|████████████████████████████████████████▏  | 14/15 [10:17<00:30, 30.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss after this epoch is: 0.17393587493114354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 15/15 [10:47<00:00, 43.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss after this epoch is: 0.1621468209220023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    data = PreProc()\n",
    "    #data.visualize(5)\n",
    "    train_x, test_x = data.flatten()\n",
    "    train_y, test_y = data.getLabels()\n",
    "    trainx = train_x/255\n",
    "    testx = test_x/255\n",
    "    neuralNet = FFNet(0,data.getInputSize(), 10)\n",
    "    neuralNet.addHiddenLayer(128)\n",
    "    neuralNet.addHiddenLayer(128)\n",
    "    neuralNet.addHiddenLayer(128)\n",
    "    neuralNet.addOutputLayer(10)\n",
    "    neuralNet.solidify()\n",
    "    net = neuralNet.getNetwork()\n",
    "    net = Algorithms.miniBatchGD(net, 100, 0.1, trainx, train_y)\n",
    "    #Algorithms.evaluateNetwork(net,testx,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Algorithms' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mAlgorithms\u001b[49m\u001b[38;5;241m.\u001b[39mBackProp\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Algorithms' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(v_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "R-YOAwnCmO3I",
    "outputId": "a052aca6-18cc-4d10-a3d6-17257b72ab4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5604\n"
     ]
    }
   ],
   "source": [
    "Algorithms.evaluateNetwork(net,testx,test_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cC3J4fScHacM"
   },
   "outputs": [],
   "source": [
    "a,h = Algorithms.ForwardProp(net, Functions.sigmoid, Functions.softmax, train_x[500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L1KBO1xmaxu8",
    "outputId": "aa351438-35e1-4e7e-99c3-1a86fae078b7"
   },
   "outputs": [],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 454
    },
    "id": "-DhPxwMbl6qX",
    "outputId": "5f0b4ef2-96a9-4a5b-f724-c8a1d521325e"
   },
   "outputs": [],
   "source": [
    "\n",
    "batchSize = 32\n",
    "gradient = np.zeros_like(net)\n",
    "lossTrack = []\n",
    "for epoch in tqdm(range(15)):\n",
    "    indices = np.arange(len(trainx))\n",
    "    np.random.shuffle(indices)\n",
    "    batchX = trainx[indices]\n",
    "    batchY = train_y[indices]\n",
    "    for i in range(math.ceil(len(trainx)/batchSize)):\n",
    "        trainer = batchX[i*batchSize:i*batchSize+batchSize]\n",
    "        labeler = batchY[i*batchSize:i*batchSize+batchSize]\n",
    "        batchLoss = 0.0\n",
    "        for data in range(batchSize):\n",
    "            a,h = Algorithms.ForwardProp(net, Functions.sigmoid, Functions.softmax, trainer[data])\n",
    "            currGrad = Algorithms.BackProp(net, a, h, trainer[data], labeler[data])\n",
    "            batchLoss += Functions.crossEntropyLoss(Functions.onehot(labeler[data]), h[-1])\n",
    "            gradient += currGrad\n",
    "        batchLoss /= 32\n",
    "        gradient /= 32\n",
    "        net = net - 0.01*gradient \n",
    "        lossTrack.append(batchLoss)\n",
    "    print(\"The loss after this epoch is: \"+ str(batchLoss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-hhR7Gex1BK9",
    "outputId": "57a17ae8-adef-4561-8bbb-a0206fa23106"
   },
   "outputs": [],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uckwQu3vbr9b"
   },
   "outputs": [],
   "source": [
    "gradient = neuralNet.BackProp(a,h,train_x[1], train_y[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "trXRsTlNb0_1"
   },
   "outputs": [],
   "source": [
    "net = net - gradient*0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SeXnBQQMbw_U",
    "outputId": "efc6c2b2-af74-4a82-f9e9-3c23fd302cd6"
   },
   "outputs": [],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CvzSP-M4btcN",
    "outputId": "001904ed-f2ec-4f76-b167-2678d1f33835"
   },
   "outputs": [],
   "source": [
    "np.argmax(np.array(h[L]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RcwywNK0blOr",
    "outputId": "d764dc8a-d727-443b-cb91-ba121fe78f26"
   },
   "outputs": [],
   "source": [
    "while np.argmax(np.array(h[L])):\n",
    "    for i in range(10):\n",
    "        a,h = Algorithms.ForwardProp(net,Functions.sigmoid, Functions.softmax, train_x[4])\n",
    "        gradient = neuralNet.BackProp(a,h,train_x[4], train_y[4])\n",
    "        net = net - 0.1*gradient\n",
    "        print(np.argmax(np.array(h[L])), end = \", \"),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-df0oArs006R",
    "outputId": "4218ed61-af21-4c62-cdac-3b2ec63efbe8"
   },
   "outputs": [],
   "source": [
    "train_y[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V0eqMHkIz3to",
    "outputId": "4a35e5fc-75b2-4bb2-de9a-8d103c5bb6c9"
   },
   "outputs": [],
   "source": [
    "num_acc = 0\n",
    "for i in range(len(test_x)):\n",
    "    a,h = Algorithms.ForwardProp(net, Functions.sigmoid, Functions.softmax, test_x[i])\n",
    "    h = np.array(h)\n",
    "    predY =   np.argmax(h[len(h)-1])\n",
    "    print(predY)\n",
    "    if test_y[i] == predY:\n",
    "        num_acc+=1\n",
    "print(num_acc/len(test_y), end = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o84TF_u3yVv9",
    "outputId": "074c498e-5519-4f15-dc86-049be1c6382a"
   },
   "outputs": [],
   "source": [
    "len(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TUcuGN-SzSyq",
    "outputId": "f6614617-7859-48d3-dff7-623f7622625c"
   },
   "outputs": [],
   "source": [
    "gradient[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OY0Zy9psDtk7"
   },
   "outputs": [],
   "source": [
    "for i in gradient[0]:\n",
    "    print(i, end = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I3YdLFqZU8Ci",
    "outputId": "52462d9f-b600-46f9-9403-a93bdb09c341"
   },
   "outputs": [],
   "source": [
    "gradient[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kPDcvGye0lF7"
   },
   "outputs": [],
   "source": [
    "gradaL = -(Functions.onehot(train_y[1])-h[len(h)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PcCtxLxzFKVG"
   },
   "outputs": [],
   "source": [
    "gradhL_1 = np.matmul(np.transpose(net[(len(net)-1)]),aL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W9xEUE6XKesg"
   },
   "outputs": [],
   "source": [
    "gradaL_1 = np.multiply(net[len(net)-1][:,:len(net[len(net)-1][0])-1], Functions.derivative_sigmoid(a[len(net)-2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K72jzJOiJSR3"
   },
   "outputs": [],
   "source": [
    "gradW = np.outer(gradaL,h[len(net)-2].T)\n",
    "gradB = gradaL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XDbZT58hUOWW"
   },
   "outputs": [],
   "source": [
    "gradB.resize((len(gradB),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L3-e7uWUUYd_",
    "outputId": "2dccdc5f-1f4f-4de9-ef20-bdc5f9e31910"
   },
   "outputs": [],
   "source": [
    "gradB.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WdZHmwrRR2rx",
    "outputId": "df26a5dc-df87-4147-c06a-0d3df7c1d70d"
   },
   "outputs": [],
   "source": [
    "np.append(gradW,gradB.resize((10,1)),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RDIXmbKHNo25",
    "outputId": "6d44560f-8280-4146-b7bd-dedf9486be76"
   },
   "outputs": [],
   "source": [
    "gradaL.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d7_71o1X06hg",
    "outputId": "5899546a-1304-47a4-e96b-f0f8edbce0ee"
   },
   "outputs": [],
   "source": [
    "a[len(net)-2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k65CaJIefUTY"
   },
   "outputs": [],
   "source": [
    "weights = net[0][:,:len(net[0][0])-1]\n",
    "bias = net[0][:,len(net[0][0])-1]\n",
    "temp = np.matmul(weights,train_x[0])+bias\n",
    "temp = temp/np.linalg.norm(temp)\n",
    "a = []\n",
    "a.append(temp)\n",
    "h = []\n",
    "h.append(Functions.sigmoid(a[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZafDhQ7LmcUx"
   },
   "outputs": [],
   "source": [
    "weights = net[L][:,:len(net[L][0])-1]\n",
    "bias = net[L][:,len(net[L][0])-1]\n",
    "temp = np.matmul(weights,h[0])+bias\n",
    "temp = temp/np.linalg.norm(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "epu4dvEji2mG"
   },
   "outputs": [],
   "source": [
    "L = len(net)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tg0dnhLfheDu",
    "outputId": "93e6fdc7-9cf7-4236-b55e-550c4459f101"
   },
   "outputs": [],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qohv21-jxL88"
   },
   "outputs": [],
   "source": [
    "#The class of FeedForwardNeuralNetwor\n",
    "\n",
    "class FFNet:\n",
    "    #constructor\n",
    "    hidden = []\n",
    "    input = []\n",
    "    output = []\n",
    "    def __init__(self,number_of_hidden_layers, number_of_inputs, number_of_outputs):\n",
    "        self.number_of_inputs = number_of_inputs\n",
    "        self.number_of_hidden_layers = number_of_hidden_layers\n",
    "        self.number_of_outputs = number_of_outputs\n",
    "        #At the same time, the layers input layers mus also be initialized.\n",
    "\n",
    "        input = [0 for i in range(number_of_inputs)]\n",
    "        output = [0 for i in range(number_of_outputs)]\n",
    "        hidden = [[]]\n",
    "\n",
    "        #input and output layers are nothing but simple lists\n",
    "    \n",
    "    #Method for creating layers\n",
    "    def add_hidden_layer(number_of_neurons):\n",
    "        temp_weights = [0 for i in range(number_of_neurons+1)] #The +1 is for bias values\n",
    "        hidden.append(temp_weights)\n",
    "    \n",
    "    def backward_propagate(a,h, pred_y):\n",
    "        delthet[L] = -(exp(y) - pred_y) #with respect to output layer\n",
    "        for k in range(0,L-1,-1):\n",
    "            delthetw = np.matmul(delthet[k], h[k-1].T)\n",
    "            delthetb = delthet[k]\n",
    "            deltheth = np.matmul(weights[k].T, delthet[k])\n",
    "            delthet[k-1] = hadamard(deltheth, preac(a)) \n",
    "\n",
    "    def forward_propagate():\n",
    "        #here, we are calculating the preactivations and activations.\n",
    "        #we then store them in an array and return it.\n",
    "        \n",
    "        for k in range(number_of_levels-1):\n",
    "            a[k] = biases[k] + np.matmul(weights[k], h[k-1])\n",
    "            h[k] = g(a[k])\n",
    "        a[number_of_levels-1] = biases[number_of_levels] + np.matmul(weights[number_of_levels],h[number_of_levels-1])\n",
    "        pred_y = output(a[number_of_levels-1])\n",
    "        return a,h, pred_y\n",
    "\n",
    "\n",
    "    def gradient_descent():\n",
    "        a,h, pred_y = forward_propagate()\n",
    "        delthet = backward_propagate(a,h, pred_y)\n",
    "        thet += delthet\n",
    "\n",
    "    def fit(dataset):\n",
    "        for x,y in dataset:\n",
    "            loss = forward(x,y)\n",
    "            delthet = backward(loss)\n",
    "            thet += learn_rate*delthet\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
