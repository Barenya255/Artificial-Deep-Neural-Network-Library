{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "vlcWrCZGiPRM"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.datasets import fashion_mnist\n",
    "from tqdm import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OLGP4dlAQjyZ",
    "outputId": "5e331d30-d8a9-46fb-f77a-3cf210f38da3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'bayes'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = {\n",
    "    'name': 'valAcc',\n",
    "    'goal': 'maximize'   \n",
    "    }\n",
    "\n",
    "sweep_config['metric'] = metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_dict = {\n",
    "    'optimizer': {\n",
    "        'values': ['nadam', 'momentum', 'nag', 'rmsprop', 'adam', 'sgd']\n",
    "        },\n",
    "    'fc_layer_size': {\n",
    "        'values': [32, 64, 128]\n",
    "        },\n",
    "    'number_of_layers': {\n",
    "        'values' : [3,4,5]\n",
    "        },\n",
    "    'epochs':{\n",
    "        'values' : [5,10]\n",
    "        },\n",
    "    'decay' : {\n",
    "        'values' : [0 ,0.0005, 0.5]\n",
    "        },\n",
    "    'learningRate' : {\n",
    "        'values' : [1e-3, 1e-4]\n",
    "        },\n",
    "    'batchSize' : {\n",
    "        'values' : [16, 32, 64]\n",
    "        },\n",
    "    'initialization' : {\n",
    "        'values' : ['random', 'Xavier']\n",
    "        },\n",
    "    'activation' : {\n",
    "        'values' : ['sigmoid', 'tanh', 'relu']\n",
    "        },\n",
    "    }\n",
    "\n",
    "sweep_config['parameters'] = parameters_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'method': 'bayes',\n",
      " 'metric': {'goal': 'maximize', 'name': 'valAcc'},\n",
      " 'parameters': {'activation': {'values': ['sigmoid', 'tanh', 'relu']},\n",
      "                'batchSize': {'values': [16, 32, 64]},\n",
      "                'decay': {'values': [0, 0.0005, 0.5]},\n",
      "                'epochs': {'values': [5, 10]},\n",
      "                'fc_layer_size': {'values': [32, 64, 128]},\n",
      "                'initialization': {'values': ['random', 'Xavier']},\n",
      "                'learningRate': {'values': [0.001, 0.0001]},\n",
      "                'number_of_layers': {'values': [3, 4, 5]},\n",
      "                'optimizer': {'values': ['nadam',\n",
      "                                         'momentum',\n",
      "                                         'nag',\n",
      "                                         'rmsprop',\n",
      "                                         'adam',\n",
      "                                         'sgd']}}}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "pprint.pprint(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: uzzqn91g\n",
      "Sweep URL: https://wandb.ai/cs22m028/test2/sweeps/uzzqn91g\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"test2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config = None):\n",
    "    \n",
    "    with wandb.init(config = config):\n",
    "        #pprint.pprint(sweep_config)        \n",
    "        config = wandb.config\n",
    "        neuralNet = FFNet(0, len(trainx[0]), 10)\n",
    "        for layer in range(config.number_of_layers):\n",
    "            neuralNet.addHiddenLayer(config.fc_layer_size, config.initialization)\n",
    "        neuralNet.addOutputLayer(10, config.initialization)\n",
    "        neuralNet.solidify()\n",
    "        weights, biases = neuralNet.fit(config.optimizer,config.batchSize, config.learningRate, config.activation, trainx, train_y, config.decay, config.epochs)\n",
    "        print(Algorithms.evaluateNetwork(weights,biases,testx, test_y))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkwAAAHFCAYAAAAAM6ZOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy88F64QAAAACXBIWXMAAA9hAAAPYQGoP6dpAABy9UlEQVR4nO3dd3gU1d4H8O8mIRsIJFITSgiRFnoJJQEREAkgoKhILmAAhRci5cLleq9EUMq9GisiCijqJSJSRBBQUQgWetGQUAQRaUFICDUJLYHkvH8M2WyfLbM7u9nv53n22d2ZM2fOzO7O/PacM2c0QggBIiIiIrLIT+0CEBEREXk6BkxEREREMhgwEREREclgwEREREQkgwETERERkQwGTEREREQyGDARERERyWDARERERCSDARMRERGRDAZMRF4mNTUVGo0Gv/76q9pFKRdOnz4NjUaD1NRU3bRdu3Zh1qxZuHbtmmrlkitHjx490KNHD7eXichXMWAiIp9Wu3Zt7N69G/3799dN27VrF2bPnu0RAZOlcixcuBALFy50f6GIfFSA2gUgInK1W7duISgoCBqNxmSeVqtFbGysW8px8+ZNVKpUSZG8mjdvrkg+RGQb1jARlVM7duxAr169UKVKFVSqVAldunTBt99+a5Dm5s2beP755xEVFYWgoCBUq1YNHTp0wIoVK3RpTp48ib/97W+oU6cOtFotwsLC0KtXL2RmZsqWYcOGDYiLi0OlSpVQpUoV9O7dG7t379bNX7duHTQaDX744QeTZRctWgSNRoODBw/qpv3666949NFHUa1aNQQFBaFdu3b44osvDJYrbbLcvHkznn32WdSsWROVKlVCYWGh2TIaN8nNmjUL//rXvwAAUVFR0Gg00Gg0+Pnnn3XLrFq1CnFxcQgODkblypXRp08fZGRkGOQ7atQoVK5cGYcOHUJ8fDyqVKmCXr16AQDS0tLw2GOPoV69eggKCkKjRo0wbtw4XLp0Sbe8XDnMNclduXIF48ePR926dREYGIj7778f06dPN9l2jUaDiRMn4rPPPkOzZs1QqVIltGnTBt98841BuosXL2Ls2LGIiIiAVqtFzZo10bVrV2zZssXsviQqz1jDRFQObd26Fb1790br1q3xySefQKvVYuHChRg4cCBWrFiBhIQEAMDUqVPx2Wef4b///S/atWuHGzdu4PDhw7h8+bIur0ceeQTFxcV44403UL9+fVy6dAm7du2Sba5avnw5hg8fjvj4eKxYsQKFhYV444030KNHD/zwww944IEHMGDAANSqVQtLlizRBROlUlNT0b59e7Ru3RoA8NNPP6Fv377o3LkzPvjgA4SGhmLlypVISEjAzZs3MWrUKIPln332WfTv3x+fffYZbty4gQoVKti078aMGYMrV67gvffew9q1a1G7dm0AZTU6r776KmbMmIFnnnkGM2bMQFFREd58801069YN+/btM6j5KSoqwqOPPopx48Zh2rRpuHv3LgDgxIkTiIuLw5gxYxAaGorTp09j7ty5eOCBB3Do0CFUqFBBthzGbt++jZ49e+LEiROYPXs2Wrduje3btyMlJQWZmZkmwfK3336LX375BXPmzEHlypXxxhtv4PHHH8exY8dw//33AwASExOxf/9+vPLKK2jSpAmuXbuG/fv3G3w/iHyGICKvsmTJEgFA/PLLLxbTxMbGilq1aomCggLdtLt374qWLVuKevXqiZKSEiGEEC1bthSDBg2ymM+lS5cEADFv3jy7ylhcXCzq1KkjWrVqJYqLi3XTCwoKRK1atUSXLl1006ZOnSoqVqworl27ppt25MgRAUC89957umnR0dGiXbt24s6dOwbrGjBggKhdu7ZuPaX7Z8SIETaV9dSpUwKAWLJkiW7am2++KQCIU6dOGaTNysoSAQEBYtKkSQbTCwoKRHh4uBgyZIhu2siRIwUA8b///c/q+ktKSsSdO3fEmTNnBACxfv162XIIIUT37t1F9+7dde8/+OADAUB88cUXBulef/11AUBs3rxZNw2ACAsLE/n5+bppOTk5ws/PT6SkpOimVa5cWUyZMsVq+Yl8BZvkiMqZGzduYO/evRg8eDAqV66sm+7v74/ExET89ddfOHbsGACgU6dO+O677zBt2jT8/PPPuHXrlkFe1apVQ8OGDfHmm29i7ty5yMjIQElJiWwZjh07hvPnzyMxMRF+fmWHmcqVK+PJJ5/Enj17cPPmTQBSTdCtW7ewatUqXbolS5ZAq9Vi2LBhAIA///wTv//+O4YPHw4AuHv3ru7xyCOPIDs7W7dNpZ588kl7dptNNm3ahLt372LEiBEGZQgKCkL37t0Nmu2slSM3NxdJSUmIiIhAQEAAKlSogMjISADA0aNHHSrbjz/+iODgYAwePNhgemnNm3GzZ8+ePVGlShXd+7CwMNSqVQtnzpzRTevUqRNSU1Px3//+F3v27MGdO3ccKhtRecCAiaicuXr1KoQQuiYcfXXq1AEAXZPK/Pnz8cILL2DdunXo2bMnqlWrhkGDBuH48eMAoOtf1KdPH7zxxhto3749atasib///e8oKCiwWIbS/C2VoaSkBFevXgUAtGjRAh07dsSSJUsAAMXFxVi2bBkee+wxVKtWDQBw4cIFAMDzzz+PChUqGDzGjx8PAAb9fyyt21ml5ejYsaNJOVatWmVShkqVKiEkJMRgWklJCeLj47F27Vr8+9//xg8//IB9+/Zhz549AGAStNrq8uXLCA8PN+nYXqtWLQQEBJg0o1WvXt0kD61Wa7D+VatWYeTIkfj4448RFxeHatWqYcSIEcjJyXGojETejH2YiMqZqlWrws/PD9nZ2Sbzzp8/DwCoUaMGACA4OBizZ8/G7NmzceHCBV1t08CBA/H7778DACIjI/HJJ58AAP744w988cUXmDVrFoqKivDBBx+YLUPpydhSGfz8/FC1alXdtGeeeQbjx4/H0aNHcfLkSWRnZ+OZZ57RzS8tb3JyMp544gmz62zatKnBe3NXxDmrtBxffvmlrkbIGnNlOHz4MA4cOIDU1FSMHDlSN/3PP/90qmzVq1fH3r17IYQwWG9ubi7u3r2rK7s9atSogXnz5mHevHnIysrChg0bMG3aNOTm5uL77793qrxE3oY1TETlTHBwMDp37oy1a9ca1BaUlJRg2bJlqFevHpo0aWKyXFhYGEaNGoWhQ4fi2LFjuiYzfU2aNMGMGTPQqlUr7N+/32IZmjZtirp162L58uUQQuim37hxA2vWrNFdOVdq6NChCAoKQmpqKlJTU1G3bl3Ex8cb5Ne4cWMcOHAAHTp0MPvQb15yllarBWBa29OnTx8EBATgxIkTFsshpzSYKV1HqQ8//NDmcpjTq1cvXL9+HevWrTOYvnTpUt18Z9SvXx8TJ05E7969rX72ROUVa5iIvNSPP/6I06dPm0x/5JFHkJKSgt69e6Nnz554/vnnERgYiIULF+Lw4cNYsWKF7qTduXNnDBgwAK1bt0bVqlVx9OhRfPbZZ7qA5uDBg5g4cSKeeuopNG7cGIGBgfjxxx9x8OBBTJs2zWLZ/Pz88MYbb2D48OEYMGAAxo0bh8LCQrz55pu4du0aXnvtNYP09913Hx5//HGkpqbi2rVreP755w36PgFSQNGvXz/06dMHo0aNQt26dXHlyhUcPXoU+/fvx+rVq53fqfe0atUKAPDuu+9i5MiRqFChApo2bYoGDRpgzpw5mD59Ok6ePIm+ffuiatWquHDhAvbt26ersbMmOjoaDRs2xLRp0yCEQLVq1fD1118jLS3N5nKYCw5HjBiBBQsWYOTIkTh9+jRatWqFHTt24NVXX8UjjzyChx9+2K59kJeXh549e2LYsGGIjo5GlSpV8Msvv+D777+3WMtHVK6p2+eciOxVehWYpUfpFVXbt28XDz30kAgODhYVK1YUsbGx4uuvvzbIa9q0aaJDhw6iatWqQqvVivvvv1/84x//EJcuXRJCCHHhwgUxatQoER0dLYKDg0XlypVF69atxTvvvCPu3r0rW9Z169aJzp07i6CgIBEcHCx69eoldu7caTbt5s2bddvwxx9/mE1z4MABMWTIEFGrVi1RoUIFER4eLh566CHxwQcfmOwfa1cR6jN3lZwQQiQnJ4s6deoIPz8/AUD89NNPBtvVs2dPERISIrRarYiMjBSDBw8WW7Zs0aUZOXKkCA4ONrvOI0eOiN69e4sqVaqIqlWriqeeekpkZWUJAGLmzJk2lcP4KjkhhLh8+bJISkoStWvXFgEBASIyMlIkJyeL27dvG6QDICZMmGBSrsjISDFy5EghhBC3b98WSUlJonXr1iIkJERUrFhRNG3aVMycOVPcuHHD8g4lKqc0QujVlxMRERGRCfZhIiIiIpLBgImIiIhIBgMmIiIiIhkMmIiIiIhkMGAiIiIiksGAiYiIiEgGB640o6SkBOfPn0eVKlVccnsFIiIiUp4QAgUFBahTp47J4LfOYsBkxvnz5xEREaF2MYiIiMgBZ8+eRb169RTNkwGTGaW3HTh79qzJncaJiIjIM+Xn5yMiIkLRe0uWYsBkRmkzXEhICAMmIiIiL+OK7jTs9E1EREQkgwETERERkQwGTEREREQyGDARERERyWDARERERCSDARMRERGRDAZMRERERDIYMBERERHJYMBEREREJIMBExEREZEMBkxEREREMhgwEREREclgwEREpLSim4AQapeCiBTEgImISEk5h4FXawMbJqpdEvIlhQXAD3OA7ANql6TcYsBERKSkHXOl54xl6paDfMvWN4DtbwMfPqjO+m9cVme9bsSAiYiIyNtdPKbeujOWAW/eD/zwH/XK4AYMmIiIiLydX4B66/72n9Lz9rfUK4MbMGAiIiLydhqN2iUo9xgwEREREclgwEREREQkgwETEZV/QgBXTgIlJWqXhMg12CTncgyYiKj82/shML8d8O1UN6yMJy5SA793rsaAiYjKvx/vXe6cvkTdchC5CmuYXI4BExEREZEM1QOmhQsXIioqCkFBQYiJicH27dttWm7nzp0ICAhA27ZtDaanpqZCo9GYPG7fvu2C0hMREZEvUDVgWrVqFaZMmYLp06cjIyMD3bp1Q79+/ZCVlWV1uby8PIwYMQK9evUyOz8kJATZ2dkGj6CgIFdsAhERkQdgk5yrqRowzZ07F6NHj8aYMWPQrFkzzJs3DxEREVi0aJHV5caNG4dhw4YhLi7O7HyNRoPw8HCDBxH5Mp5MqJxjHyaXUy1gKioqQnp6OuLj4w2mx8fHY9euXRaXW7JkCU6cOIGZM2daTHP9+nVERkaiXr16GDBgADIyMqyWpbCwEPn5+QYPIiKH8MRFquD3ztVUC5guXbqE4uJihIWFGUwPCwtDTk6O2WWOHz+OadOm4fPPP0dAgPn75kRHRyM1NRUbNmzAihUrEBQUhK5du+L48eMWy5KSkoLQ0FDdIyIiwvENIyLfJoTaJSBfxEDd5VTv9K0x+pCFECbTAKC4uBjDhg3D7Nmz0aRJE4v5xcbG4umnn0abNm3QrVs3fPHFF2jSpAnee+89i8skJycjLy9P9zh79qzjG0RERGQPITioqhdQ7fbGNWrUgL+/v0ltUm5urkmtEwAUFBTg119/RUZGBiZOnAgAKCkpgRACAQEB2Lx5Mx566CGT5fz8/NCxY0erNUxarRZardbJLSIiAv/pk/1WDgeunACSdgD+FRzMhN87V1OthikwMBAxMTFIS0szmJ6WloYuXbqYpA8JCcGhQ4eQmZmpeyQlJaFp06bIzMxE586dza5HCIHMzEzUrl3bJdtBRETklGPfAhd/B87uU7skZIVqNUwAMHXqVCQmJqJDhw6Ii4vD4sWLkZWVhaSkJABSU9m5c+ewdOlS+Pn5oWXLlgbL16pVC0FBQQbTZ8+ejdjYWDRu3Bj5+fmYP38+MjMzsWDBArduGxERkV2cqZ1kzabLqRowJSQk4PLly5gzZw6ys7PRsmVLbNy4EZGRkQCA7Oxs2TGZjF27dg1jx45FTk4OQkND0a5dO2zbtg2dOnVyxSYQERF5AAZMrqYRgpd0GMvPz0doaCjy8vIQEhKidnGIyFkpEUDhveFCZuW5dl1rxgCHVrtnXVQ+zAqVnp/5Dog07ZJik7VjgYOr7uXn5u/df2oBxYXqrNuIK8/fql8lR0REROTpGDARERF5PRWb5Hyk/xQDJiIiRfnGyYNcwJkeMmoGLT7Ss4cBExGRonzj5EHkaxgwEREReQJvbdry1nLbiQETEZGifOPkQS7gI01b3ooBExEREZEMBkxE5ANY60PlHb/jrsaAiYiIiEgGAyYiIiIiGQyYiIiIvJ0zV6rdLVKuHOUYAyYiIiX5yCXWVE788jHw35rA7xvVLonHY8BERETkq779p/T85TPqlsMLMGAiIiIiksGAiYiIyCOoOXAlm5LlMGAiovKP5wIi1/GREcoZMBGRcvL+AvLPq10KIi/lTGTPfwWuFqB2AYionLhzC3inhfT6pcuAPw8vRD7BR64MZQ0TESnj5uWy13dvqVcOIl/kbMziTNDDJjkiIjv4yEGTiHwTAyYicgHfqKIn8ngZnwPfTAVKSly3Dh9pkmMnAyJyAV+ubfKNkwe5ggt+N+vHS8+NegHR/ZXPH/CZ2mXWMBEREZV3t67KJGCgL4cBExG5gKcdfD2tPETmuPB7qmQtkBDA3g+BrD3SezbJERHZwzeq5Ylcx5nfkBuDlj++B777t/R6Vh6b5IiIiKicULIW6PKfyuXlRRgwERER+TpnAiofaZJjwEREyvORA6hZvrztpB5+71yOARMRkZJ8pD8HeRl+L53GgImIiIhs56PBFwMmIlKGjx5ETbBphLwSv7dyGDARkQt42MGXQQyRcox/Tz7yZ4kBExG5gG8cQIk8hxv/FDgaIOUeBba/Ddy5pWx53IQDVxKR6xz6EqhUDWj4kNolKf+K7wDZB4DabQF/Htq9krfW1Nhag7swVnouvA48PNN15XER1jARkUKMDvZXTwNrRgOfPa5KaXzO11OAj3sBaS+rXRKyh6cESc40W9u7Def3O74uFakeMC1cuBBRUVEICgpCTEwMtm/fbtNyO3fuREBAANq2bWsyb82aNWjevDm0Wi2aN2+Or776SuFSE5FVQgDXc9UuhR4f6MOUuUx63rNA3XKQ4zh4pEdTNWBatWoVpkyZgunTpyMjIwPdunVDv379kJWVZXW5vLw8jBgxAr169TKZt3v3biQkJCAxMREHDhxAYmIihgwZgr1797pqM4jI43nIv3ii8shHgjVVA6a5c+di9OjRGDNmDJo1a4Z58+YhIiICixYtsrrcuHHjMGzYMMTFxZnMmzdvHnr37o3k5GRER0cjOTkZvXr1wrx581y0FURklqc0Nbidb5w8SCHl4XdSHrbBBqoFTEVFRUhPT0d8fLzB9Pj4eOzatcvickuWLMGJEycwc6b5DmO7d+82ybNPnz5W8ywsLER+fr7Bg4h8QN454G6hwpn6xsmDyhsG+nJUC5guXbqE4uJihIWFGUwPCwtDTk6O2WWOHz+OadOm4fPPP0dAgPmrQHJycuzKEwBSUlIQGhqqe0RERNi5NURkyNOCBjMng5zDwDvNgUVd3F8covKETXLuoTHa0UIIk2kAUFxcjGHDhmH27Nlo0qSJInmWSk5ORl5enu5x9uxZO7aAiLzSb2ul58t/Kpyxb5w8yAWcatpS8XvnI01yqg3WUaNGDfj7+5vU/OTm5prUEAFAQUEBfv31V2RkZGDixIkAgJKSEgghEBAQgM2bN+Ohhx5CeHi4zXmW0mq10Gq1CmwVkQ/zkYOmw/KzgeCaHCOJjPB34y1Uq2EKDAxETEwM0tLSDKanpaWhSxfTKvKQkBAcOnQImZmZukdSUhKaNm2KzMxMdO7cGQAQFxdnkufmzZvN5klEvsyN/8jP7gPmRgOfDnDfOsn7uLRpSyYwc2bVPtIkp+pfnalTpyIxMREdOnRAXFwcFi9ejKysLCQlJQGQmsrOnTuHpUuXws/PDy1btjRYvlatWggKCjKYPnnyZDz44IN4/fXX8dhjj2H9+vXYsmULduzY4dZtI/JpQnjWQVTtsqSnSs9Zu1UtBrnR+gnA+QPA//0IBATatowztbRqfsd9pHZZ1YApISEBly9fxpw5c5CdnY2WLVti48aNiIyMBABkZ2fLjslkrEuXLli5ciVmzJiBl156CQ0bNsSqVat0NVBE5IN85IBOHqKkBMi4N5Don1uA6EfULQ8A9q1znuqN6ePHj8f48ePNzktNTbW67KxZszBr1iyT6YMHD8bgwYMVKB0R2Y5BCREA4KzeQMmixHpatwXzLlyP2jW4bqL6VXJEVB4Jz6/VcdVB3kdOHmRFsdJje7mDG+8l56W1XQyYiKj8MxfEeHpAR+VEefyelcdtkseAiYiICJBGffeGQPr6ReDgaheMUk/WMGAiImVYO9F44kmITWek7+YV4JXawNJH3bxiYeG1Ff/rA6wdA/z0it5Ed36fjdblI78lBkxEpDxPDJCIrDm2ERDFwKltKhbCxsDjygnp+cgGBVdtT9Bj9Pv2kd87AyYicj3VD6gq/wNWfftJnou+I6787O/cArL2AsV3XbcO0lF9WAEiIpfJPgD8sZl9PUieRzQr2RlcXc8B/hcPdPm7m8vPJjkiIhdRqYblwweBn/4LFBWYmenGg7yPnFAUdzwN2PqGm2roVPqMlNi2XfOdz8MuvtkkxxomInIB3ziAkot9fm8A4rCWHjJadnnGoF4Oa5iIyPV85B8ouUj+OceWu3lFugTfFkrWAgoHrnzzZj5Sg8oaJiIiRXGQTI9QUgy8ESW9nn4BqBCkbnnKMx/5frOGiYiUZ3IA9cADqj3/iovvAoXm+kGRWzhyQr5zs+z1jVzlyqI4pX4bKnb69hEMmIiI5CzuAaTUAwouqF0ScoRNAZeZIODsPuDgF4oXxyM5Mw6TjzTJMWAiItfz9ir7C4ek5z/TDKffzvf+bSOJuZP+J72Btf8nDWbp6Odsz3Ku/C55a94ehAETESnD3mY4IYBDXwKXT7isSC71VzrwWgSw7jm1S0KKsFJL8ulA4MtnXLNapYINb6rl8aay6mHARESuV3Qd2DJLGkSy1OE1wJrRwHvtVSqUAwdt/ZPbjrnS84EVyhSHFKbwSfm3r5TNz91cGaTYm7eX1kjxKjkicr3Sq5XwDjArT3qZtVu14jjOhgO9uXOHl/6j9inl/TOSDVKc2H4vDYDsxRomIlKJl5+gLJ0kzE32kRMKmcPP3oSXBqcMmIhIIcYnBpmDojceNBn4qKQ87/fyvG3lCwMmIlKeo5dxu5M3BmzkOq76PjgbZBfdkMYBk8Xvs6sxYCIispktt7xgjYF38oSAw+i7czsfeLUOsKCT61fNPxCyGDARkYswcCAv4okBw9m90vMVLx16o5xhwERELmDL1WQeeIKSY0vziiNNML+tA9Jmem8fqbP7gPRU+fKXlLilOKY8uIlYkc9cY/vvqfgucLdIgXX6HgZMRKQSmQP8rWsuPsG68QQpdzJbPRLYOQ849p1biqO4T3oDX08GTv5kOc3ZfcDrkVJg5YlcFsA7ERApHUALAbzbBnirMVB8R9m8fQADJiJShr0Hd2snqEvHpZPr0kedK5MrWdxeM9Nt3TcefZNYG1gbtX31KKAwXwqs7OVQ4GDvMl5Y42mv4iIg/y/g9jUg76zRTB/YficxYCJyVMbnwJbZ3tuM4skylknPp7erWw4id8rPBi7+4cCCDhyDlDxuWfvz8/PrwKqngZJi5danEo70TeSo9eOl5yZ9gfqd1S2Lp3F2WAGNp/6Xc1EfJlKe3TWean3n9Mq5Lkl6/ucfQJUwdYrjCGv7+udXpecTVpprvYSnHpWI3OPmFWmcE2fcuqpMWbyegk1yHhswqejmFWDpIODgarVL4pj8c44v644LBDxpHKZLx0oXtnEBNzenObJNd24qXw4341GJfNftPOkeZyn17FuONQiu56kBk8Fn7+Lvwbn9wLHvy97/9KrUqXrtGNeu1xO5pQ+TB3Foe50MmpwJGG1ZVqh1haRyPPSoRB7jei7w6xKg8LraJVFe7lHp2Z4f8q//k64wyTnkmjJ5M0X7RLjh0OSJV0Xp+6gnsCKhrE/LzUvK5OuL7G0idvq7rLd8efyDZfzbsWm4De/vw8SAiaxb8gjwzRTgu3+rXRLP8M0/gBsXga+e05toxwHxfCbwQbdy0Z5vnZMnCU+tYVKjD9PVU9Kzp3aaNR764XousPoZ4NQ2dcrjKI2SAZOe7Ezr+Sm1LneOa+ZImctB4OipRyXyFJePS89Hv1G3HB7HwR//sieAnIPAZ4MULY1nYB8ml/LUJo2Pexm+3/g88Nta4NOBCq7EHSdo/YBJwX29+33g8Bo7F7pXdm8JMtgkR0QWOXogYwdxPdYCJveVwqPsXwrcuW008d7O8NQapvP7Dd9fy1KnHM4yOOkrHKgcWGFf+huXpKE17jh5QYoBIXPccuJHZ1OTnPcHTBxWgHyYQmdlb/kX6GrCzn4bVv+VuiNi8sA+TOfSga2vAQ/PMs2vHPQBcZyZz0oI6cq7UEsXbdjwOdwtAk7+DETGQdk+TPYws641o6Xn4JpuLIeLGQRM3vmPiDVMZCMGBaQ0azVMHnpAtTcodMSJH12Tr1czs6+3zATeaQHsXuh4tj/OAZY/BSz/m1EfJg+pDblxUcHMVP5NufMKUxdRPWBauHAhoqKiEBQUhJiYGGzfbnlk3x07dqBr166oXr06KlasiOjoaLzzzjsGaVJTU6HRaEwet28bV3MTkbKMDoKu/JdefFc+zaXjrlu/NU5vt/GJzc0nuptXgJ3vAgU57l2vvXa+Kz1vSjY/35bPYf9S6fnMDqgeULicgLcGKp5C1Sa5VatWYcqUKVi4cCG6du2KDz/8EP369cORI0dQv359k/TBwcGYOHEiWrdujeDgYOzYsQPjxo1DcHAwxo4dq0sXEhKCY8eOGSwbFBTk8u0hX+L9/5ZU52gt0t4PgU0vAiM2AA26mk+z/zNgw0THy+bLvnxGaqY6sBIYv9uBDDwx8PCF36iK4zDZvzI3rks5qtYwzZ07F6NHj8aYMWPQrFkzzJs3DxEREVi0aJHZ9O3atcPQoUPRokULNGjQAE8//TT69OljUiul0WgQHh5u8CAn+VI/nZtXgNM7fWublWBc5S57ALYwf9f7wA9zLC/23b+Bkrtlt5Ewm8d8mXVbXr1VvvCdOPmz9Jx7xP5lPWm0bOdW6MZVldN1ma5cxXUrQ7WAqaioCOnp6YiPjzeYHh8fj127dtmUR0ZGBnbt2oXu3bsbTL9+/ToiIyNRr149DBgwABkZGVbzKSwsRH5+vsGDjHn/l91m73cEUh8BjqxXuyTeo+gmkH3AvmUsnVw3T3e+PG5h6Tch81vZ9T6w04aAzhWOfS99v8/tl0/rCK8LKL2zpoPUoVrAdOnSJRQXFyMszPAGg2FhYcjJsd52Xq9ePWi1WnTo0AETJkzAmDFltwqIjo5GamoqNmzYgBUrViAoKAhdu3bF8eOW+zOkpKQgNDRU94iIiHBu48g7WDphl46ofGyjbfnYdZIopwfoT+LLbkZsM2f3hdHyJSXAl6OBHe+YT64IOweuvHJKuhJL3+bpQNpLUk2mOcbfS3Pf059fB9I/ta0M+lYkAJf+AD4fbHlZT+TuiwC8LfDTaGzcRypul7ftUzNUH1ZAY/QhCyFMphnbvn07rl+/jj179mDatGlo1KgRhg4dCgCIjY1FbGysLm3Xrl3Rvn17vPfee5g/3/y/uuTkZEydOlX3Pj8/n0ETuWh0Xu8/aJh1wU23ivn5NcvzTvwIHP5Seli7HPvIeqDpI8qXTUfvM57fFqjTDqjZzDRZcZHpNKvZ6uVbegf4mJHm0341DnhiseW8bl62b9328JQrHL3hBL17oXRPS2cpsq2u/ty84POQoVrAVKNGDfj7+5vUJuXm5prUOhmLiooCALRq1QoXLlzArFmzdAGTMT8/P3Ts2NFqDZNWq4VWq7VzC3yMNxx8yHM4PQ6TGTcuAz+nWJ6vP8iftcuxvxgBdH8B8A+0b/2OOp9hPmBy5W/q4CrrAZO75GdLn3MVK/1I//wBqHY/UC3KcpryePwpLLB8hZ+rlMf96EaqNckFBgYiJiYGaWlpBtPT0tLQpUsXm/MRQqCwsNDq/MzMTNSuXdvhshJQHv4dmFLqH1V53DcKUPrgXGz5d26339Ypl5e7eErNjTXGZZwbDbzdFCi+Yz79mV3S7YLmt3VBYZz5/rnhN21pn5DHUrVJburUqUhMTESHDh0QFxeHxYsXIysrC0lJ0tUvycnJOHfuHJYulcbKWLBgAerXr4/o6GgA0rhMb731FiZNmqTLc/bs2YiNjUXjxo2Rn5+P+fPnIzMzEwsWLHD/BhKZ8IKTntuoeRm0gydEm24BYWfeJjepLYffkaLr5qf/9Yt71m/pM9H/Dnn6bj+8BghvA9RopEx+7q5tKge1W6oGTAkJCbh8+TLmzJmD7OxstGzZEhs3bkRkZCQAIDs7G1lZZfclKikpQXJyMk6dOoWAgAA0bNgQr732GsaNG6dLc+3aNYwdOxY5OTkIDQ1Fu3btsG3bNnTq1Mnt2+cT7tyWDjoBbmzSPPQlsHsBMORT4D7T8bocImy5DN5gAUdX5OByRorvSLUkDboCIXWUyVNRagyS5+QZL2OZ9L0augKo2kA+vRIngLP7FL5JrZ7MFUBb810VXMvCbUzczZl1urK8JSWAnwONO18+Kz3PcqbPk5Xt8oYaTJWp3ul7/PjxGD/e/NU1qampBu8nTZpkUJtkzjvvvGMy+jcpwNwBpPgO8FoE4K8FpmU5dhBwROl9ljb+Cxi2yoUrsvGgqcbJYPf7wJZZQGBl4MVz7l+/EtQ8QJv7zNZPkJ6/e8HK98qWz9rW74MAsvaYTra0X+z9nq1LAup1VK5GwhmecqsRe5y3PhyNQ378L/DrEmDcVqBCJeXzdxtHjnneX8Ok+q1RyIsVZEtX+hQVAHdvuX/9txUcL8vbqov//EF6ttTU4Qlk96ma/2itlO3OTfcVw6aTiBP7qSDb8WWV5G0BU8kdYHEP5fPd9qY0bIm1qz0dYeufD287znkYBkye7sZlYPMMIPd3tUtinSpV7l52EAbg+R0l3MjpGiYV9qXS33NL+Sm2Hg85QZr7rZ79Bdjzga0ZOLJSB5a5566CFxi4i9zv6eRW4P0OSq3MvnWXEwyYPN2GScCu94CFsfJpXcrcwUfhH8ndQulH7a6DlcGP3F1X1LjgBFZ0U7p825MIW/owqdwk5/RBXm/7hIXXDmftIYGOUswFTJ88DBScd39ZDKj4HXR3kPHbWuD6BSsJLJTn1/9Jx2UDDtxo+8pJ+TQejgGTpztfegsDC1/Iq6eB7IPuKo0VChzgNz4PLH0U+HqK+9ZpNXtXDFzpAnOjpce1s65bx6U/gQ+6AUc22L7M0a+VWXf+eamjrDFbRsS2yNHP1sK81AG25W3zesxw5ATr7u9pSQlw7lcz5VChNlh/24+st29fuHy/edDxw5qsvcA3/5COy87a+a7zeaiMAZPHkzlIvtsG+LCb59UwOGK/NHwEDiy3Lb2SBzW3nVgU+lepf/IsHSnY5PJ0AD++AnzcG7jjZB+zr8YBOQeBLxJtX2avTHOLrQHA3GZlHf091ZkdDi5oqeZWGL4FHPyOuvnEnL7Eveuz1Q+zgWPfmU43+A4qVeNsI0/602XJtTMWZqg5JIh6GDCVF1dOuDZ/b/hxO9WU56GXIDtr2xvAX/uAQ6udy6fQFTektuOg+dta+9LLsfaZubMDrS19mKyt5of/OJa/q+Qe8YxySCs1fHt2rx2LOlleueWFABZ481A3HnzMcyEGTOWFGgck1f8l6G3zj68A/61l/jLtcsncvrfyHXC6X5i9n7Utt0axtwwKfsevngJ+mKNcfvrs+i1aSmtjHtvfcjB/a9T+XbtI5nI77tvm4uPpravWb9/jEDOf29XTdixeTj93BTFg8nQe8yW24R+Tu+n3i9j2hvS86UUH87K3/B74D8uTa7rMcvK7feUk8Pu3yhTFVoruY2E+vwuHgU3T7cvKUvDndd8JBRlv+41cYM3/qVMWY+74XDI+l7pskGIYMKnBrh+LgyeVkmLHlrNE7QOvuU6/5rhj33qs8n5yNPN5rRym/Goc6fTt+MpMJ929Dez7sOx96WZb+/O0/W3b8/dGSh1/jm9y7/osr8DF+aPsT6SaZShnGDC526EvgbeaSLdEUJT+FSEbgFdq23dFkyc7sxtIqQekpxpOt3RQu3BEurLKLu4aVkAh5k6erjzI21vTKVeWW9esdCh1lLsDYDcPJ6H053t4LZD+qel0j6nVdgdL26r0b8l4PS7Yxy753HzpuyCPAZM7XTklXe1zI9c1/4xLfZEo3dndniuaZMmNw+TCk/XqkcCdG8DXk+XLlPcXsChOurLKafwH5jJvNiy7KhKQAqg8D7nFi7UTj9o1rfbaMsvy+DdfPgN8/Xcz+91o+29dA/Z9BFx3ps+NDfst+4DCQ2N40Gd19zawZbbeBKUHQFVi2AYGR3IYMLmTfie/O7eAPzYDi7pKBwpLPPnfntvKZse9tW7kOpatvSdCe9JnHwDmtgAOKHnfOzs7fbtkfU4ouWv4/vVI4J3mrj8pu4O574a5+5IJYWORndj3OYekYSWskesIvWGiNEbaW42kzsqOsOX38uGDwLyWjuVf6vIJYEl/4PgW5/JROjA+vR3YMVfZPGV58LnDSzFgcif/CmWvi64Dy5+SOniufNr5vF39z1fNf9ZuCcxcOKzAl88C+X8BX411fB1KlMOcopvAjneAi38oXx5H5Fj582DN9y8CqxT4HQEK9GEyk+aipVsbueF3dfOSTAKjMhj/3vQ71it9DzSlrR0rjYn1+ZNOHrMcWDb3qHRXBpuuSPWWYMbBfVh8RwqA17r4mOdmAWoXwKf4a81PLyqwspDCY8/s/xSo0x6o3Vq5fF3OUg2Tp9xLTuagUlyk/CrNBpEOHNx+flU6yG+ZBcyy9ZJrWzh4oHX0YoU9CxxbzhluuwWK2n+G9L5rpbXkxXel0Z/DW9m6EkdKZj97apitceSzLb191d1CoE47Zcrhrc7slGrWrbWeeCHWMLmTf6D56X5uiluPrJf6AX3YzYGFVRxWwGINk4tH+va2/irWWNoWxS8+cJLSV3d6PAW+Y1vlroayl/EtZ/ROE6V/Uk7+JJ0U5UZz1y3n5HbmnwMKrN0HrZSd/Sr1jy1K3Vvy3H75NK4IIJ299YjZw6yDf9g95s+sshgwuVOAhYBJ8QHMLLhw2PFlzR7wvKVa+Z6Lx4CMZXpDFLjrVghu2k8OnZRsLJu7+qsJhQKmoFDHl3V0W0v3v82fg4VxmEwLZH32T6/YuD4r5bC6er3TRGlAa9wHzdl1yNmzEHi7iZmbwBrx5D6f5PXYJOdOAUH2L+M1v38VAg57V1l6KwKNP9B2qFFeTgxcqUpNlId/MRzdJ1ZrmOzIU+Oi/4LWtmvDRCCwMjym87mtjLfJOOjw8wdKPxZHaw6U+o3sWQjc391KAr2yO7NOV/d/8pba64JycI9SBbGGyZ0q13JgIb0DwMU/gBM/mU926AuHimQ7D+z07ejB+1y6mXy95ABmjdX9IYBPHwVWGA1nYfM/cncFaEp9Dk6U99Q2G5tVjGQsk5qo7t52fN2qMN7nVsYM8vSmFmdqmOwNYr6e4sTtddxwvJHbFx88AKwdZz2Np3fydzMGTO4285rjyy7oCHw2CMgx07SWsQy4dNzxvD2a0ifr0oOVQv9G5biimcDePK+eAU5tBY59C9xxwwn9m384tpy1z8Gez8jZff5RT0uFcC5fl+fnSBFkapg0ZgKmSx5yVaWryH3XLv0JpC8xP8K6Ld9TT6hhunsbOLhSb4KZ30zJnbLXqQOAEz8qtHIPryG3gAGTu2k00tVIs/KAp9c4loelO4LLjm6t8JdUYxRwXM+VLidVmsViK3nQcTKvm1dsS6daHwsnt8/ecp/4wcEVufO2JM5ysjzCxj5MntQvp7S8aS87tpyrHFh1b5wze2uNHbygpNjZm1l7odPbgc8ety3tDTuHsvASDJjUpH8QsXSvNHffAsMuemW79AfwVmPgQ2v9CxTmbH8Kg6t/nBi48ufXgDeiDEeuNuCmmiyb6ZfBg07GAJA2U6GMXLRdQgCbXwJWDLX8/bPrM/a07wNMmxQNtsfR8rq4qfWrsdKj6IZC65Ghv09MPm8PGCpCMXYMGqxvrYfc5FhhDJjUpN+n6ftpQOF15/JT85/o4Xu1Zbm/GU4vKVbgUnEXbZdsHyYbazsu32sK/XqK82Wymb2BtLP70E3frTwrt8ZwZbBZaG0sNCO75gPHNgKntltIYGM5NRrPCKDtKYOj5XXXduoHe86ssyDH+bJY40zZvkpSrhyq8bA/ajZiwKSm++qXvd73IbD1dTOJ7Phiyf0IXRpQmWv/LgEWxgELOluuQbMpawf/5VikwJhLZtN7wMnPFvplN7dv144D1pTPf4gWpdRzYCELn7et3yUhLOdhz/qc5o4yuOm3odQx7tMBtq/H+PO2qQ+TE8fDAyscX9Yae/adJzUTuxEDJjUZX/rs1Z22zRwkbl8DLh2TamCcGmtK4avkdAc0Z5rKHDwBKHWgsXekb1vXe+ua1BH00BdSnzSP4sZO37pVCuDG5bL3h23od2jrd+nycSA91ba0Ny4Dfzp5fzQlqF0jZtfn6q7+YY7sE3fsRxcGNWp/D1TCgElVXh6lO3PwEsK2Jsjiu8DVU7blaS9nhhUwF6zZchCxe8A/hdjaD0V/n6hVVkvsOkgr9NtaPwF48/6y9zYN/mpjOZc9Cdy8LJ8OkMYfchW54iox/IZSJ9hjG9UP5O/cAra9ZSWBB1wlJwRw5YT9yxW76zfvnQEXAyY12TK4ntk0XvJls9ap+osRQEpd6e7i1hxYrny50pcAB1YaTrP31iie+BGUlvfObeB8puWmN2vbZW5UZ8DrY3uHZX5u/zIuGavIQ75wDm+bguXfPMPKTDtrjQ06uNtYxu1vA7+ttbwem5paPeTz1Hf1NPDfmsAvH6tdEo/FgElNttTQKF0F7U7mxm8pdXSD9PzLJ9bzyPvL8jynOk6Og8HBtahAugHt+UwbM7CjD5NL2vut5LnsSWBxd+lGy2ZZu0pO/zPztPu6edj32xKXBEyujFg9rNP3kfXW59+66lgZjF09A9x24IbT5zOMJhht2+1r8nl4apOWKAG+/af0uviuC4dP8M5/YAyY1OSq2zd4DBuq8u/esiMPI06fmPTK9ON/gR3vSIFGqTs3rSzqhgNewQWpJszmgSbvlenMDun51yUWkumVvTStcR6A0dWNXnaAK0+dUoVw7fbI/Y7cPazAFyNkslKgrx4A/Gr0Z83mIhqt484tw9/KX78AexbZXg5XUOL45MpmYC/Fe8mpypYaJjuDKlurqxUhk5+tzUC25uFK2QdNpx3fbGUBC9vz6/+A4FpAM/2rbBzcho8fBvKygAu/AfH/MZzn1H6x1tRoZ23DsicBbWUnymIHT/1XbsxbyukIj982G/tb3bwCHHTwdlLGv73XIoCwVobT5Droe/x+BPD7t2qXwOOU9yoOz2ZTMGTneDu73nO4OPbTK4e5qm2Dwd0crQ2yFhjYcNC5eAxY/QyQe9R6OnvLZyn9N/8AVg23Ly+DfAVw5ZT0nJclTSs9cG2ZDfxQGjjZ8r1Q4rJ3K66ekkb0lmtCcXT9pgvanvT6BQfXoQDFm+QEXH7F06U/gfntgP2fuW4dbmHDeq6clAJ9JW8se+GQnQt4QcBEJljDpKby1GxwcJVMAoUOEPrjOdlyYvp0oHTyPPmzmSLJNDVoQy3n66oTwNbXgZ9TgAf/pb8y6R/xjrnS266TbcvLUhnlbtJrLp0njDj/p6O3W3EzpQMmAdcfK76ZIgUSGybKlMVN4zAVXrdSc+lkwL90EHDtjOP5KhG8uvy3w4DMFVjDpCbjGqarp4Cd7xpebm/PgVLVgSvNsaWGyY5mPQD48EHD94XXrd9Oo7Sm4ZbMvd7MlS+8lek0a+ktsWe//5wiPW9703C6/j36LF7ub+/QCHJXBop7wz/YMQq2o+u1xcbnlS2Hyyh9snLDybW4yLakZ3Y4Vgtl72f+8cP2rwOATcGM2WDJnlWoNXYTqY01TKoy+uFd/F26qeWVU8DAeebTOKLoBrC4h313GL9+ETi02nBa6c1C/WyMs63eb8kGd80cxPWrvgWkAGP3+/Lrl2N2XCUrQZE9AZOz/yaV/jdamp9cvkIAa8dKg1gq2iRUzk8WLqk9cPUghHbkL1cLZU7+OfvSX5RpQncFd9aYekMfJjLBgElNlv6pnNkpn8Yeh9fYHiwd/Ro49KXU5+fSMcN5n/QGbucD43cDfv7Ol8ua374CVo8CqjeynEaUALlHrMy3YywUs6ME2Nh05WpXTwOnthpOs6mJzFIZhZX5RrWCh74wnX7nFlChoqXSkuJNci6+Sg7Cen9KJda94m/O51HK2u/65iXl1mORF9QwMSBzCTbJqcmmA5Gdt8Awx5YD+MHVwJuNgFVPA0fWmQZLgHS57KVj0gkcsPyjPJ8JHP3G/nLqWz1Ker78p5VEcv+M7Vm/ueYpazVMduTt9AlH2Hj373udd/XemrXxX9JYNrKjlVvIYMPfpdGWl/SzoUzmilneD+auaJJzcXO6tYDJWz8vV93uSIkA0iv2qTeU0b1UD5gWLlyIqKgoBAUFISYmBtu3W7oDOLBjxw507doV1atXR8WKFREdHY133nnHJN2aNWvQvHlzaLVaNG/eHF999ZUrN8G1rP04bf7R2fADXzvG9vu9ya13cXfpSrEcG64cKd2+vYttW7dxORTZP7AQHBktf11v/7hkcEIbWWpCEQLY9KL88r+tBb57QX7/WOpkfegLaZBPh69E85IDcZ6dzUilXNWE6iour8EiU+z07Y1UDZhWrVqFKVOmYPr06cjIyEC3bt3Qr18/ZGVlmU0fHByMiRMnYtu2bTh69ChmzJiBGTNmYPHispPt7t27kZCQgMTERBw4cACJiYkYMmQI9u7d667NUpgHHshWPW16axFzjJsBi24aDYao57t/mZ9ulR1NbmZny/SxMg6KNv7TjnXrU/oztLJugzJbSXfiJ9PtO77FsEN52kuWl1dqtGVP5ujwEK4IcJwNaG5eAXJ/tzDT2wImFwUDSv4BlfPXL87nQW6nasA0d+5cjB49GmPGjEGzZs0wb948REREYNEi86OktmvXDkOHDkWLFi3QoEEDPP300+jTp49BrdS8efPQu3dvJCcnIzo6GsnJyejVqxfmzZvnpq1SgME9wJzMq/gOzB5gbl0DPh8i9Vey18Wj924tImOT3iCahQXAq7Wl2ieDA5MTGyjbadmeWiAbAiaD5i4V/8FZXLe1QMpo3o1cacwofZ8/CWx9w8kyuHhZdzK5BYatPLBJ7o0oYGFnC9nL9GHyWi76nnlVcKmCr5Lk03jpPlTtV1JUVIT09HTEx8cbTI+Pj8euXbtsyiMjIwO7du1C9+7dddN2795tkmefPn2s5llYWIj8/HyDh+oKC+7dOdqeJiczB4hX6wBfTzadvu1N4PgmYM1oJwopc0DSv+3JqW3Sc84hYE51J9Zpx/qVuErubhFwZpfpFXtqn/TNHXBuXAL+TCt7L9cfydyNjR252azdvCRgcpTSzbUF2cBROwcHtYsb+kgpyWW/vXL0vXS0f6ESDqyQT6P28dNBqgVMly5dQnFxMcLCwgymh4WFIScnx+qy9erVg1arRYcOHTBhwgSMGTNGNy8nJ8fuPFNSUhAaGqp7REREOLBFCrqRC6TUk2pjnB0w0NL4Kjf0riYpsnLPNKX4Vyh7bXxT17sO3uBRCMPtME0gl0HZS3P9cYQAvv2HdPD5fprRZ6FAp++PezvWd8vSunfNdyAv46xt3S7vPOC5hdIng2/+YVt/QGd40z/+rN22pXPFSfl8pswxx0Owyc8lVK+H1Rj9UIUQJtOMbd++Hb/++is++OADzJs3DytWGEa09uaZnJyMvLw83ePs2bN2boXCSm8zcuEwrF8l58QB4aBeHySbrsBykp+VESz2feRgpgLIzrQy28kD5vULQMYy6bXJjTptrEU4+IXlIR3+2udY3y1Hghqbb4di43b5QpOcw7xs++wdh0ltd2+7ZtR3W76Xi7sDZ/cov25P5MrfqTcF6HpUG4epRo0a8Pf3N6n5yc3NNakhMhYVFQUAaNWqFS5cuIBZs2Zh6NChAIDw8HC789RqtdBqtY5shuuZ/WJppE63xh2or5x0bB2/f+PYcvawNm7Tdes1ihbZM86SI8tbuwpMLrDIz5YC0dOWr/p0nLB9ZGb9ZVRJp/SyXsCeAWI9gkynb088uS17woZE5fx75koZy6Q/dLYo93+AyqhWwxQYGIiYmBikpaUZTE9LS0OXLl1szkcIgcLCsiaduLg4kzw3b95sV54er+Qu8HoD4M2GhtO//afZ5C5j1w/F2gHZwa+h3CB1zgZUJvS2QS7vdc+5KFiC1JH/+Gb5dPpl3PuBsmXwoYNkuSdXw+Qzn7WvbKcN1k9QuwQeyaEaprNnz0Kj0aBevXoAgH379mH58uVo3rw5xo4da3M+U6dORWJiIjp06IC4uDgsXrwYWVlZSEqSetknJyfj3LlzWLp0KQBgwYIFqF+/PqKjowFI4zK99dZbmDRpki7PyZMn48EHH8Trr7+Oxx57DOvXr8eWLVuwY8cORzZVfeaCiUIP6JQOACuH2p7W0r/UvLNA9gFlymNCwU7hgNE5RWbZi5Yu4VbAzcs2JnTgBOCOk6PPnIC9iCfWIpFn+vMH5+/H56UcCpiGDRuGsWPHIjExETk5OejduzdatGiBZcuWIScnBy+//LJN+SQkJODy5cuYM2cOsrOz0bJlS2zcuBGRkZEAgOzsbIMxmUpKSpCcnIxTp04hICAADRs2xGuvvYZx48ouce/SpQtWrlyJGTNm4KWXXkLDhg2xatUqdO5s4ZJaj2fmQGbx5qtu5vBl13r++N75PCxRuobp8omy13Y3iVlRdAMIDAaObLAtvUsHzWSTnO/xsj5MtmJg7hrGzaE+tJ8dCpgOHz6MTp06AQC++OILtGzZEjt37sTmzZuRlJRkc8AEAOPHj8f48ePNzktNTTV4P2nSJIPaJEsGDx6MwYMH21wGj3buV9NpxR4SMNlFhQOyXGBp7+/8jouuJny1DjBwPvD1321L78oDlK15/7nFdWUg97pxGTjhgk7UzrI0yK2r+NCJX1E5B4BiB6909jIOBUx37tzRdZLesmULHn30UQBAdHQ0srOzlSsdmffTf9Uugf1+fs3965S7TYgra2rsHQnb1mAJcO2VbMZDPriCmreVIVNrx1ied8PW5l87Zdlwpdmcas6tw/iG1eQai3uoXQK3cai3bYsWLfDBBx9g+/btSEtLQ9++fQEA58+fR/XqSg1KSOVKwXn3r9OWAdRc5e5t1+Vtc8Dhof+YGTB5j7VjgDs3lM/3f32Uz9PYd/+2cwEP/b2Qx3AoYHr99dfx4YcfokePHhg6dCjatGkDANiwYYOuqY7I83npATL3N7VL4BwGTN7jxI9ql4DIYzjUJNejRw9cunQJ+fn5qFq1qm762LFjUalSJcUKR+RS305VuwSOMb4HnCWe2idD6XIVODiOF5E+T/29kMdwqIbp1q1bKCws1AVLZ86cwbx583Ds2DHUqlVL0QISuYyrbzehtsvH1S6BeUp35lWjfxwR+RyHAqbHHntMNzbStWvX0LlzZ7z99tsYNGgQFi1apGgBiaicUbpJruSOsvmRj2INE1nnUMC0f/9+dOvWDQDw5ZdfIiwsDGfOnMHSpUsxf74CNwAlovLL2v3/HHHKRSOqExHpcShgunnzJqpUqQJAuu3IE088AT8/P8TGxuLMGd8cAZSIbLR8iLL5+eiow6Qw9mFyH1tu7eSBHAqYGjVqhHXr1uHs2bPYtGkT4uPjAUg3uQ0JCVG0gERERERqcyhgevnll/H888+jQYMG6NSpE+Li4gBItU3t2rVTtIBERESuxxomss6hYQUGDx6MBx54ANnZ2boxmACgV69eePzxxxUrHBEREZEncChgAoDw8HCEh4fjr7/+gkajQd26dTloJREReSf2YSIZDjXJlZSUYM6cOQgNDUVkZCTq16+P++67D//5z39QUsJRfImIiKh8caiGafr06fjkk0/w2muvoWvXrhBCYOfOnZg1axZu376NV155RelyEhERuRBrmMg6hwKmTz/9FB9//DEeffRR3bQ2bdqgbt26GD9+PAMmIiIiKlccapK7cuUKoqOjTaZHR0fjypUrTheKiIjIrb58Vu0SkIdzKGBq06YN3n//fZPp77//Plq3bu10oYiIiKgcu52ndgns5lCT3BtvvIH+/ftjy5YtiIuLg0ajwa5du3D27Fls3LhR6TISERFRefLrEuCBKWqXwi4O1TB1794df/zxBx5//HFcu3YNV65cwRNPPIHffvsNS5YsUbqMREREVJ6U3FW7BHbTCKHc4BMHDhxA+/btUVxcrFSWqsjPz0doaCjy8vJcf6uXWaGuzZ+IiMjT9JwBdP+X4tm68vztUA0TERERkcNuXFS7BHZjwERERETute9DtUtgNwZMRERERDLsukruiSeesDr/2rVrzpSFiIiIyCPZFTCFhlrvoBwaGooRI0Y4VSAiIiIiT2NXwMQhA4iIiMgXsQ8TERERkQwGTEREREQyGDARERERyWDARJ5N4692CbxLzWi1S0BEVC4xYPJEQ1cCE9PVLoVnePIjZfOLerDs9QP/cDyf+l2cL4sr9HzRPeup0dQ96yEi8hAMmDxVjUbAg/8ue98uUZl8qzZQJh9rXswGHnkL6Dnd9euyV9NHgCc/ASb8Amic+PqH1LYvfaOHXR9kVG8sbZ8SAqtYn+9n1wW2RERejwGT2toMk56Da5nOq9O27HWXvzu/rsZ9gMkHrKfp+zowPQcIujfm1rAvgH5v2LeewEpAp/8D6sfJp+37mn15lzK3v2zVajBQswkAjeN5hNQxfD9uu/X0T68BJu6znqbLJODh2UCTvtbT+Qeanz5hrzJNmMG1AD/9Q4OZ/aRxYt8REXkhBkxqG7QQmJYF/Ou46bymjwCPfwg8t1s6wU/PKZs38F2pluTJT4A67aRp9/cwTGNCyJcnuAZQoSIw9aj0aNIHqNfBni0q0+AB+TQdxziW9+OLDN8PmGfbchGdbEvX8CH5NPpBbO3WQLNHbcvbkmoNgQemAO1HWk835ZD56X7+hoFM92lA1SggbmLZtKb9Laz7/rLXfV4Bes8pe28cHAJwKtgkIvJCDJjUptGU1eaUCgwum9fmb0BYc+l9hYpS/6bY8UDbp6UgqtVg4Jnvgb8tB4Z8JqWxpG6MfHmEKCtD6Ymybgww6lsgvJXl5R5fLD33n2u4bU9+AvhrgeCaFhaUO/FamG/cnNakj/VsJu0HRm003AeWaknaDgdiRsmUC9K+B8qa2mxp7uz4f9LzoA/MzLy375v2A8ZtM53d6GFg/F6gSrjl/PW3qUFXYHImENW9bNrQ5UCncdLr2m3N5+EXADR/rOx90H1m1mO5CERE5ZHqAdPChQsRFRWFoKAgxMTEYPt2y00ba9euRe/evVGzZk2EhIQgLi4OmzZtMkiTmpoKjUZj8rh9+7arN8V5fV8D2o8AGnSznKZpP6BvCuCv14ekQhAQ3R8ICrGef9cpjpetwQNAq6fK3g9Zaji/TYLUd6njaMPprQYD07MtNzPJ9iOyoVZMyggY/D/Ls0PrSQGE8TKl9PuLCQE0jgfui7ScX4VKQO02wD9+A5LufWdt6dfzyJtSjWLboZbTaDRS3vr8KkjNerUcuArOODDsPUfaV4lflU0TAuicBNRpL32X9LfFOKAHpP1DRORDVA2YVq1ahSlTpmD69OnIyMhAt27d0K9fP2RlZZlNv23bNvTu3RsbN25Eeno6evbsiYEDByIjI8MgXUhICLKzsw0eQUFB7tgk58Q+Bzz6nuv6h1S4tw8a9Zae9ZtqdKwEKLHjpc7cE3+Vmp+aDzKcH1jJ/HJ+/pYDI9lttTBfvx9P9ACp1qXlk8CsPKmMtuZT6qHpQPcXgEo1gJ7JUk3d3zMtp4+bID2H1gMCtNLrKL1AN7wV0GG06XLmahTlNOkL/PuEfcsAZbWFxtteIUjaV5WqlU2r0w7o9zow9idpe7RVpE77D/4beOJDw+EK6nWU9hURkTPOZ8in8SCqXuoyd+5cjB49GmPGSP1Y5s2bh02bNmHRokVISUkxST9v3jyD96+++irWr1+Pr7/+Gu3atdNN12g0CA+30mzh64auBArOA/fVB35bB+T/Zdty/hWkztylGj0MHFln27L6AdMTHwFr7+UjFzBZml+vk9QsWSUM6PWy4TxhJugzl4/xtJ4vSv1+Sjs8+1kI8kasNx/0NHxIahq9/Kd0VWPeX8Cvn5hv0jLHUmfu++qbrm/IZ8AXiUBkV+DMTtvyN2f8XuDACqDrZNN53fVq3SbsBWbdK0Pb4WVBIhGRoxb3kP7kegnVAqaioiKkp6dj2rRpBtPj4+Oxa9cum/IoKSlBQUEBqlWrZjD9+vXriIyMRHFxMdq2bYv//Oc/BgGVscLCQhQWFure5+fn27ElHqzbP4HC68C+Dw2n+wdIJ2EAGLMFOLYR+Haq/flXDrM9bWRXIP3ezZtbDwFKigFtZel94jrgp1eAv34xs6BRUPPvU9JzQCAwaIH5dZntpGyGXwUz04yCpMrhwHW9jvRhLYFIK53Zo/U6VVeqJo2nVUVmP7UfCVw8BrQcbDi97dNA5jKp5tFY80eBGRelQHRTsvkO9qWfT8X7LK+7VjTQe7b18hnjFXJE5INUC5guXbqE4uJihIUZnkzCwsKQk2PtSq8yb7/9Nm7cuIEhQ4bopkVHRyM1NRWtWrVCfn4+3n33XXTt2hUHDhxA48aNzeaTkpKC2bPtPGl4g5A6QKXqpgGTQZraUr+j0oDJXO2MJY17Aw/+CwhvLZ+21WDpRFu3vfRevw9Pw57So7QGo9O4sjIbn5wrGQbHZkX3l5qSgkKBzaVjQZk5yXf6P+DgSsMOzsYmpQMF2dK+LCmW7ydmrEYjy/Om/g4U5gM1LYzPNGgBMGCu5dqcgHs1Uo+8aTh9+JdA/vmy/k51Y6RBOp0dg+v+HsDJrUD0QOfyISLyQqqPPqcxOiEKIUymmbNixQrMmjUL69evR61aZWPyxMbGIjY2Vve+a9euaN++Pd577z3Mnz/fbF7JycmYOrWshiU/Px8RERH2born6DAa+HML0DpB6pzcZqjtl9PbQ6MBHpphe9pWg62neSoVuHIK6DZVL8hzoDZDo5H6JOWd0wuYzKh4nxQQWaOtDGjNB9pOC6kNQGYATEeavhr3Nnyv0QAPz7I/H2OJ64C7hWV94YiIfIhqAVONGjXg7+9vUpuUm5trUutkbNWqVRg9ejRWr16Nhx9+2GpaPz8/dOzYEcePmxnn6B6tVgutthz1yRgwV6opKg08Hzd3CbsHavG46TTNvavfvnwWGGg+4LUotC7Q40VpiAR/1f8beD+NhsESEfks1a6SCwwMRExMDNLS0gymp6WloUsXy/fpWrFiBUaNGoXly5ejf38Lg/DpEUIgMzMTtWvbeSsLb+dwPxM7muTcoVJ16YquGblAjMyAjub0eAHoYu5qQHK71n9TuwRERA5T9W/31KlTkZiYiA4dOiAuLg6LFy9GVlYWkpKSAEhNZefOncPSpdKYPytWrMCIESPw7rvvIjY2Vlc7VbFiRYSGSv1fZs+ejdjYWDRu3Bj5+fmYP38+MjMzsWCBhQ7CZMhT7nb/+IdSR+jS26vwqiwiIlKRqgFTQkICLl++jDlz5iA7OxstW7bExo0bERkpDRiYnZ1tMCbThx9+iLt372LChAmYMGGCbvrIkSORmpoKALh27RrGjh2LnJwchIaGol27dti2bRs6dXJBH57yJGknkHfW8P51amrD2ohyRxSrXQIiIodphLDnsijfkJ+fj9DQUOTl5SEkxM6rooh8xSw7B+Bs+SRweI1rykJE3knhcZhcef5W/dYoROQjRInaJSAichgDJiJyDwZMROTFGDARkWP0b8ZsSaXqZa8Dq7iuLERELsaAiYgc0+tl6RY7fV6VRmc3p47eLYnixrunXERELsCAiYgcc199YMohIG4C0HGM+TQa/7LXYS3cUy5P07CX2iUgIgUwYCIi5/n5m5+u4SEG/mZu8kxEXodHMyJynqXA6PY1txbDI7k7aKxs/dZSROQYBkxE5Lwq4eanZ+12bzk8kqO3KXIQh9YjcgkGTETkvAoV1S6B6/k52rTm4gDmH78Zvnf4PpJEZA0DJiIiOQ/NAFonqF0K8/SHbgCAbs8DwbWAbv9UpzxE5RQDJiLyHAPnq10C8x78lwfX3BiVK6Q28Pwf0rAPRKQYBkxE5DlC67lnPfYEZo1633vhgqa1Jz5yPg/jTuUafw8O7oi8FwMmIiqf/AIsz6tQyfZ86nWQnqvd71g5rHXCbj3EsTz1GQdMloZ4ICKnMGAiovIp6D7L8xypgYkdD7QdDiR87nCR7FI1Cvjbcvl0DJiI3IIBExF5rlrOjA6uUBNa6UjdFSoCgxYCzQYok6+c/m/bVqtlHPxZq1kj15nwi9olIBdjwERE6un7GvDwLPPzajYD+r7qeN4lxZbn2VrDNHYrENHR8TJY0zheJoGATWM4GW+LxotrmOImql0CSbNH7UtfoylQuZZrykIegwETEakn9jkgsHLZe/2T/4Q9QGhE2fs67YEnP7GcV9fJRic6azVMNgQiFasBddrKp3PEI28BTyy2nkYIx5oOL/wmn8ZT9XlF7RJIajW3L71/Bd4GyAfwEyYiZbmqSajni0Cddpbn954DJHymzLo6PwdMPWJ5ftunDd9HdrWSmZnArdP/ARWrWi+DKHHsJHznhv3LlHLnjYKr1FE+zyb9gAbdnM/H3kA1MJhXJvoABkxEpKzGfRxf1toVZVaDEnN5WZknd3ILrm599PJBCwzfN3wIiHrQfFr9GjRz7u9Z9rrva2WvhYUmufvqW8/PmVuj2HrS1y+zMVv7nVkLSAH7g8V/HgOGrQTui7RvOSU8+j5MPiv92tHy7JG37Es/ab9ryuEGDJiISFl+Nh5WbDmxWjuBx4yyvqwosZax/LptFRQqNQfWs9DXqXpD68u3ekrvjV65RIn57TdXMxXeWi8LG7atRlP5NNb4690mprLefQS7TgHGbAGSdsjnIVdOewPv0vsZOvLR/m2FAwvdUzcGqNnEdHsqVAQeW2h+mRfOOL4+T9JysFRb+u9Tti/j6PAcHoABExEpo9M46d99u8Syab3/Yz5t9UbA6M02ZKp3EjI+IcmdULVWanaU7G/S+m9SAGHxXnN65Y5/BRi33cZyGfVh6vWyFGSaG+xyxPqy17KdyY3X46RuU8te954NBFYCwltZbz61JnoAUL0x0OABBwvkQMQU/YhhTWD1Rg6s1sw+tVRLWfE++/O3RSsFxvWyR/1Y6blSNdvS9/6PVzddMmAiImU88gYw+QCgrVI2LbyV+bR1Y6wHNLaQO/BWCbc8TzZgsOegfq8JLDbJtObmoRmG5ewyEajd2jBNYLDeao1qmPTL0eJxYPwuoKZM7VClGjaX3HF65QoIkk9jLLy15XkJy4AJ+wD/QIdKZlMwOH6PuQXLXrZ4wvb16ZpAzWxvs4FAk7625+WoOu2kCyKeVGDkeFs9+j7Q4Vn7lmk7zDVlcRMGTESkHI0GCNCWvb+/B9D9BeCpVOl9aYfcmGfK0ljrc2MQQBilkzsx2pqvOdoQ6/MNM5OeKlYFJu4rm9znVekedHLBV6NeUo1GxzGG042b5Kxtr36Qauu/fXNs6f9Ur5Ph+9YJQEQs0H2abetIWFZWI9bvTdP5Go3tzbrm6Nd4WWIuGNPf135+pp+HRcJ0eWmCVPM4bJX5xZo/ZmP+VjR6WAqUxv4MtBosTbNWg6mk9on2D5Lq5VcScoQzIlJWnfZS08B99aWTSM8Xy+aNWA/cuGi99seAtWDDmap9K8s27Q+0H2FHVjLlkAtgtFWASenS6316NQTGnb6tnWz8KwD/OiEtox+w2stav6/2I6WTcp32wJd6NQsVgoDRm0zTW9ovzQaWve48FujwDHA+E6jR2KEim6jawMEF7fg+afwBYTzOl5XlowcAv39jOO3J/wHd/wA2JQMnf7Z93fqeXmM6zbgG05N4ecDk3aUnIs+j0UhNA71eMp3n528mWLK1Jsi4hknmBGdtvsYPCNNrLiyt6ajRBBi6XAoCbCV3Emg/Emj5JDBoke15AqbDCsgNSBlcA6hc0751mFunJSF1pCsBtZWV7YfiX0EaHNS4X48z65C7csuWE7etVxvGvyKf51Ofmk7zDwDCjMZ7ioi1/6ozb+Llt+1hwEREnis0QhpOoGEvMzfMlTmhmjvhlV5yXr+zdAl63ERg8kGppmPCPmDcNgcKKVOOgEBg8P/s779hT5OcnIo2NtNZC5jsvkxer+ytE+xc1gYjv7E8r9HD1pc1ty9tjc8Slhm+b3BvuAvjAO/hmWWv/a005ugPgzB6ExCmd/Xog/+yvFyXSdbLqSSl+sWxhomIyAkGgY2ZWqRR30pNDya3AJHNuOxlrebSJdCT0oHkv6T+RqH1pJGlq947YdVsan3sJUss1YQ4Mx5SWeZ6L504XFsdYkE/nV6ZBy8xnNfmb2WvH/y39Kx/RaQx/f3iSDNZadBTOcx8rUtUN2D4GunqNnM1ONaY/cyMp5n5/AYvMWxStLR80k4gur9tZXl4FtBmaFm/ruI7ZfMemmF+GIlZeUD8f23L3xpr42npc7QDvjFvvm0P2IeJiDydpVoWewKI53aV5eNv6fJ/B+lf5eYs/YBFyRom4+Ctbgxw8aj1ZVo+AVw9BfwwR3qv35xSLwZIPqfsthurFiUNRhkUChy00HG68cPAtLOmncT1A8TGfYDjxn2szARMxkGU2SDTShDs5yeNu1R0HQhvaTmdsUrVgMc/sLzeJz4CPh9se3726P4CcPIn+XTGTWmODhnhTP86D8AaJiJSl6WhB8yxJ4DQDxJcMfZLvzeA+nGuu2Gs8YnTmf4f+nk1HwTUbW8+Xf+3pOa7PvduemytlkyuL1PBBb03Du7/KuHytX5yV9SZrYkxt13GAZOZNMLSFXH3tBsOdB5nvTxyI9bb05ncEf5a4O8ZQOI6oF4H25bRvwgipC4w/Ev71ztig1ePwQQwYCIitTXoKvULMTs2jhH9IEn/hGb2dhhKNIlZ0Xkc8Oz3QJA9QxDI0a9hEs4HfaX9jvSbh6K6mc8r6kGgVjPg3yeBuAn2r8tYXpbzeTiq2v3SSb7bP6VRuI2Zqz0y2SdWvj/ONLfK1XCW2Nh86qiGD0n7p2FPGARjCZ9bXuYBvaEaYkZJFxjYw18L3N/dvmU8EAMmIlJfs4HSyVqOfsCkf7n+c7vMJXa6WM5x4KSqf+NYUWKYhyNNchP2Svfu6vNK2TSDW7Hc88THUn8gwKgWz4l9qORNmFs8Ln+zYn0aDfDoe9Lo6JbI1Wwq0gfNjDCZ5rpA44sbFKZfU6n/+dYwE1iW0u+0HlrPfJryfHXfPQyYiMh76AcNVRsA/edKHX7Njhru4homV9C/zFyUGNUwOXC4DgyW7mUXXAOYkQu8fEXqE6R/1dwTHwOtn5Ku5lOS/gjXHcdI620/0rG8gkKB5/8EardRpmxCmNZKVjDuj2WlSa71vVuQ1I2xfZ1jt0o1NT2SraeL7CrVjpU2izqi9P5+larfm2ApCDYatqNymGlepfd8HL4GeOAflq947PR/Vgrkhb9FM9jpm4i8h79ep1GNH9BxtHplcTkBwxONkzVm+h1umw0E2j4tjX/U2kyNUym52hBr9GsyKteUbtDqTD8s/wDlLksXJVKwrdEb0TvhM2D1KOmqNcDCOf7exP5vA1HdgSZ23CC4TlvpIae0dswZSTuAs3uAJv2A3CPSGFpv3rsJtMGFExogsApQVCAFkM/tAk7vAFbrBbalfcgaPyw9fBgDJiLyHA0elJoGLDXPBVeXxqbR+BveDsQctf/U2j1ukRGTGia9gCmwsnQ1lqP8/IFBC+TTNY4HHltgX8f8UsaXkCsyaKFCzax+/kCVMClIKlWnLTA5Uy+RlRqmwGCg7VBlyiKn+v32L1O5ZtnwB8Yjf+t/lhoN8O8TQMldabDWCkGm975TtI+ed2OTHBF5joBAaQDJIUstp3loBtBTpllDTU+vBXpOB5o96lw+Jn1o9IKFkRukS7tHfevcOuRoNEC7px1rCrN0JZ4z+r0uPZeOA+WI1gkWLhIw0nWK9Kx/hZgaV3lVux9I/Mr5fMZulb6XXf5uOD1Aa3QDaCfCgqGrpKswWxoNg1A/1vE8PYjqAdPChQsRFRWFoKAgxMTEYPt2yzcOXLt2LXr37o2aNWsiJCQEcXFx2LTJ9B5Ga9asQfPmzaHVatG8eXN89ZUCXzYicg/FTkoqVTE16gV0/7dzN5AFpBqm++pLQxc07GV4eX3dGOmGqw0ecG4drtQ5SbqkP2mncnlGdJL6Yj003bHlu0wCnlhs23esZhNg+gWpeSxmlFTzaXHQShdr+JDUdOaMOm2l76XcWEjGAZOtg54CQNO+wJBPDS/IeGCqdIPgckDVgGnVqlWYMmUKpk+fjoyMDHTr1g39+vVDVpb5y1G3bduG3r17Y+PGjUhPT0fPnj0xcOBAZGRk6NLs3r0bCQkJSExMxIEDB5CYmIghQ4Zg79697tosIvIE+leceZPSDtkNe0on9me+Mz/SuafzryAFKPYM4mgLRwY/jB4gPXews89b6T0FB74r1Xw6MhK8Utz1+ds0gKdsJmUvH54JVK7lVJE8hUYIV107Ka9z585o3749Fi0quylls2bNMGjQIKSkpNiUR4sWLZCQkICXX5YuH01ISEB+fj6+++47XZq+ffuiatWqWLFihU155ufnIzQ0FHl5eQgJYfstkVeZFSo9P/q+1CxUOVzq++Qtim4Ct64CoXXVLkn5IQRw55brL9l3pZQIoDBfej0rz3XrEQKYfV/Z+4HvSjVs9vhuGrD33nndXFlLf6OW5jvBledv1WqYioqKkJ6ejvj4eIPp8fHx2LXL3JgqpkpKSlBQUIBq1cqq/3bv3m2SZ58+fazmWVhYiPz8fIMHEXk7Id3I1JuCJUA6qTNYUpZG493BEgC3jSumX8PUJwVoN8JyWlvyKEdUC5guXbqE4uJihIUZjvsQFhaGnJwcm/J4++23cePGDQwZMkQ3LScnx+48U1JSEBoaqntERDh5dQsRqad0LBlbbyxK5A3q3RvzKSDIfetsNtCxvnhKDf/gYVQfVkBjFIkKIUymmbNixQrMmjUL69evR61ahu2j9uaZnJyMqVPLhn7Pz89n0ETkrSYflJouykm/CSIAwOMfAjvfdXzwT3v8bTlwOw+4z8HzYDmtYVItYKpRowb8/f1Nan5yc3NNaoiMrVq1CqNHj8bq1avx8MOGA2mFh4fbnadWq4VW6913USaie0rHkyEqTyrXMrzFjSvp33vQEf7l83yqWr1ZYGAgYmJikJaWZjA9LS0NXbp0sbjcihUrMGrUKCxfvhz9+5t+qHFxcSZ5bt682WqeREREpJC4CUDNaKDnDLVLoihVm+SmTp2KxMREdOjQAXFxcVi8eDGysrKQlJQEQGoqO3fuHJYulQaxW7FiBUaMGIF3330XsbGxupqkihUrIjRU6nU/efJkPPjgg3j99dfx2GOPYf369diyZQt27NihzkYSERH5kkrVpBs/WzJwPvD1360PUOuBVB1WAJAGrnzjjTeQnZ2Nli1b4p133sGDDz4IABg1ahROnz6Nn3/+GQDQo0cPbN261SSPkSNHIjU1Vff+yy+/xIwZM3Dy5Ek0bNgQr7zyCp544gmby8RhBYiIiFzobpHyN3yGa8/fqgdMnogBExERkfcpl+MwEREREXkLBkxEREREMhgwEREREclgwEREREQkgwETERERkQwGTEREREQyGDARERERyWDARERERCSDARMRERGRDAZMRERERDIYMBERERHJYMBEREREJIMBExEREZEMBkxEREREMhgwEREREclgwEREREQkgwETERERkQwGTEREREQyGDARERERyWDARERERCSDARMRERGRDAZMRERERDIYMBERERHJYMBEREREJIMBExEREZEMBkxEREREMhgwEREREclgwEREREQkgwETERERkQwGTEREREQyGDARERERyWDARERERCSDARMRERGRDAZMRERERDIYMBERERHJUD1gWrhwIaKiohAUFISYmBhs377dYtrs7GwMGzYMTZs2hZ+fH6ZMmWKSJjU1FRqNxuRx+/ZtF24FERERlWeqBkyrVq3ClClTMH36dGRkZKBbt27o168fsrKyzKYvLCxEzZo1MX36dLRp08ZiviEhIcjOzjZ4BAUFuWoziIiIqJxTNWCaO3cuRo8ejTFjxqBZs2aYN28eIiIisGjRIrPpGzRogHfffRcjRoxAaGioxXw1Gg3Cw8MNHkRERESOUi1gKioqQnp6OuLj4w2mx8fHY9euXU7lff36dURGRqJevXoYMGAAMjIynMqPiIiIfJtqAdOlS5dQXFyMsLAwg+lhYWHIyclxON/o6GikpqZiw4YNWLFiBYKCgtC1a1ccP37c4jKFhYXIz883eBARERGVUr3Tt0ajMXgvhDCZZo/Y2Fg8/fTTaNOmDbp164YvvvgCTZo0wXvvvWdxmZSUFISGhuoeERERDq+fiIiIyh/VAqYaNWrA39/fpDYpNzfXpNbJGX5+fujYsaPVGqbk5GTk5eXpHmfPnlVs/UREROT9VAuYAgMDERMTg7S0NIPpaWlp6NKli2LrEUIgMzMTtWvXtphGq9UiJCTE4EFERERUKkDNlU+dOhWJiYno0KED4uLisHjxYmRlZSEpKQmAVPNz7tw5LF26VLdMZmYmAKlj98WLF5GZmYnAwEA0b94cADB79mzExsaicePGyM/Px/z585GZmYkFCxa4ffuIiIiofFA1YEpISMDly5cxZ84cZGdno2XLlti4cSMiIyMBSANVGo/J1K5dO93r9PR0LF++HJGRkTh9+jQA4Nq1axg7dixycnIQGhqKdu3aYdu2bejUqZPbtouIiIjKF40QQqhdCE+Tn5+P0NBQ5OXlsXmOiIjIS7jy/K36VXJEREREno4BExEREZEMBkxEREREMhgwEREREclgwEREREQkgwETERERkQwGTEREREQyGDARERERyWDARERERCSDARMRERGRDAZMRERERDIYMBERERHJYMBEREREJIMBExEREZEMBkxEREREMhgwEREREclgwEREREQkgwETERERkQwGTEREREQyGDARERERyWDARERERCSDARMRERGRDAZMRERERDIYMBERERHJYMBEREREJIMBExEREZEMBkxEREREMhgwEREREclgwEREREQkgwETERERkQwGTEREREQyGDARERERyWDARERERCSDARMRERGRDAZMRERERDJUD5gWLlyIqKgoBAUFISYmBtu3b7eYNjs7G8OGDUPTpk3h5+eHKVOmmE23Zs0aNG/eHFqtFs2bN8dXX33lotITERGRL1A1YFq1ahWmTJmC6dOnIyMjA926dUO/fv2QlZVlNn1hYSFq1qyJ6dOno02bNmbT7N69GwkJCUhMTMSBAweQmJiIIUOGYO/eva7cFCIiIirHNEIIodbKO3fujPbt22PRokW6ac2aNcOgQYOQkpJiddkePXqgbdu2mDdvnsH0hIQE5Ofn47vvvtNN69u3L6pWrYoVK1bYVK78/HyEhoYiLy8PISEhtm8QERERqcaV52/VapiKioqQnp6O+Ph4g+nx8fHYtWuXw/nu3r3bJM8+ffo4lScRERH5tgC1Vnzp0iUUFxcjLCzMYHpYWBhycnIczjcnJ8fuPAsLC1FYWKh7n5+f7/D6iYiIqPxRvdO3RqMxeC+EMJnm6jxTUlIQGhqqe0RERDi1fiIiIipfVAuYatSoAX9/f5Oan9zcXJMaInuEh4fbnWdycjLy8vJ0j7Nnzzq8fiIiIip/VAuYAgMDERMTg7S0NIPpaWlp6NKli8P5xsXFmeS5efNmq3lqtVqEhIQYPIiIiIhKqdaHCQCmTp2KxMREdOjQAXFxcVi8eDGysrKQlJQEQKr5OXfuHJYuXapbJjMzEwBw/fp1XLx4EZmZmQgMDETz5s0BAJMnT8aDDz6I119/HY899hjWr1+PLVu2YMeOHW7fPiIiIiofVA2YEhIScPnyZcyZMwfZ2dlo2bIlNm7ciMjISADSQJXGYzK1a9dO9zo9PR3Lly9HZGQkTp8+DQDo0qULVq5ciRkzZuCll15Cw4YNsWrVKnTu3Nlt20VERETli6rjMHkqjsNERETkfcrlOExERERE3oIBExEREZEMBkxEREREMhgwEREREclgwEREREQkgwETERERkQwGTEREREQyGDARERERyWDARERERCRD1VujeKrSwc/z8/NVLgkRERHZqvS87YqbmDBgMqOgoAAAEBERoXJJiIiIyF4FBQUIDQ1VNE/eS86MkpISnD9/HlWqVIFGo1E07/z8fERERODs2bM+f5867osy3BcS7ocy3BdluC/KcF+UMbcvhBAoKChAnTp14OenbK8j1jCZ4efnh3r16rl0HSEhIT7/ZS/FfVGG+0LC/VCG+6IM90UZ7osyxvtC6ZqlUuz0TURERCSDARMRERGRDAZMbqbVajFz5kxotVq1i6I67osy3BcS7ocy3BdluC/KcF+Ucfe+YKdvIiIiIhmsYSIiIiKSwYCJiIiISAYDJiIiIiIZDJiIiIiIZDBgcqOFCxciKioKQUFBiImJwfbt29UuktO2bduGgQMHok6dOtBoNFi3bp3BfCEEZs2ahTp16qBixYro0aMHfvvtN4M0hYWFmDRpEmrUqIHg4GA8+uij+OuvvwzSXL16FYmJiQgNDUVoaCgSExNx7do1F2+d7VJSUtCxY0dUqVIFtWrVwqBBg3Ds2DGDNL6yLxYtWoTWrVvrBpOLi4vDd999p5vvK/vBWEpKCjQaDaZMmaKb5iv7YtasWdBoNAaP8PBw3Xxf2Q+lzp07h6effhrVq1dHpUqV0LZtW6Snp+vm+8r+aNCggcn3QqPRYMKECQA8cD8IcouVK1eKChUqiI8++kgcOXJETJ48WQQHB4szZ86oXTSnbNy4UUyfPl2sWbNGABBfffWVwfzXXntNVKlSRaxZs0YcOnRIJCQkiNq1a4v8/HxdmqSkJFG3bl2RlpYm9u/fL3r27CnatGkj7t69q0vTt29f0bJlS7Fr1y6xa9cu0bJlSzFgwAB3baasPn36iCVLlojDhw+LzMxM0b9/f1G/fn1x/fp1XRpf2RcbNmwQ3377rTh27Jg4duyYePHFF0WFChXE4cOHhRC+sx/07du3TzRo0EC0bt1aTJ48WTfdV/bFzJkzRYsWLUR2drbukZubq5vvK/tBCCGuXLkiIiMjxahRo8TevXvFqVOnxJYtW8Sff/6pS+Mr+yM3N9fgO5GWliYAiJ9++kkI4Xn7gQGTm3Tq1EkkJSUZTIuOjhbTpk1TqUTKMw6YSkpKRHh4uHjttdd0027fvi1CQ0PFBx98IIQQ4tq1a6JChQpi5cqVujTnzp0Tfn5+4vvvvxdCCHHkyBEBQOzZs0eXZvfu3QKA+P333128VY7Jzc0VAMTWrVuFEL69L4QQomrVquLjjz/2yf1QUFAgGjduLNLS0kT37t11AZMv7YuZM2eKNm3amJ3nS/tBCCFeeOEF8cADD1ic72v7Q9/kyZNFw4YNRUlJiUfuBzbJuUFRURHS09MRHx9vMD0+Ph67du1SqVSud+rUKeTk5Bhst1arRffu3XXbnZ6ejjt37hikqVOnDlq2bKlLs3v3boSGhqJz5866NLGxsQgNDfXY/ZeXlwcAqFatGgDf3RfFxcVYuXIlbty4gbi4OJ/cDxMmTED//v3x8MMPG0z3tX1x/Phx1KlTB1FRUfjb3/6GkydPAvC9/bBhwwZ06NABTz31FGrVqoV27drho48+0s33tf1RqqioCMuWLcOzzz4LjUbjkfuBAZMbXLp0CcXFxQgLCzOYHhYWhpycHJVK5Xql22Ztu3NychAYGIiqVataTVOrVi2T/GvVquWR+08IgalTp+KBBx5Ay5YtAfjevjh06BAqV64MrVaLpKQkfPXVV2jevLnP7YeVK1di//79SElJMZnnS/uic+fOWLp0KTZt2oSPPvoIOTk56NKlCy5fvuxT+wEATp48iUWLFqFx48bYtGkTkpKS8Pe//x1Lly4F4FvfC33r1q3DtWvXMGrUKACeuR8C7EpNTtFoNAbvhRAm08ojR7bbOI259J66/yZOnIiDBw9ix44dJvN8ZV80bdoUmZmZuHbtGtasWYORI0di69atuvm+sB/Onj2LyZMnY/PmzQgKCrKYzhf2Rb9+/XSvW7Vqhbi4ODRs2BCffvopYmNjAfjGfgCAkpISdOjQAa+++ioAoF27dvjtt9+waNEijBgxQpfOV/ZHqU8++QT9+vVDnTp1DKZ70n5gDZMb1KhRA/7+/ibRbG5urkn0XJ6UXgVjbbvDw8NRVFSEq1evWk1z4cIFk/wvXrzocftv0qRJ2LBhA3766SfUq1dPN93X9kVgYCAaNWqEDh06ICUlBW3atMG7777rU/shPT0dubm5iImJQUBAAAICArB161bMnz8fAQEBunL6wr4wFhwcjFatWuH48eM+9Z0AgNq1a6N58+YG05o1a4asrCwAvnesAIAzZ85gy5YtGDNmjG6aJ+4HBkxuEBgYiJiYGKSlpRlMT0tLQ5cuXVQqletFRUUhPDzcYLuLioqwdetW3XbHxMSgQoUKBmmys7Nx+PBhXZq4uDjk5eVh3759ujR79+5FXl6ex+w/IQQmTpyItWvX4scff0RUVJTBfF/aF+YIIVBYWOhT+6FXr144dOgQMjMzdY8OHTpg+PDhyMzMxP333+8z+8JYYWEhjh49itq1a/vUdwIAunbtajLkyB9//IHIyEgAvnmsWLJkCWrVqoX+/fvrpnnkfrCrizg5rHRYgU8++UQcOXJETJkyRQQHB4vTp0+rXTSnFBQUiIyMDJGRkSEAiLlz54qMjAzdcAmvvfaaCA0NFWvXrhWHDh0SQ4cONXtZaL169cSWLVvE/v37xUMPPWT2stDWrVuL3bt3i927d4tWrVp51OWxzz33nAgNDRU///yzwWWyN2/e1KXxlX2RnJwstm3bJk6dOiUOHjwoXnzxReHn5yc2b94shPCd/WCO/lVyQvjOvvjnP/8pfv75Z3Hy5EmxZ88eMWDAAFGlShXd8c9X9oMQ0hATAQEB4pVXXhHHjx8Xn3/+uahUqZJYtmyZLo0v7Y/i4mJRv3598cILL5jM87T9wIDJjRYsWCAiIyNFYGCgaN++ve6Sc2/2008/CQAmj5EjRwohpEtkZ86cKcLDw4VWqxUPPvigOHTokEEet27dEhMnThTVqlUTFStWFAMGDBBZWVkGaS5fviyGDx8uqlSpIqpUqSKGDx8url696qatlGduHwAQS5Ys0aXxlX3x7LPP6r7nNWvWFL169dIFS0L4zn4wxzhg8pV9UTp+ToUKFUSdOnXEE088IX777TfdfF/ZD6W+/vpr0bJlS6HVakV0dLRYvHixwXxf2h+bNm0SAMSxY8dM5nnaftAIIYR9dVJEREREvoV9mIiIiIhkMGAiIiIiksGAiYiIiEgGAyYiIiIiGQyYiIiIiGQwYCIiIiKSwYCJiIiISAYDJiIiCzQaDdatW6d2MYjIAzBgIiKPNGrUKGg0GpNH37591S4aEfmgALULQERkSd++fbFkyRKDaVqtVqXSEJEvYw0TEXksrVaL8PBwg0fVqlUBSM1lixYtQr9+/VCxYkVERUVh9erVBssfOnQIDz30ECpWrIjq1atj7NixuH79ukGa//3vf2jRogW0Wi1q166NiRMnGsy/dOkSHn/8cVSqVAmNGzfGhg0bdPOuXr2K4cOHo2bNmqhYsSIaN25sEuARUfnAgImIvNZLL72EJ598EgcOHMDTTz+NoUOH4ujRowCAmzdvom/fvqhatSp++eUXrF69Glu2bDEIiBYtWoQJEyZg7NixOHToEDZs2IBGjRoZrGP27NkYMmQIDh48iEceeQTDhw/HlStXdOs/cuQIvvvuOxw9ehSLFi1CjRo13LcDiMh97L5dLxGRG4wcOVL4+/uL4OBgg8ecOXOEEEIAEElJSQbLdO7cWTz33HNCCCEWL14sqlatKq5fv66b/+233wo/Pz+Rk5MjhBCiTp06Yvr06RbLAEDMmDFD9/769etCo9GI7777TgghxMCBA8UzzzyjzAYTkUdjHyYi8lg9e/bEokWLDKZVq1ZN9zouLs5gXlxcHDIzMwEAR48eRZs2bRAcHKyb37VrV5SUlODYsWPQaDQ4f/48evXqZbUMrVu31r0ODg5GlSpVkJubCwB47rnn8OSTT2L//v2Ij4/HoEGD0KVLF4e2lYg8GwMmIvJYwcHBJk1kcjQaDQBACKF7bS5NxYoVbcqvQoUKJsuWlJQAAPr164czZ87g22+/xZYtW9CrVy9MmDABb731ll1lJiLPxz5MROS19uzZY/I+OjoaANC8eXNkZmbixo0buvk7d+6En58fmjRpgipVqqBBgwb44YcfnCpDzZo1MWrUKCxbtgzz5s3D4sWLncqPiDwTa5iIyGMVFhYiJyfHYFpAQICuY/Xq1avRoUMHPPDAA/j888+xb98+fPLJJwCA4cOHY+bMmRg5ciRmzZqFixcvYtKkSUhMTERYWBgAYNasWUhKSkKtWrXQr18/FBQUYOfOnZg0aZJN5Xv55ZcRExODFi1aoLCwEN988w2aNWum4B4gIk/BgImIPNb333+P2rVrG0xr2rQpfv/9dwDSFWwrV67E+PHjER4ejs8//xzNmzcHAFSqVAmbNm3C5MmT0bFjR1SqVAlPPvkk5s6dq8tr5MiRuH37Nt555x08//zzqFGjBgYPHmxz+QIDA5GcnIzTp0+jYsWK6NatG1auXKnAlhORp9EIIYTahSAispdGo8FXX32FQYMGqV0UIvIB7MNEREREJIMBExEREZEM9mEiIq/E3gRE5E6sYSIiIiKSwYCJiIiISAYDJiIiIiIZDJiIiIiIZDBgIiIiIpLBgImIiIhIBgMmIiIiIhkMmIiIiIhkMGAiIiIikvH/CZdhO1MPbqUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|███████████████████████████████████████▌    | 9/10 [01:36<00:10, 10.64s/it]"
     ]
    }
   ],
   "source": [
    "def train(config = None):\n",
    "    \n",
    "    with wandb.init(config = config):\n",
    "        #pprint.pprint(sweep_config)        \n",
    "        config = wandb.config\n",
    "        neuralNet = FFNet(0, len(trainx[0]), 10)\n",
    "        for layer in range(config.number_of_layers):\n",
    "            neuralNet.addHiddenLayer(config.fc_layer_size, config.initialization)\n",
    "        neuralNet.addOutputLayer(10, config.initialization)\n",
    "        neuralNet.solidify()\n",
    "        config.optimizer = \"relu\"\n",
    "        weights, biases = neuralNet.fit(config.optimizer,config.batchSize, config.learningRate, config.activation, trainx, train_y, config.decay, config.epochs)\n",
    "        print(Algorithms.evaluateNetwork(weights,biases,testx, test_y))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.agent(sweep_id, train, count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "TfO2B6v6BVWj"
   },
   "outputs": [],
   "source": [
    "class PreProc:\n",
    "    def __init__(self):\n",
    "        (self.trainx,self.trainy),(self.testx, self.testy) = fashion_mnist.load_data()\n",
    "        \n",
    "    def visualize(self,n):\n",
    "        for i in range(n):\n",
    "            plt.subplot(330+1+i) # ask someone why??\n",
    "            plt.imshow(self.trainx[i], cmap = plt.get_cmap('gray'))\n",
    "        plt.show()\n",
    "        \n",
    "    def flattenAndCentralize(self):\n",
    "        trainx_flattened = np.copy(self.trainx).astype('float64')\n",
    "        testx_flattened = np.copy(self.testx).astype('float64')\n",
    "        trainx_flattened -= np.mean(trainx_flattened, axis = 0)\n",
    "        testx_flattened -= np.mean(testx_flattened, axis = 0)\n",
    "        trainx_flattened.shape = (60000,784)\n",
    "        testx_flattened.shape = (10000,784)\n",
    "        return trainx_flattened,testx_flattened\n",
    "    \n",
    "\n",
    "    \n",
    "    def getLabels(self):\n",
    "        return self.trainy, self.testy\n",
    "    \n",
    "    def getInputSize(self):\n",
    "        return len(self.trainx[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "4HhzRi0xhOLc"
   },
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "7BwoVy0bPFA0"
   },
   "outputs": [],
   "source": [
    "class Functions:\n",
    "    @staticmethod\n",
    "    def sigmoid(input):\n",
    "        input = np.clip(input, -100,100)\n",
    "        return  1.0/(1.0+np.exp(-input))\n",
    "    \n",
    "    @staticmethod\n",
    "    def reLU(input):\n",
    "        return np.maximum(0.01*input,input)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tanh(input):\n",
    "        return np.tanh(input)\n",
    "    \n",
    "    @staticmethod\n",
    "    def identity(input):\n",
    "        return input\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax(input):\n",
    "        input = np.clip(input, -100,100)\n",
    "        return np.exp(input)/(np.sum(np.exp(input)))\n",
    "    \n",
    "    @staticmethod\n",
    "    def onehot(input):\n",
    "        result = np.zeros(10)\n",
    "        result[input] = 1\n",
    "        return result\n",
    "    \n",
    "    @staticmethod\n",
    "    def crossEntropyLoss(y,yHat):\n",
    "        loss = (-1/10.0) * np.sum(np.multiply(y, np.log(yHat+1e-10)) + np.multiply((1 - y), np.log(1 - (yHat+1e-10))))\n",
    "        return loss\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative_sigmoid(input):\n",
    "        return Functions.sigmoid(input)*(1-Functions.sigmoid(input))\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative_tanh(input):\n",
    "        return (1 - (np.tanh(input)**2))\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative_reLU(input):\n",
    "        return np.where(input > 0, 1, 0.01)\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative_identity(input):\n",
    "        return 1\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot(input):\n",
    "        plt.plot(input)\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Loss over iterations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "Bu5XtsgmjyaH"
   },
   "outputs": [],
   "source": [
    "class Algorithms:\n",
    "    @staticmethod\n",
    "    def ForwardProp(weights, bias, activate, output, inputLayer):\n",
    "        L = len(weights)-1\n",
    "        a = []\n",
    "        h = []\n",
    "        a.append(np.matmul(weights[0],inputLayer)+bias[0])\n",
    "        h.append(activate(a[0]))\n",
    "        for k in range(1,L):\n",
    "            a.append(np.matmul(weights[k],h[k-1].T)+bias[k])\n",
    "            h.append(activate(a[k]))\n",
    "        a.append(np.matmul(weights[L],h[L-1].T)+bias[L])\n",
    "        h.append(output(a[L]))\n",
    "        return a,h\n",
    "    @staticmethod\n",
    "    def BackProp(weights, biases, a, h, derivative, dataPoint, dataLabel):\n",
    "        L = len(weights)-1\n",
    "        gradaL = -(Functions.onehot(dataLabel)-h[len(h)-1])\n",
    "        dw = np.zeros_like(weights)\n",
    "        db = np.zeros_like(biases)\n",
    "        for k in range(L,0,-1):\n",
    "            gradW = np.outer(gradaL, h[k-1].T)\n",
    "            gradB = gradaL\n",
    "            dw[k] = gradW\n",
    "            db[k] = gradB\n",
    "\n",
    "            gradhL_1 = np.matmul(np.transpose(weights[k]),gradaL)\n",
    "            gradaL_1 = np.multiply(gradhL_1, derivative(a[k-1]))\n",
    "            gradaL = gradaL_1\n",
    "        dw[0] = np.outer(gradaL,dataPoint.T)\n",
    "        db[0] = gradaL\n",
    "        return dw, db\n",
    "\n",
    "    @staticmethod\n",
    "    def miniBatchMGD(weights, biases, batchSize, learningRate, activate, output, derivative, dataPoints, dataLabels, valTest_x, valTest_y, decay, epochs):\n",
    "        lossTrack = []\n",
    "        LOSS = 0.0\n",
    "        validation = []\n",
    "        beta = 0.9\n",
    "        prevWeights = np.zeros_like(weights)\n",
    "        prevBiases = np.zeros_like(biases)\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            indices = np.arange(len(dataPoints))\n",
    "            np.random.shuffle(indices)\n",
    "            batchX = dataPoints[indices]\n",
    "            batchY = dataLabels[indices]\n",
    "            for i in range(math.floor(len(dataPoints)/batchSize)):\n",
    "                trainer = batchX[i*batchSize:i*batchSize+batchSize]\n",
    "                labeler = batchY[i*batchSize:i*batchSize+batchSize]\n",
    "                batchLoss = 0.0\n",
    "                dw = np.zeros_like(weights)\n",
    "                db = np.zeros_like(biases)\n",
    "                for data in range(batchSize):\n",
    "                    a,h = Algorithms.ForwardProp(weights, biases , activate, output, trainer[data])\n",
    "                    currWeights, currBiases = Algorithms.BackProp(weights, biases, a, h, derivative, trainer[data], labeler[data])\n",
    "                    batchLoss += Functions.crossEntropyLoss(Functions.onehot(labeler[data]), h[-1])\n",
    "                    dw += currWeights\n",
    "                    db += currBiases\n",
    "                batchLoss /= batchSize\n",
    "                dw /= batchSize\n",
    "                db /= batchSize\n",
    "                momentumWeights = prevWeights*beta + dw*1.0\n",
    "                momentumBiases = prevBiases*beta + db*1.0\n",
    "                weights -= learningRate*(momentumWeights + decay*weights)\n",
    "                biases -= learningRate*(momentumBiases + decay*biases)\n",
    "                prevWeights = momentumWeights\n",
    "                prevBiases = momentumBiases\n",
    "                lossTrack.append(batchLoss)\n",
    "                \n",
    "            wandb.log({'valAcc':(Algorithms.evaluateNetwork(weights, biases, activate, output, valTest_x, valTest_y)), 'epoch':epoch, 'loss':lossTrack[-1]})\n",
    "            validation.append(Algorithms.evaluateNetwork(weights, biases, activate, output, valTest_x, valTest_y))\n",
    "            #print(\"The val_acc after this epoch is: \"+ str(validation))\n",
    "        #Functions.plot(validation)\n",
    "        return weights, biases\n",
    "\n",
    "    @staticmethod\n",
    "    def ADAM(weights, biases, batchSize, learningRate, activate, output, derivative, dataPoints, dataLabels, valTest_x, valTest_y, decay, epochs):\n",
    "        validation = []\n",
    "        prev_val = 0.0\n",
    "        lossTrack = []\n",
    "        beta1 = 0.9\n",
    "        beta2 = 0.99\n",
    "        epsilon = 0.000001\n",
    "        patience = 3\n",
    "        m_w = np.zeros_like(weights)\n",
    "        v_w = np.zeros_like(weights)\n",
    "        m_b = np.zeros_like(biases)\n",
    "        v_b = np.zeros_like(biases)\n",
    "        learnerRateW = np.full_like(weights, learningRate)\n",
    "        learnerRateB = np.full_like(biases, learningRate)\n",
    "        i = 0\n",
    "        \n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            indices = np.arange(len(dataPoints))\n",
    "            np.random.shuffle(indices)\n",
    "            batchX = dataPoints[indices]\n",
    "            batchY = dataLabels[indices]\n",
    "            for i in range(math.floor(len(dataPoints)/batchSize)):\n",
    "                trainer = batchX[i*batchSize:i*batchSize+batchSize]\n",
    "                labeler = batchY[i*batchSize:i*batchSize+batchSize]\n",
    "                batchLoss = 0.0\n",
    "                dw = np.zeros_like(weights)\n",
    "                db = np.zeros_like(biases)\n",
    "                for data in range(batchSize):\n",
    "                    a,h = Algorithms.ForwardProp(weights, biases,activate, output, trainer[data])\n",
    "                    currWeights, currBiases = Algorithms.BackProp(weights, biases , a, h, derivative, trainer[data], labeler[data])\n",
    "                    batchLoss += Functions.crossEntropyLoss(Functions.onehot(labeler[data]), h[-1])\n",
    "                    \n",
    "                    dw += currWeights\n",
    "                    db += currBiases\n",
    "                batchLoss /= batchSize\n",
    "                dw /= batchSize\n",
    "                db /= batchSize\n",
    "                m_w = beta1*m_w + (1-beta1)*dw\n",
    "                m_b = beta1*m_b + (1-beta1)*db\n",
    "                v_w = v_w*beta2 + (1-beta2)*dw**2\n",
    "                v_b = v_b*beta2 + (1-beta2)*db**2\n",
    "                \n",
    "                m_w_hat = m_w/(1 - np.power(beta1, i+1))\n",
    "                m_b_hat = m_b/(1 - np.power(beta1, i+1))\n",
    "                v_w_hat = v_w/(1 - np.power(beta2, i+1))\n",
    "                v_b_hat = v_b/(1 - np.power(beta2, i+1))\n",
    "                \n",
    "                i+=1\n",
    "                \n",
    "                tempW = np.zeros_like(m_w)\n",
    "                tempB = np.zeros_like(m_b)\n",
    "                for i in range(len(dw)):\n",
    "                    tempW[i] = np.sqrt(v_w_hat[i])\n",
    "                    tempB[i] = np.sqrt(v_b_hat[i])\n",
    "                weights = weights - ((learnerRateW*(dw + decay*weights))/(tempW + epsilon))\n",
    "                biases = biases - (learnerRateB*(db + decay*biases))/(tempB+epsilon)\n",
    "\n",
    "                lossTrack.append(batchLoss)\n",
    "            \n",
    "            #wandb.log({'valAcc':(Algorithms.evaluateNetwork(weights, biases, activate, output, valTest_x, valTest_y)), 'epoch':epoch, 'loss':lossTrack[-1]})\n",
    "            #wandb.log({'valAcc':(Algorithms.evaluateNetwork(weights, biases, valTest_x, valTest_y)), 'epoch':epoch})\n",
    "            validation.append(Algorithms.evaluateNetwork(weights, biases, activate, output, valTest_x, valTest_y))\n",
    "            if validation[-1] <= prev_val+1e-2:\n",
    "                patience -= 1\n",
    "            else:\n",
    "                patience = 3\n",
    "                prev_weights = weights\n",
    "                prev_biases = biases\n",
    "                prev_val = validation[-1]\n",
    "            \n",
    "            if patience == 0:\n",
    "                #print(\"decreasing learning rate in \"+ str(epoch))\n",
    "                #learningRate *= 1.5\n",
    "                weights = prev_weights\n",
    "                biases = prev_biases\n",
    "            #print(\"The val_acc after this epoch is: \"+ str(validation))\n",
    "        #Functions.plot(validation)\n",
    "        if prev_val > validation[-1]:\n",
    "            return prev_weights, prev_biases\n",
    "            #print(\"The loss after this epoch is: \"+ str(batchLoss))\n",
    "        return weights,biases\n",
    "\n",
    "    @staticmethod\n",
    "    def NADAM(weights, biases, batchSize, learningRate, activate, output, derivative, dataPoints, dataLabels, valTest_x, valTest_y, decay, epochs):\n",
    "        patience = 3\n",
    "        lossTrack = []\n",
    "        validation = []\n",
    "        prev_val = 0.0\n",
    "        prevWeights = np.zeros_like(weights)\n",
    "        prevBiases = np.zeros_like(biases)\n",
    "        beta1 = 0.9\n",
    "        beta2 = 0.99\n",
    "        epsilon = 0.000001\n",
    "        prev_weights = np.zeros_like(weights)\n",
    "        prev_biases = np.zeros_like(biases)\n",
    "        m_w = np.zeros_like(weights)\n",
    "        v_w = np.zeros_like(weights)\n",
    "        m_b = np.zeros_like(biases)\n",
    "        v_b = np.zeros_like(biases)\n",
    "        learnerRateW = np.full_like(weights, learningRate)\n",
    "        learnerRateB = np.full_like(biases, learningRate)\n",
    "        i = 0\n",
    "        \n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            indices = np.arange(len(dataPoints))\n",
    "            np.random.shuffle(indices)\n",
    "            batchX = dataPoints[indices]\n",
    "            batchY = dataLabels[indices]\n",
    "            for i in range(math.floor(len(dataPoints)/batchSize)):\n",
    "                trainer = batchX[i*batchSize:i*batchSize+batchSize]\n",
    "                labeler = batchY[i*batchSize:i*batchSize+batchSize]\n",
    "                batchLoss = 0.0\n",
    "                dw = np.zeros_like(weights)\n",
    "                db = np.zeros_like(biases)\n",
    "                for data in range(batchSize):\n",
    "                    a,h = Algorithms.ForwardProp(weights - v_w*(beta1), biases - v_b*(beta1), activate, output, trainer[data])\n",
    "                    currWeights, currBiases = Algorithms.BackProp(weights - v_w*(beta1), biases - v_b*(beta1), a, h, derivative, trainer[data], labeler[data])\n",
    "                    batchLoss += Functions.crossEntropyLoss(Functions.onehot(labeler[data]), h[-1])\n",
    "                    \n",
    "                    dw += currWeights\n",
    "                    db += currBiases\n",
    "                batchLoss /= batchSize\n",
    "                dw /= batchSize\n",
    "                db /= batchSize\n",
    "                m_w = beta1*m_w + (1-beta1)*dw\n",
    "                m_b = beta1*m_b + (1-beta1)*db\n",
    "                v_w = v_w*beta2 + (1-beta2)*dw**2\n",
    "                v_b = v_b*beta2 + (1-beta2)*db**2\n",
    "                \n",
    "                m_w_hat = m_w/(1 - np.power(beta1, i+1))\n",
    "                m_b_hat = m_b/(1 - np.power(beta1, i+1))\n",
    "                v_w_hat = v_w/(1 - np.power(beta2, i+1))\n",
    "                v_b_hat = v_b/(1 - np.power(beta2, i+1))\n",
    "                \n",
    "                i+=1\n",
    "                \n",
    "                tempW = np.zeros_like(m_w)\n",
    "                tempB = np.zeros_like(m_b)\n",
    "                for j in range(len(dw)):\n",
    "                    tempW[j] = np.sqrt(v_w_hat[j])\n",
    "                    tempB[j] = np.sqrt(v_b_hat[j])\n",
    "                weights = weights - ((learnerRateW*(dw + decay*weights))/(tempW + epsilon))\n",
    "                biases = biases - (learnerRateB*(db + decay*biases))/(tempB+epsilon)\n",
    "                lossTrack.append(batchLoss)\n",
    "            #wandb.log({'valAcc':(Algorithms.evaluateNetwork(weights, biases, valTest_x, valTest_y)), 'epoch':epoch})\n",
    "            validation.append(Algorithms.evaluateNetwork(weights, biases, activate, output, valTest_x, valTest_y))\n",
    "            if validation[-1] <= prev_val:\n",
    "                patience -= 1\n",
    "            else:\n",
    "                patience = 3\n",
    "                prev_weights = weights\n",
    "                prev_biases = biases\n",
    "                prev_val = validation[-1]\n",
    "            \n",
    "            if patience <= 0:\n",
    "                weights = prev_weights\n",
    "                biases = prev_biases\n",
    "                #learningRate /= 2\n",
    "            wandb.log({'valAcc':(Algorithms.evaluateNetwork(weights, biases, activate, output, valTest_x, valTest_y)), 'epoch':epoch, 'loss':lossTrack[-1]})\n",
    "            #print(\"The val_acc after this epoch is: \"+ str(validation))\n",
    "        #Functions.plot(validation)\n",
    "            #print(\"The loss after this epoch is: \"+ str(batchLoss))\n",
    "        return prev_weights, prev_biases\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def miniBatchNAG(weights, biases, batchSize, learningRate, activate, output, derivative, dataPoints, dataLabels, valTest_x, valTest_y, decay, epochs):\n",
    "        lossTrack = []\n",
    "        validation = []\n",
    "        beta = 0.9\n",
    "        prevWeights = np.zeros_like(weights)\n",
    "        prevBiases = np.zeros_like(biases)\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            indices = np.arange(len(dataPoints))\n",
    "            np.random.shuffle(indices)\n",
    "            batchX = dataPoints[indices]\n",
    "            batchY = dataLabels[indices]\n",
    "            for i in range(math.floor(len(dataPoints)/batchSize)):\n",
    "                trainer = batchX[i*batchSize:i*batchSize+batchSize]\n",
    "                labeler = batchY[i*batchSize:i*batchSize+batchSize]\n",
    "                batchLoss = 0.0\n",
    "                tempWeights = np.zeros_like(weights)\n",
    "                tempBiases = np.zeros_like(biases)\n",
    "                for data in range(batchSize):\n",
    "                    a,h = Algorithms.ForwardProp(weights-prevWeights, biases-prevBiases, activate, output, trainer[data])\n",
    "                    currWeights, currBiases = Algorithms.BackProp(weights-prevWeights, biases-prevBiases, a, h, derivative, trainer[data], labeler[data])\n",
    "                    batchLoss += Functions.crossEntropyLoss(Functions.onehot(labeler[data]), h[-1])\n",
    "                    tempWeights += currWeights\n",
    "                    tempBiases += currBiases\n",
    "                batchLoss /= batchSize\n",
    "                tempWeights /= batchSize\n",
    "                tempBiases /= batchSize\n",
    "                momentumWeights = beta*prevWeights + tempWeights*1.0\n",
    "                momentumBiases = beta*prevBiases + tempBiases*1.0\n",
    "                weights = weights - learningRate*(momentumWeights + decay*weights) \n",
    "                biases = biases - learningRate*(momentumBiases + decay*biases)\n",
    "                prevWeights = momentumWeights\n",
    "                prevBiases = momentumBiases\n",
    "                lossTrack.append(batchLoss)\n",
    "            wandb.log({'valAcc':(Algorithms.evaluateNetwork(weights, biases, activate, output, valTest_x, valTest_y)), 'epoch':epoch, 'loss':lossTrack[-1]})\n",
    "            #wandb.log({'valAcc':(Algorithms.evaluateNetwork(weights, biases, valTest_x, valTest_y)), 'epoch':epoch})\n",
    "            validation.append(Algorithms.evaluateNetwork(weights, biases, activate, output, valTest_x, valTest_y))\n",
    "            #print(\"The val_acc after this epoch is: \"+ str(validation))\n",
    "            #Functions.plot(validation)\n",
    "            #print(\"The loss after this epoch is: \"+ str(batchLoss))\n",
    "            #Functions.plot(lossTrack)\n",
    "        return net\n",
    "    \n",
    "    @staticmethod\n",
    "    def RMSProp(weights, biases, batchSize, learningRate, activate, output, derivative, dataPoints, dataLabels, valTest_x, valTest_y, decay, epochs):\n",
    "        lossTrack = []\n",
    "        validation = []\n",
    "        beta = 0.5\n",
    "        epsilon = 0.000001\n",
    "        momentumWeights = np.zeros_like(weights)\n",
    "        momentumBiases = np.zeros_like(biases)\n",
    "        learnerRateW = np.full_like(weights, learningRate)\n",
    "        learnerRateB = np.full_like(biases, learningRate)\n",
    "        \n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            indices = np.arange(len(dataPoints))\n",
    "            np.random.shuffle(indices)\n",
    "            batchX = dataPoints[indices]\n",
    "            batchY = dataLabels[indices]\n",
    "            for i in range(math.floor(len(dataPoints)/batchSize)):\n",
    "                trainer = batchX[i*batchSize:i*batchSize+batchSize]\n",
    "                labeler = batchY[i*batchSize:i*batchSize+batchSize]\n",
    "                batchLoss = 0.0\n",
    "                dw = np.zeros_like(weights)\n",
    "                db = np.zeros_like(biases)\n",
    "                for data in range(batchSize):\n",
    "                    a,h = Algorithms.ForwardProp(weights, biases, activate, output, trainer[data])\n",
    "                    currWeights, currBiases = Algorithms.BackProp(weights, biases , a, h, derivative, trainer[data], labeler[data])\n",
    "                    batchLoss += Functions.crossEntropyLoss(Functions.onehot(labeler[data]), h[-1])\n",
    "                    dw += currWeights\n",
    "                    db += currBiases\n",
    "                batchLoss /= batchSize\n",
    "                dw /= batchSize\n",
    "                db /= batchSize\n",
    "                momentumWeights = momentumWeights*beta + (1-beta)*dw**2\n",
    "                momentumBiases = momentumBiases*beta + (1-beta)*db**2\n",
    "                tempW = np.zeros_like(momentumWeights)\n",
    "                tempB = np.zeros_like(momentumBiases)\n",
    "                for i in range(len(dw)):\n",
    "                    tempW[i] = np.sqrt(momentumWeights[i])\n",
    "                    tempB[i] = np.sqrt(momentumBiases[i])\n",
    "                weights = weights - ((learnerRateW)*(dw + decay*weights)/(tempW + epsilon))\n",
    "                biases = biases - (learnerRateB*(db + decay*biases))/(tempB+epsilon)\n",
    "                lossTrack.append(batchLoss)\n",
    "            wandb.log({'valAcc':(Algorithms.evaluateNetwork(weights, biases, valTest_x, valTest_y)), 'epoch':epoch, 'loss':lossTrack[-1]})\n",
    "            #wandb.log({'valAcc':(Algorithms.evaluateNetwork(weights, biases, valTest_x, valTest_y)), 'epoch':epoch})\n",
    "            validation.append(Algorithms.evaluateNetwork(weights, biases, valTest_x, valTest_y))\n",
    "            #print(\"The val_acc after this epoch is: \"+ str(validation))\n",
    "            #Functions.plot(validation)\n",
    "            #print(\"The loss after this epoch is: \"+ str(batchLoss))\n",
    "        return weights, biases\n",
    "\n",
    "    @staticmethod\n",
    "    def miniBatchGD(weights, biases, batchSize, learningRate, activate, output, derivative, dataPoints, dataLabels, valTest_x, valTest_y, decay, epochs):\n",
    "        validation = []\n",
    "        lossTrack = []\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            indices = np.arange(len(dataPoints))\n",
    "            np.random.shuffle(indices)\n",
    "            batchX = dataPoints[indices]\n",
    "            batchY = dataLabels[indices]\n",
    "            for i in range(math.floor(len(dataPoints)/batchSize)):\n",
    "                trainer = batchX[i*batchSize:i*batchSize+batchSize]\n",
    "                labeler = batchY[i*batchSize:i*batchSize+batchSize]\n",
    "                batchLoss = 0.0\n",
    "                dw = np.zeros_like(weights)\n",
    "                db = np.zeros_like(biases)\n",
    "                for data in range(batchSize):\n",
    "                    a,h = Algorithms.ForwardProp(weights, biases , activate, output, trainer[data])\n",
    "                    tempw,tempb = Algorithms.BackProp(weights, biases, a, h, derivative, trainer[data], labeler[data])\n",
    "                    dw+=tempw\n",
    "                    db+=tempb\n",
    "                    batchLoss += Functions.crossEntropyLoss(Functions.onehot(labeler[data]), h[-1])\n",
    "                batchLoss /= batchSize\n",
    "                dw /= batchSize\n",
    "                db /= batchSize\n",
    "                weights -= learningRate*(dw + decay*weights)\n",
    "                biases -= learningRate*(db + decay*biases)\n",
    "                lossTrack.append(batchLoss)\n",
    "            #print(\"The loss after this epoch is: \"+ str(batchLoss))\n",
    "            wandb.log({'valAcc':(Algorithms.evaluateNetwork(weights, biases, valTest_x, valTest_y)), 'epoch':epoch, 'loss':lossTrack[-1]})\n",
    "            #wandb.log({'valAcc':(Algorithms.evaluateNetwork(weights, biases, valTest_x, valTest_y)), 'epoch':epoch})\n",
    "            validation.append(Algorithms.evaluateNetwork(weights, biases, valTest_x, valTest_y))\n",
    "            #print(\"The val_acc after this epoch is: \"+ str(validation))\n",
    "            #Functions.plot(validation)\n",
    "        return weights, biases\n",
    "    \n",
    "    @staticmethod\n",
    "    def evaluateNetwork(weights, biases,activate, output, test_x, test_y):\n",
    "        num_acc = 0\n",
    "        for i in range(len(test_x)):\n",
    "            a,h = Algorithms.ForwardProp(weights, biases, activate, output, test_x[i])\n",
    "            h = np.array(h, dtype = object)\n",
    "            predY =   np.argmax(h[len(h)-1])\n",
    "            if test_y[i] == predY:\n",
    "                num_acc+=1\n",
    "        return (num_acc/len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "wpFxAmhE9t2C",
    "outputId": "fdd02dd5-6147-4d7a-9d86-9cabebea2f81"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████▊                                   | 2/10 [00:38<02:34, 19.36s/it]/var/folders/hm/9gm9jdm90q5fz1jnmjxxsxh40000gn/T/ipykernel_75561/233683582.py:32: RuntimeWarning: invalid value encountered in log\n",
      "  loss = (-1/10.0) * np.sum(np.multiply(y, np.log(yHat+1e-10)) + np.multiply((1 - y), np.log(1 - (yHat+1e-10))))\n",
      " 60%|██████████████████████████▍                 | 6/10 [01:59<01:19, 19.89s/it]"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    data = PreProc()\n",
    "    #data.visualize(5)\n",
    "    train_x, test_x = data.flattenAndCentralize()\n",
    "    trainx = train_x/255.0\n",
    "    testx = test_x/255.0\n",
    "    train_y, test_y = data.getLabels()\n",
    "    neuralNet = FFNet(0, len(trainx[0]), 10)\n",
    "    neuralNet.addHiddenLayer(128, \"xavier\")\n",
    "    neuralNet.addHiddenLayer(128, \"xavier\")#drastic decrease in accuracy\n",
    "    neuralNet.addHiddenLayer(128, \"xavier\")#drastic decrease in accuracy\n",
    "    neuralNet.addOutputLayer(10, \"xavier\")\n",
    "    neuralNet.solidify()\n",
    "    #print(trainx.shape)\n",
    "    #wandb.init()\n",
    "    weights,biases = neuralNet.fit(\"adam\",64, 0.001, \"relu\", trainx, train_y, 0, 10)\n",
    "    #neuralNet.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qHfSx0jHSqtk",
    "outputId": "5e2dae53-035e-468c-98d7-d29b08d821f1"
   },
   "outputs": [],
   "source": [
    "#The class of FeedForwardNeuralNetwor\n",
    "\n",
    "class FFNet:\n",
    "    #constructor\n",
    "    def __init__(self,number_of_hidden_layers, number_of_inputs, number_of_outputs):\n",
    "        self.number_of_inputs = number_of_inputs\n",
    "        self.number_of_hidden_layers = number_of_hidden_layers\n",
    "        self.number_of_outputs = number_of_outputs\n",
    "        self.input = [0 for i in range(number_of_inputs)]\n",
    "        self.output = [0 for i in range(10)]\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        #self.hidden.append(np.random.random((number_of_inputs+1)))\n",
    "    \n",
    "    #Method for creating layers\n",
    "    def addHiddenLayer(self,number_of_neurons, initialization):\n",
    "        if(len(self.weights) == 0):\n",
    "            temp_weights = np.random.randn(number_of_neurons, self.number_of_inputs)*0.01\n",
    "            temp_biases = np.full((number_of_neurons), 0.01)\n",
    "            if initialization == \"xavier\":\n",
    "                temp_weights = np.random.randn(number_of_neurons, self.number_of_inputs)/np.sqrt((self.number_of_inputs)/2)\n",
    "                #temp_biases = np.random.randn(number_of_neurons)*np.sqrt(1/(number_of_neurons))\n",
    "        else:\n",
    "            prev_neurons = len(self.weights[-1])\n",
    "            temp_weights = np.random.randn(number_of_neurons, prev_neurons)*0.01\n",
    "            temp_biases = np.full((number_of_neurons), 0.01)\n",
    "            if initialization == \"xavier\":\n",
    "                temp_weights = np.random.randn(number_of_neurons, prev_neurons)/np.sqrt((prev_neurons)/2)\n",
    "                #temp_biases = np.random.randn(number_of_neurons)*np.sqrt(1/(number_of_neurons))\n",
    "\n",
    "        self.weights.append(temp_weights)\n",
    "        self.biases.append(temp_biases)\n",
    "    \n",
    "    def addOutputLayer(self, number_of_outputs, initialization):\n",
    "        if(len(self.weights) == 0):\n",
    "            #print(\"number of inputs: \"+str(self.number_of_inputs))\n",
    "            temp_weights = np.random.randn(number_of_outputs, self.number_of_inputs)*0.01\n",
    "            temp_biases = np.full((number_of_outputs), 0.01)\n",
    "            if initialization == \"xavier\":\n",
    "                temp_weights = np.random.randn(number_of_outputs, self.number_of_inputs)/np.sqrt((prev_neurons)/2)\n",
    "        else:\n",
    "            prev_neurons = len(self.weights[-1])\n",
    "            temp_weights = np.random.randn(number_of_outputs, prev_neurons)*0.01\n",
    "            temp_biases = np.full((number_of_outputs), 0.01)\n",
    "            if initialization == \"xavier\":\n",
    "                temp_weights = np.random.randn(number_of_outputs, prev_neurons)/np.sqrt((prev_neurons)/2)\n",
    "                \n",
    "        \n",
    "        self.weights.append(temp_weights)\n",
    "        self.biases.append(temp_biases)\n",
    "\n",
    "    def solidify(self):\n",
    "        self.weights = np.array(self.weights, dtype = object)\n",
    "        self.biases = np.array(self.biases, dtype = object)\n",
    "\n",
    "    def getNetwork(self):\n",
    "        return self.weights,self.biases\n",
    "    \n",
    "    def ForwardProp(self, activate, output, inputLayer):\n",
    "        return Algorithms.ForwardProp(self.network, activate, output, inputLayer)\n",
    "    \n",
    "    def lossCalc(self, lossFunction, Y):\n",
    "        predY = self.historyA[(len(self.historyA)-1)]\n",
    "        return lossFunction(Y,self.predY)\n",
    "\n",
    "    def BackProp(self, a, h, dataPoint, dataLabel):\n",
    "        return Algorithms.BackProp(self.network, a, h, dataPoint, dataLabel)\n",
    "    \n",
    "    def fit(self, optimizer, batchSize, learningRate, activation, trainx, train_y, decay, epochs):\n",
    "        \n",
    "        #break data into training and validation\n",
    "        indices = np.arange(len(trainx))\n",
    "        np.random.shuffle(indices)\n",
    "        trainx = trainx[indices]\n",
    "        train_y = train_y[indices]\n",
    "        \n",
    "        valTest_x = trainx[int(0.9*len(trainx)):]\n",
    "        valTest_y = train_y[int(0.9*len(train_y)):]\n",
    "        \n",
    "        trainx = trainx[:int(0.9*len(trainx))]\n",
    "        train_y = train_y[:int(0.9*len(train_y))]\n",
    "        \n",
    "        if activation == \"relu\":\n",
    "            activate = Functions.reLU\n",
    "            derivative = Functions.derivative_reLU\n",
    "            output = Functions.softmax\n",
    "        elif activation == \"tanh\":\n",
    "            activate = Functions.tanh\n",
    "            derivative = Functions.derivative_tanh\n",
    "            output = Functions.softmax\n",
    "        elif activation == \"identity\":\n",
    "            activate = Functions.identity\n",
    "            derivative = Functions.derivative_identity\n",
    "            output = Functions.softmax\n",
    "        else:\n",
    "            activate = Functions.sigmoid\n",
    "            derivative = Functions.derivative_sigmoid\n",
    "            output = Functions.softmax\n",
    "        \n",
    "        #print(optimizer)\n",
    "        \n",
    "        if optimizer == \"momentum\":\n",
    "            self.weights, self.biases = Algorithms.miniBatchMGD(self.weights,self.biases , batchSize, learningRate, activate, output, derivative,  trainx, train_y, valTest_x, valTest_y, decay, epochs)\n",
    "        elif optimizer == \"nag\":\n",
    "            self.weights, self.biases = Algorithms.miniBatchNAG(self.weights,self.biases , batchSize, learningRate,activate, output, derivative , trainx, train_y, valTest_x, valTest_y, decay, epochs)\n",
    "        elif optimizer == \"rmsprop\":\n",
    "            self.weights, self.biases = Algorithms.RMSProp(self.weights,self.biases , batchSize, learningRate, activate, output, derivative,  trainx, train_y, valTest_x, valTest_y, decay, epochs)\n",
    "        elif optimizer == \"adam\":\n",
    "            self.weights, self.biases = Algorithms.ADAM(self.weights,self.biases , batchSize, learningRate,activate, output, derivative,  trainx, train_y, valTest_x, valTest_y, decay, epochs)\n",
    "        elif optimizer == \"nadam\":\n",
    "            self.weights, self.biases = Algorithms.NADAM(self.weights,self.biases , batchSize, learningRate, activate, output, derivative,  trainx, train_y, valTest_x, valTest_y, decay, epochs)\n",
    "        else:\n",
    "            self.weights, self.biases = Algorithms.miniBatchGD(self.weights,self.biases , batchSize, learningRate, activate, output, derivative , trainx, train_y, valTest_x, valTest_y, decay, epochs)\n",
    "        print(Algorithms.evaluateNetwork(self.weights, self.biases, activate, output, testx, test_y))       \n",
    "        \n",
    "        return self.weights,self.biases\n",
    "            \n",
    "    def evaluateNetwork(self, testx, tes_ty):\n",
    "        Algorithms.evaluateNetwork(self.weights, self.biases, testx, test_y)        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = trainx[:int(0.9*len(trainx))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "valTest = trainx[int(0.9*len(trainx)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "R-YOAwnCmO3I",
    "outputId": "a052aca6-18cc-4d10-a3d6-17257b72ab4e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cC3J4fScHacM"
   },
   "outputs": [],
   "source": [
    "a,h = Algorithms.ForwardProp(net, Functions.sigmoid, Functions.softmax, train_x[500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L1KBO1xmaxu8",
    "outputId": "aa351438-35e1-4e7e-99c3-1a86fae078b7"
   },
   "outputs": [],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 454
    },
    "id": "-DhPxwMbl6qX",
    "outputId": "5f0b4ef2-96a9-4a5b-f724-c8a1d521325e"
   },
   "outputs": [],
   "source": [
    "\n",
    "batchSize = 32\n",
    "gradient = np.zeros_like(net)\n",
    "lossTrack = []\n",
    "for epoch in tqdm(range(15)):\n",
    "    indices = np.arange(len(trainx))\n",
    "    np.random.shuffle(indices)\n",
    "    batchX = trainx[indices]\n",
    "    batchY = train_y[indices]\n",
    "    for i in range(math.ceil(len(trainx)/batchSize)):\n",
    "        trainer = batchX[i*batchSize:i*batchSize+batchSize]\n",
    "        labeler = batchY[i*batchSize:i*batchSize+batchSize]\n",
    "        batchLoss = 0.0\n",
    "        for data in range(batchSize):\n",
    "            a,h = Algorithms.ForwardProp(net, Functions.sigmoid, Functions.softmax, trainer[data])\n",
    "            currGrad = Algorithms.BackProp(net, a, h, trainer[data], labeler[data])\n",
    "            batchLoss += Functions.crossEntropyLoss(Functions.onehot(labeler[data]), h[-1])\n",
    "            gradient += currGrad\n",
    "        batchLoss /= 32\n",
    "        gradient /= 32\n",
    "        net = net - 0.01*gradient \n",
    "        lossTrack.append(batchLoss)\n",
    "    print(\"The loss after this epoch is: \"+ str(batchLoss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-hhR7Gex1BK9",
    "outputId": "57a17ae8-adef-4561-8bbb-a0206fa23106"
   },
   "outputs": [],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uckwQu3vbr9b"
   },
   "outputs": [],
   "source": [
    "gradient = neuralNet.BackProp(a,h,train_x[1], train_y[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "trXRsTlNb0_1"
   },
   "outputs": [],
   "source": [
    "net = net - gradient*0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SeXnBQQMbw_U",
    "outputId": "efc6c2b2-af74-4a82-f9e9-3c23fd302cd6"
   },
   "outputs": [],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CvzSP-M4btcN",
    "outputId": "001904ed-f2ec-4f76-b167-2678d1f33835"
   },
   "outputs": [],
   "source": [
    "np.argmax(np.array(h[L]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RcwywNK0blOr",
    "outputId": "d764dc8a-d727-443b-cb91-ba121fe78f26"
   },
   "outputs": [],
   "source": [
    "while np.argmax(np.array(h[L])):\n",
    "    for i in range(10):\n",
    "        a,h = Algorithms.ForwardProp(net,Functions.sigmoid, Functions.softmax, train_x[4])\n",
    "        gradient = neuralNet.BackProp(a,h,train_x[4], train_y[4])\n",
    "        net = net - 0.1*gradient\n",
    "        print(np.argmax(np.array(h[L])), end = \", \"),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-df0oArs006R",
    "outputId": "4218ed61-af21-4c62-cdac-3b2ec63efbe8"
   },
   "outputs": [],
   "source": [
    "train_y[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V0eqMHkIz3to",
    "outputId": "4a35e5fc-75b2-4bb2-de9a-8d103c5bb6c9"
   },
   "outputs": [],
   "source": [
    "num_acc = 0\n",
    "for i in range(len(test_x)):\n",
    "    a,h = Algorithms.ForwardProp(net, Functions.sigmoid, Functions.softmax, test_x[i])\n",
    "    h = np.array(h)\n",
    "    predY =   np.argmax(h[len(h)-1])\n",
    "    print(predY)\n",
    "    if test_y[i] == predY:\n",
    "        num_acc+=1\n",
    "print(num_acc/len(test_y), end = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o84TF_u3yVv9",
    "outputId": "074c498e-5519-4f15-dc86-049be1c6382a"
   },
   "outputs": [],
   "source": [
    "len(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TUcuGN-SzSyq",
    "outputId": "f6614617-7859-48d3-dff7-623f7622625c"
   },
   "outputs": [],
   "source": [
    "gradient[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OY0Zy9psDtk7"
   },
   "outputs": [],
   "source": [
    "for i in gradient[0]:\n",
    "    print(i, end = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I3YdLFqZU8Ci",
    "outputId": "52462d9f-b600-46f9-9403-a93bdb09c341"
   },
   "outputs": [],
   "source": [
    "gradient[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kPDcvGye0lF7"
   },
   "outputs": [],
   "source": [
    "gradaL = -(Functions.onehot(train_y[1])-h[len(h)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PcCtxLxzFKVG"
   },
   "outputs": [],
   "source": [
    "gradhL_1 = np.matmul(np.transpose(net[(len(net)-1)]),aL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W9xEUE6XKesg"
   },
   "outputs": [],
   "source": [
    "gradaL_1 = np.multiply(net[len(net)-1][:,:len(net[len(net)-1][0])-1], Functions.derivative_sigmoid(a[len(net)-2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K72jzJOiJSR3"
   },
   "outputs": [],
   "source": [
    "gradW = np.outer(gradaL,h[len(net)-2].T)\n",
    "gradB = gradaL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XDbZT58hUOWW"
   },
   "outputs": [],
   "source": [
    "gradB.resize((len(gradB),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L3-e7uWUUYd_",
    "outputId": "2dccdc5f-1f4f-4de9-ef20-bdc5f9e31910"
   },
   "outputs": [],
   "source": [
    "gradB.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WdZHmwrRR2rx",
    "outputId": "df26a5dc-df87-4147-c06a-0d3df7c1d70d"
   },
   "outputs": [],
   "source": [
    "np.append(gradW,gradB.resize((10,1)),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RDIXmbKHNo25",
    "outputId": "6d44560f-8280-4146-b7bd-dedf9486be76"
   },
   "outputs": [],
   "source": [
    "gradaL.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d7_71o1X06hg",
    "outputId": "5899546a-1304-47a4-e96b-f0f8edbce0ee"
   },
   "outputs": [],
   "source": [
    "a[len(net)-2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k65CaJIefUTY"
   },
   "outputs": [],
   "source": [
    "weights = net[0][:,:len(net[0][0])-1]\n",
    "bias = net[0][:,len(net[0][0])-1]\n",
    "temp = np.matmul(weights,train_x[0])+bias\n",
    "temp = temp/np.linalg.norm(temp)\n",
    "a = []\n",
    "a.append(temp)\n",
    "h = []\n",
    "h.append(Functions.sigmoid(a[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZafDhQ7LmcUx"
   },
   "outputs": [],
   "source": [
    "weights = net[L][:,:len(net[L][0])-1]\n",
    "bias = net[L][:,len(net[L][0])-1]\n",
    "temp = np.matmul(weights,h[0])+bias\n",
    "temp = temp/np.linalg.norm(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "epu4dvEji2mG"
   },
   "outputs": [],
   "source": [
    "L = len(net)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tg0dnhLfheDu",
    "outputId": "93e6fdc7-9cf7-4236-b55e-550c4459f101"
   },
   "outputs": [],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qohv21-jxL88"
   },
   "outputs": [],
   "source": [
    "#The class of FeedForwardNeuralNetwor\n",
    "\n",
    "class FFNet:\n",
    "    #constructor\n",
    "    hidden = []\n",
    "    input = []\n",
    "    output = []\n",
    "    def __init__(self,number_of_hidden_layers, number_of_inputs, number_of_outputs):\n",
    "        self.number_of_inputs = number_of_inputs\n",
    "        self.number_of_hidden_layers = number_of_hidden_layers\n",
    "        self.number_of_outputs = number_of_outputs\n",
    "        #At the same time, the layers input layers mus also be initialized.\n",
    "\n",
    "        input = [0 for i in range(number_of_inputs)]\n",
    "        output = [0 for i in range(number_of_outputs)]\n",
    "        hidden = [[]]\n",
    "\n",
    "        #input and output layers are nothing but simple lists\n",
    "    \n",
    "    #Method for creating layers\n",
    "    def add_hidden_layer(number_of_neurons):\n",
    "        temp_weights = [0 for i in range(number_of_neurons+1)] #The +1 is for bias values\n",
    "        hidden.append(temp_weights)\n",
    "    \n",
    "    def backward_propagate(a,h, pred_y):\n",
    "        delthet[L] = -(exp(y) - pred_y) #with respect to output layer\n",
    "        for k in range(0,L-1,-1):\n",
    "            delthetw = np.matmul(delthet[k], h[k-1].T)\n",
    "            delthetb = delthet[k]\n",
    "            deltheth = np.matmul(weights[k].T, delthet[k])\n",
    "            delthet[k-1] = hadamard(deltheth, preac(a)) \n",
    "\n",
    "    def forward_propagate():\n",
    "        #here, we are calculating the preactivations and activations.\n",
    "        #we then store them in an array and return it.\n",
    "        \n",
    "        for k in range(number_of_levels-1):\n",
    "            a[k] = biases[k] + np.matmul(weights[k], h[k-1])\n",
    "            h[k] = g(a[k])\n",
    "        a[number_of_levels-1] = biases[number_of_levels] + np.matmul(weights[number_of_levels],h[number_of_levels-1])\n",
    "        pred_y = output(a[number_of_levels-1])\n",
    "        return a,h, pred_y\n",
    "\n",
    "\n",
    "    def gradient_descent():\n",
    "        a,h, pred_y = forward_propagate()\n",
    "        delthet = backward_propagate(a,h, pred_y)\n",
    "        thet += delthet\n",
    "\n",
    "    def fit(dataset):\n",
    "        for x,y in dataset:\n",
    "            loss = forward(x,y)\n",
    "            delthet = backward(loss)\n",
    "            thet += learn_rate*delthet\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
