{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 241,
      "metadata": {
        "id": "vlcWrCZGiPRM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from matplotlib import pyplot as plt\n",
        "from keras.datasets import fashion_mnist\n",
        "from tqdm import tqdm\n",
        "from numba import jit,cuda\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PreProc:\n",
        "    def __init__(self):\n",
        "        (self.trainx,self.trainy),(self.testx, self.testy) = fashion_mnist.load_data()\n",
        "    def visualize(self,n):\n",
        "        for i in range(n):\n",
        "            plt.subplot(330+1+i) # ask someone why??\n",
        "            plt.imshow(self.trainx[i], cmap = plt.get_cmap('gray'))\n",
        "        plt.show()\n",
        "    def flatten(self):\n",
        "        trainx_flattened = self.trainx\n",
        "        testx_flattened = self.testx\n",
        "        trainx_flattened.shape = (60000,784)\n",
        "        testx_flattened.shape = (10000,784)\n",
        "        return trainx_flattened,testx_flattened\n",
        "    def getLabels(self):\n",
        "        return self.trainy, self.testy\n",
        "    def getInputSize(self):\n",
        "        return len(self.trainx[0])"
      ],
      "metadata": {
        "id": "TfO2B6v6BVWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.getInputSize()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HhzRi0xhOLc",
        "outputId": "c3c442f0-9d09-4d9a-a9dd-d6be5c4c41d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(784,)"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Functions:\n",
        "    @staticmethod\n",
        "    def sigmoid(input):\n",
        "        return  1.0/(1.0+np.exp(-input))\n",
        "    @staticmethod\n",
        "    def softmax(input):\n",
        "        return np.exp(input)/(np.sum(np.exp(input)))\n",
        "    @staticmethod\n",
        "    def onehot(input):\n",
        "        result = np.zeros(10)\n",
        "        result[input] = 1\n",
        "        return result\n",
        "    @staticmethod\n",
        "    def crossEntropyLoss(y,yHat):\n",
        "        loss = (-1/10.0) * np.sum(np.multiply(y, np.log(yHat)) + np.multiply((1 - y), np.log(1 - yHat)))\n",
        "        return loss\n",
        "    @staticmethod\n",
        "    def derivative_sigmoid(input):\n",
        "        return Functions.sigmoid(input)*(1-Functions.sigmoid(input))\n",
        "    def plot(input):\n",
        "        plt.plot(input)\n",
        "        plt.xlabel(\"Epochs\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "        plt.title(\"Loss over iterations\")"
      ],
      "metadata": {
        "id": "7BwoVy0bPFA0"
      },
      "execution_count": 268,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Algorithms:\n",
        "    @staticmethod\n",
        "    def ForwardProp(net, activate, output, inputLayer):\n",
        "        L = len(net)-1\n",
        "        a = []\n",
        "        h = []\n",
        "        weights = net[0][:,:len(net[0][0])-1]\n",
        "        bias = net[0][:,len(net[0][0])-1]\n",
        "        temp = np.matmul(weights,inputLayer)+bias\n",
        "        a.append(temp)\n",
        "        h.append(activate(a[0]))\n",
        "        for k in range(1,L):\n",
        "            weights = net[k][:,:len(net[k][0])-1]\n",
        "            bias = net[k][:,len(net[k][0])-1]\n",
        "            temp = np.matmul(weights,h[k-1])+bias\n",
        "            a.append(bias + np.matmul(weights,h[k-1]))\n",
        "            h.append(activate(a[k]))\n",
        "        weights = net[L][:,:len(net[L][0])-1]\n",
        "        bias = net[L][:,len(net[L][0])-1]\n",
        "        temp = np.matmul(weights,h[L-1])+bias\n",
        "        a.append(temp)\n",
        "        h.append(output(a[L]))\n",
        "        return a,h\n",
        "    @staticmethod\n",
        "    def BackProp(net, a, h, dataPoint, dataLabel):\n",
        "        L = len(net)-1\n",
        "        gradaL = -(Functions.onehot(dataLabel)-h[len(h)-1])\n",
        "        gradient = np.zeros_like(net)\n",
        "        for k in range(L,0,-1):\n",
        "            gradW = np.outer(gradaL,h[k-1].T)\n",
        "            gradB = gradaL\n",
        "            gradB.resize((len(gradB),1))\n",
        "            grad = np.append(gradW,gradB,axis=1)\n",
        "            gradient[k] = grad\n",
        "\n",
        "            gradhL_1 = np.matmul(np.transpose(net[k][:,len(net[k])-1]),gradaL)\n",
        "            gradaL_1 = np.multiply(gradhL_1, Functions.derivative_sigmoid(a[k-1]))\n",
        "            gradaL = gradaL_1\n",
        "        gradW = np.outer(gradaL,dataPoint.T)\n",
        "        gradB = gradaL\n",
        "        gradB.resize((len(gradB),1))\n",
        "        grad = np.append(gradW,gradB,axis=1)\n",
        "        gradient[0] = grad\n",
        "        return gradient\n",
        "    def mGD(net, learningRate, train, label, activate, output):\n",
        "        beta = 0.9\n",
        "        prevGrad = np.zeros_like(net)\n",
        "        for i in range(len(train)):\n",
        "            a,h = Algorithms.ForwardProp(net, activate, output, train[i])\n",
        "            gradient = Algorithms.BackProp(net, a, h, train[i], label[i])\n",
        "            gradient += beta*prevGrad + learningRate*gradient\n",
        "        net = net - gradient"
      ],
      "metadata": {
        "id": "Bu5XtsgmjyaH"
      },
      "execution_count": 276,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(h)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsKHjO2hWrvW",
        "outputId": "3efc9ad6-da10-42df-bf76-4fb8dfc592d9"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(L,0,-1):\n",
        "    print(i, end = \" \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYUFuUhfSjBG",
        "outputId": "f75413db-7810-4b53-d9a9-d230e43191c3"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3 2 1 "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHfSx0jHSqtk",
        "outputId": "5e2dae53-035e-468c-98d7-d29b08d821f1"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#The class of FeedForwardNeuralNetwor\n",
        "\n",
        "class FFNet:\n",
        "    #constructor\n",
        "    def __init__(self,number_of_hidden_layers, number_of_inputs, number_of_outputs):\n",
        "        self.number_of_inputs = number_of_inputs\n",
        "        self.number_of_hidden_layers = number_of_hidden_layers\n",
        "        self.number_of_outputs = number_of_outputs\n",
        "        self.input = [0 for i in range(number_of_inputs)]\n",
        "        self.output = [0 for i in range(10)]\n",
        "        self.hidden = []\n",
        "        #self.hidden.append(np.random.random((number_of_inputs+1)))\n",
        "    \n",
        "    #Method for creating layers\n",
        "    def addHiddenLayer(self,number_of_neurons):\n",
        "        if(len(self.hidden) == 0):\n",
        "            temp_weights = np.random.random((number_of_neurons, self.number_of_inputs+1)) #The +1 is for biases.\n",
        "        else:\n",
        "            prev_neurons = len(self.hidden[len(self.hidden) - 1])\n",
        "            temp_weights = np.random.random((number_of_neurons, prev_neurons + 1)) # The +1 is for biases.\n",
        "        temp_weights = temp_weights/np.linalg.norm(temp_weights)\n",
        "        self.hidden.append(temp_weights)\n",
        "    \n",
        "    def addOutputLayer(self, number_of_outputs):\n",
        "        if(len(self.hidden) == 0):\n",
        "            #print(\"number of inputs: \"+str(self.number_of_inputs))\n",
        "            temp_weights = np.random.random((number_of_outputs, self.number_of_inputs)) #Bias not needed for the output layer.\n",
        "        else:\n",
        "            prev_neurons = len(self.hidden[len(self.hidden) - 1])\n",
        "            temp_weights = np.random.random((number_of_outputs, prev_neurons + 1)) #Bias not needed for the output layer.\n",
        "        temp_weights = temp_weights/np.linalg.norm(temp_weights)\n",
        "        self.hidden.append(temp_weights)\n",
        "\n",
        "    def solidify(self):\n",
        "        self.network = np.array(self.hidden)\n",
        "\n",
        "    def getNetwork(self):\n",
        "        return self.network\n",
        "    \n",
        "    def ForwardProp(self, activate, output, inputLayer):\n",
        "        return Algorithms.ForwardProp(self.network, activate, output, inputLayer)\n",
        "    \n",
        "    def lossCalc(self, lossFunction, Y):\n",
        "        predY = self.historyA[(len(self.historyA)-1)]\n",
        "        return lossFunction(Y,self.predY)\n",
        "\n",
        "    def BackProp(self, a, h, dataPoint, dataLabel):\n",
        "        return Algorithms.BackProp(self.network, a, h, dataPoint, dataLabel)\n",
        "\n",
        "    def gradientDescent(self, learningRate, lossFunction, activate, output, dataPoints, dataLabels):\n",
        "        losses = []\n",
        "        for i in range(5):\n",
        "            gradient = 0.0\n",
        "            loss = 0\n",
        "            for index in tqdm(range(len(dataPoints))):\n",
        "                a,h = Algorithms.ForwardProp(self.network, activate, output, dataPoints[index])\n",
        "                predY = h[(len(h)-1)]\n",
        "\n",
        "                loss += Functions.crossEntropyLoss(dataLabels[index], predY)\n",
        "                gradient+= Algorithms.BackProp(self.network, a, h, dataPoints[index], dataLabels[index])\n",
        "            losses.append(loss)\n",
        "            #gradient /= 60000\n",
        "            self.network = self.network - learningRate * gradient\n",
        "        return self.network, losses\n",
        "    def stochGradientDescent(self, learningRate, lossFunction, activate, output, dataPoints, dataLabels):\n",
        "        #losses = []\n",
        "        net = self.network\n",
        "        for i in range(3):\n",
        "            for i in tqdm(range(len(dataPoints))):\n",
        "                a,h = Algorithms.ForwardProp(net,Functions.sigmoid, Functions.softmax, dataPoints[i])\n",
        "                gradient = Algorithms.BackProp(net, a,h,dataPoints[i], dataLabels[i])\n",
        "                net = net - 0.1*gradient\n",
        "        return self.network\n",
        "    \n",
        "    def batchGD(self, learningRate, lossFunction, activate, output, dataPoints, dataLabels, batchSize = 32):\n",
        "        indices = np.arrange(60000)\n",
        "        for epoch in range(15):\n",
        "            np.shuffle(indices)\n",
        "            for i in range(batchSize):\n",
        "                \n",
        "    def evaluateNetwork(self,test_x, test_y):\n",
        "        num_acc = 0\n",
        "        for i in range(len(test_x)):\n",
        "            a,h = Algorithms.ForwardProp(self.network, Functions.sigmoid, Functions.softmax, test_x[i])\n",
        "            h = np.array(h)\n",
        "            predY =   np.argmax(h[len(h)-1])\n",
        "            if test_y[i] == predY:\n",
        "                num_acc+=1\n",
        "        print(num_acc/len(test_y))"
      ],
      "metadata": {
        "id": "FpxCgrhAinMc"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Parallelizer:\n",
        "    @staticmethod\n",
        "    @cuda.jit\n",
        "    def entireDataSet(net, dataPoints, dataLabels, activate, output, learningRate, answer, losses):\n",
        "        index = cuda.grid()\n",
        "        a,h = ParallelAlgorithms.ForwardProp(net, activate, output, dataPoints[index])\n",
        "        predY = h[(len(h)-1)]\n",
        "\n",
        "        loss = Functions.crossEntropyLoss(dataLabels[index], predY)\n",
        "        losses[index] = (loss)\n",
        "        gradient = ParallelAlgorithms.BackProp(net, a, h, dataPoints[index], dataLabels[index])\n",
        "        cuda.atomic.add(answer,0,gradient,answer)\n"
      ],
      "metadata": {
        "id": "Ew6hJr2qWgsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    data = PreProc()\n",
        "    #data.visualize(5)\n",
        "    train_x, test_x = data.flatten()\n",
        "    train_y, test_y = data.getLabels()\n",
        "    trainx = train_x/255\n",
        "    testx = test_x/255\n",
        "    neuralNet = FFNet(0,data.getInputSize(), 10)\n",
        "    neuralNet.addHiddenLayer(128)\n",
        "    neuralNet.addOutputLayer(10)\n",
        "    neuralNet.solidify()\n",
        "    net = neuralNet.getNetwork()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpFxAmhE9t2C",
        "outputId": "2a1a88c0-f8a0-407c-ee7d-59c089e523b5"
      },
      "execution_count": 277,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-158-6d2012216449>:35: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  self.network = np.array(self.hidden)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-YOAwnCmO3I",
        "outputId": "c323a238-81d2-4a87-b2f0-c08d2098c09a"
      },
      "execution_count": 261,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 261
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a,h = Algorithms.ForwardProp(net, Functions.sigmoid, Functions.softmax, train_x[500])"
      ],
      "metadata": {
        "id": "cC3J4fScHacM"
      },
      "execution_count": 228,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.argmax(np.array(h[L]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1KBO1xmaxu8",
        "outputId": "2c539861-8d84-44d0-93f5-050dd12704c7"
      },
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 222
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batchSize = 32\n",
        "gradient = np.zeros_like(net)\n",
        "lossTrack = []\n",
        "for epoch in tqdm(range(15)):\n",
        "    indices = np.arange(len(trainx))\n",
        "    np.random.shuffle(indices)\n",
        "    batchX = trainx[indices]\n",
        "    batchY = train_y[indices]\n",
        "    for i in range(math.ceil(len(trainx)/batchSize)):\n",
        "        trainer = batchX[i*batchSize:i*batchSize+batchSize]\n",
        "        labeler = batchY[i*batchSize:i*batchSize+batchSize]\n",
        "        batchLoss = 0.0\n",
        "        for data in range(batchSize):\n",
        "            a,h = Algorithms.ForwardProp(net, Functions.sigmoid, Functions.softmax, trainer[data])\n",
        "            currGrad = Algorithms.BackProp(net, a, h, trainer[data], labeler[data])\n",
        "            batchLoss += Functions.crossEntropyLoss(Functions.onehot(labeler[data]), h[-1])\n",
        "            gradient += currGrad\n",
        "        batchLoss /= 32\n",
        "        gradient /= 32\n",
        "        net = net - 0.1*gradient \n",
        "        lossTrack.append(batchLoss)\n",
        "    print(\"The loss after this epoch is: \"+ str(batchLoss))\n",
        "        #print(net[0][23][34], end = \" \"),"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DhPxwMbl6qX",
        "outputId": "72bb0af2-ab12-42c9-d8fd-1006a9668635"
      },
      "execution_count": 278,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  7%|▋         | 1/15 [00:52<12:18, 52.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The loss after this epoch is: 0.25368447447464426\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 13%|█▎        | 2/15 [01:44<11:18, 52.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The loss after this epoch is: 0.25368243603437446\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 3/15 [02:36<10:25, 52.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The loss after this epoch is: 0.2043754757074874\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 27%|██▋       | 4/15 [03:31<09:44, 53.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The loss after this epoch is: 0.21886851356646156\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 33%|███▎      | 5/15 [04:24<08:53, 53.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The loss after this epoch is: 0.20172673836221683\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 6/15 [05:17<07:58, 53.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The loss after this epoch is: 0.2307524412119744\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 47%|████▋     | 7/15 [06:09<07:01, 52.69s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The loss after this epoch is: 0.14706237206224892\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 53%|█████▎    | 8/15 [07:01<06:06, 52.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The loss after this epoch is: 0.18703337750088328\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 9/15 [07:52<05:12, 52.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The loss after this epoch is: 0.15243200916120495\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 67%|██████▋   | 10/15 [08:44<04:20, 52.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The loss after this epoch is: 0.16581671194408368\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 73%|███████▎  | 11/15 [09:35<03:26, 51.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The loss after this epoch is: 0.1230430818456662\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 12/15 [10:28<02:36, 52.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The loss after this epoch is: 0.15852804129766906\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 87%|████████▋ | 13/15 [11:21<01:44, 52.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The loss after this epoch is: 0.1564776869395773\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 93%|█████████▎| 14/15 [12:19<00:54, 54.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The loss after this epoch is: 0.17367634158021686\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15/15 [13:12<00:00, 52.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The loss after this epoch is: 0.1151368321699927\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hhR7Gex1BK9",
        "outputId": "57a17ae8-adef-4561-8bbb-a0206fa23106"
      },
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([array([[-1.69749915e+01, -1.24787929e+02, -6.52443942e+02, ...,\n",
              "               -1.80137723e+04, -1.55251033e+03, -5.60673889e+06],\n",
              "              [-1.69772276e+01, -1.24783938e+02, -6.52447094e+02, ...,\n",
              "               -1.80137747e+04, -1.55251533e+03, -5.60673889e+06],\n",
              "              [-1.69772186e+01, -1.24785388e+02, -6.52444464e+02, ...,\n",
              "               -1.80137759e+04, -1.55251294e+03, -5.60673889e+06],\n",
              "              ...,\n",
              "              [-1.69768807e+01, -1.24784904e+02, -6.52445089e+02, ...,\n",
              "               -1.80137719e+04, -1.55251244e+03, -5.60673889e+06],\n",
              "              [-1.69748415e+01, -1.24786799e+02, -6.52443977e+02, ...,\n",
              "               -1.80137749e+04, -1.55251116e+03, -5.60673889e+06],\n",
              "              [-1.69798755e+01, -1.24784666e+02, -6.52443596e+02, ...,\n",
              "               -1.80137722e+04, -1.55251218e+03, -5.60673889e+06]])   ,\n",
              "       array([[-0.29924753, -0.30440764, -0.29254498, ..., -0.29417291,\n",
              "               -0.29298389, -0.6355123 ],\n",
              "              [-0.29778866, -0.29956263, -0.29529557, ..., -0.29984078,\n",
              "               -0.29962598, -0.63394272],\n",
              "              [-0.29596155, -0.29512222, -0.29987573, ..., -0.29661895,\n",
              "               -0.29652828, -0.63468509],\n",
              "              ...,\n",
              "              [-0.29821443, -0.29810218, -0.29106239, ..., -0.29710616,\n",
              "               -0.29554872, -0.63236897],\n",
              "              [-0.29960424, -0.30059384, -0.30400582, ..., -0.29751691,\n",
              "               -0.29377328, -0.6363041 ],\n",
              "              [-0.30038229, -0.30177166, -0.29262756, ..., -0.29698847,\n",
              "               -0.29558177, -0.63028542]])                             ,\n",
              "       array([[-0.35603675, -0.36114969, -0.36034156, ..., -0.35805804,\n",
              "               -0.35163284, -4.09534904],\n",
              "              [-0.36110599, -0.35195429, -0.35829808, ..., -0.3576915 ,\n",
              "               -0.35131113, -4.09231163],\n",
              "              [-0.35408598, -0.35556385, -0.35872793, ..., -0.35352485,\n",
              "               -0.36102812, -4.09771438],\n",
              "              ...,\n",
              "              [-0.35618788, -0.35952328, -0.35424198, ..., -0.36078628,\n",
              "               -0.35469334, -4.09439258],\n",
              "              [-0.35681312, -0.36250212, -0.36073046, ..., -0.35279153,\n",
              "               -0.35671927, -4.09577164],\n",
              "              [-0.35775912, -0.35373887, -0.3566889 , ..., -0.3606737 ,\n",
              "               -0.3517531 , -4.0971801 ]])                             ,\n",
              "       array([[ 0.03676318,  0.01381966,  0.01608838, ...,  0.04241946,\n",
              "                0.0064685 ,  0.03619146],\n",
              "              [ 0.02928313,  0.02440879,  0.01764481, ...,  0.04921184,\n",
              "                0.0258148 ,  0.11032924],\n",
              "              [ 0.05506714,  0.02810193,  0.03264017, ...,  0.05201969,\n",
              "                0.04650875,  0.1784706 ],\n",
              "              ...,\n",
              "              [-0.01054803,  0.00247965,  0.02113314, ...,  0.01316593,\n",
              "               -0.00659969, -0.15934119],\n",
              "              [ 0.03309531,  0.01092478,  0.00108392, ...,  0.02237801,\n",
              "                0.01264479, -0.01548217],\n",
              "              [ 0.02155826,  0.02922295,  0.01500485, ...,  0.01334014,\n",
              "                0.0164874 , -0.20689174]])                             ],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gradient = neuralNet.BackProp(a,h,train_x[1], train_y[1])\n"
      ],
      "metadata": {
        "id": "uckwQu3vbr9b"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = net - gradient*0.1"
      ],
      "metadata": {
        "id": "trXRsTlNb0_1"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SeXnBQQMbw_U",
        "outputId": "efc6c2b2-af74-4a82-f9e9-3c23fd302cd6"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([array([[ 0.00212451,  0.00348055,  0.00273668, ...,  0.00160233,\n",
              "                0.00401092, -0.10310577],\n",
              "              [ 0.0053647 ,  0.00488688,  0.00141693, ...,  0.00345387,\n",
              "                0.00127243, -0.10183299],\n",
              "              [ 0.00389507,  0.00445134,  0.00076357, ...,  0.00094713,\n",
              "                0.00404774, -0.10074752],\n",
              "              ...,\n",
              "              [ 0.00471922,  0.00443626,  0.0050084 , ...,  0.0010763 ,\n",
              "                0.00512554, -0.10414192],\n",
              "              [ 0.00516156,  0.00476448,  0.00393312, ...,  0.00430722,\n",
              "                0.00099776, -0.10057883],\n",
              "              [ 0.00329868,  0.00151445,  0.00457194, ...,  0.00074116,\n",
              "                0.00449557, -0.10262027]])                             ,\n",
              "       array([[-0.00099111, -0.00721432, -0.00519615, ..., -0.00653088,\n",
              "               -0.00090633, -0.00623443],\n",
              "              [-0.00418149, -0.00042853, -0.00603201, ..., -0.00721158,\n",
              "                0.00273719, -0.00376062],\n",
              "              [-0.0071174 ,  0.00048526, -0.00560948, ..., -0.00343002,\n",
              "                0.00413835, -0.00733703],\n",
              "              ...,\n",
              "              [-0.006337  ,  0.00369387, -0.00298794, ...,  0.00067173,\n",
              "               -0.00419329, -0.01547673],\n",
              "              [-0.00423961,  0.00333128, -0.00763342, ..., -0.00380922,\n",
              "               -0.00684935, -0.01378758],\n",
              "              [-0.00548303, -0.00673192,  0.00283304, ...,  0.00425297,\n",
              "               -0.00277077, -0.01513782]])                             ,\n",
              "       array([[-0.00041182,  0.00542547,  0.00477374, ...,  0.00462883,\n",
              "                0.00438564, -0.00515673],\n",
              "              [ 0.0060436 ,  0.00142882,  0.00654367, ...,  0.00820403,\n",
              "                0.01176557, -0.00731213],\n",
              "              [ 0.01073805,  0.00963001,  0.0094443 , ...,  0.00678066,\n",
              "                0.01037654, -0.00705194],\n",
              "              ...,\n",
              "              [-0.00110749,  0.00841541,  0.01166621, ...,  0.01175088,\n",
              "                0.00994929, -0.01429602],\n",
              "              [ 0.00051189,  0.00598212,  0.00741932, ...,  0.00561926,\n",
              "                0.00896193, -0.0091783 ],\n",
              "              [ 0.01162079,  0.0012447 ,  0.00015271, ...,  0.00290026,\n",
              "                0.00919674, -0.00594587]])                             ,\n",
              "       array([[ 0.06907301,  0.06095291,  0.0514004 , ...,  0.0687515 ,\n",
              "                0.07906688,  0.49152679],\n",
              "              [ 0.01504338,  0.03841592,  0.02650005, ...,  0.02798371,\n",
              "                0.0399895 , -0.01809882],\n",
              "              [-0.00102238,  0.02994231,  0.03949304, ...,  0.01203618,\n",
              "                0.03842931, -0.00422726],\n",
              "              ...,\n",
              "              [ 0.00821973,  0.01807136,  0.0273385 , ...,  0.01417188,\n",
              "                0.00818383, -0.0273144 ],\n",
              "              [ 0.0281483 ,  0.03046215,  0.02588083, ...,  0.03605303,\n",
              "                0.03187495, -0.03392071],\n",
              "              [ 0.01924012,  0.03757207,  0.01874902, ..., -0.00230258,\n",
              "                0.04120515, -0.01121717]])                             ],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.argmax(np.array(h[L]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvzSP-M4btcN",
        "outputId": "001904ed-f2ec-4f76-b167-2678d1f33835"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "while np.argmax(np.array(h[L])):\n",
        "    for i in range(10):\n",
        "        a,h = Algorithms.ForwardProp(net,Functions.sigmoid, Functions.softmax, train_x[4])\n",
        "        gradient = neuralNet.BackProp(a,h,train_x[4], train_y[4])\n",
        "        net = net - 0.1*gradient\n",
        "        print(np.argmax(np.array(h[L])), end = \", \"),"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RcwywNK0blOr",
        "outputId": "d764dc8a-d727-443b-cb91-ba121fe78f26"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3, 3, 0, "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_y[4]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-df0oArs006R",
        "outputId": "4218ed61-af21-4c62-cdac-3b2ec63efbe8"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_acc = 0\n",
        "for i in range(len(test_x)):\n",
        "    a,h = Algorithms.ForwardProp(net, Functions.sigmoid, Functions.softmax, test_x[i])\n",
        "    h = np.array(h)\n",
        "    predY =   np.argmax(h[len(h)-1])\n",
        "    print(predY)\n",
        "    if test_y[i] == predY:\n",
        "        num_acc+=1\n",
        "print(num_acc/len(test_y), end = \" \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0eqMHkIz3to",
        "outputId": "4a35e5fc-75b2-4bb2-de9a-8d103c5bb6c9"
      },
      "execution_count": 279,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "8\n",
            "4\n",
            "8\n",
            "9\n",
            "3\n",
            "8\n",
            "0\n",
            "9\n",
            "4\n",
            "9\n",
            "0\n",
            "9\n",
            "2\n",
            "9\n",
            "8\n",
            "0\n",
            "7\n",
            "9\n",
            "9\n",
            "4\n",
            "8\n",
            "9\n",
            "1\n",
            "0\n",
            "8\n",
            "9\n",
            "8\n",
            "0\n",
            "8\n",
            "9\n",
            "8\n",
            "0\n",
            "0\n",
            "1\n",
            "5\n",
            "4\n",
            "0\n",
            "1\n",
            "9\n",
            "9\n",
            "7\n",
            "4\n",
            "8\n",
            "0\n",
            "9\n",
            "2\n",
            "4\n",
            "8\n",
            "7\n",
            "9\n",
            "8\n",
            "9\n",
            "0\n",
            "9\n",
            "7\n",
            "8\n",
            "8\n",
            "9\n",
            "9\n",
            "3\n",
            "8\n",
            "8\n",
            "4\n",
            "3\n",
            "9\n",
            "3\n",
            "1\n",
            "0\n",
            "9\n",
            "8\n",
            "2\n",
            "8\n",
            "8\n",
            "9\n",
            "8\n",
            "8\n",
            "8\n",
            "9\n",
            "9\n",
            "0\n",
            "0\n",
            "9\n",
            "8\n",
            "2\n",
            "1\n",
            "8\n",
            "0\n",
            "3\n",
            "8\n",
            "7\n",
            "0\n",
            "8\n",
            "4\n",
            "9\n",
            "1\n",
            "1\n",
            "2\n",
            "0\n",
            "8\n",
            "9\n",
            "8\n",
            "9\n",
            "8\n",
            "8\n",
            "0\n",
            "1\n",
            "4\n",
            "0\n",
            "1\n",
            "8\n",
            "0\n",
            "3\n",
            "8\n",
            "8\n",
            "9\n",
            "3\n",
            "8\n",
            "8\n",
            "8\n",
            "3\n",
            "9\n",
            "1\n",
            "0\n",
            "1\n",
            "8\n",
            "8\n",
            "4\n",
            "0\n",
            "9\n",
            "3\n",
            "8\n",
            "0\n",
            "1\n",
            "8\n",
            "8\n",
            "1\n",
            "0\n",
            "8\n",
            "9\n",
            "0\n",
            "0\n",
            "9\n",
            "8\n",
            "3\n",
            "8\n",
            "8\n",
            "9\n",
            "8\n",
            "9\n",
            "8\n",
            "0\n",
            "8\n",
            "0\n",
            "9\n",
            "9\n",
            "9\n",
            "9\n",
            "9\n",
            "4\n",
            "8\n",
            "8\n",
            "2\n",
            "0\n",
            "1\n",
            "9\n",
            "2\n",
            "9\n",
            "8\n",
            "3\n",
            "1\n",
            "8\n",
            "8\n",
            "8\n",
            "8\n",
            "9\n",
            "0\n",
            "4\n",
            "4\n",
            "2\n",
            "1\n",
            "3\n",
            "8\n",
            "0\n",
            "3\n",
            "9\n",
            "9\n",
            "7\n",
            "9\n",
            "1\n",
            "0\n",
            "4\n",
            "4\n",
            "9\n",
            "0\n",
            "4\n",
            "3\n",
            "1\n",
            "8\n",
            "4\n",
            "8\n",
            "2\n",
            "8\n",
            "9\n",
            "3\n",
            "1\n",
            "8\n",
            "1\n",
            "9\n",
            "2\n",
            "9\n",
            "0\n",
            "2\n",
            "9\n",
            "1\n",
            "8\n",
            "4\n",
            "9\n",
            "8\n",
            "8\n",
            "0\n",
            "9\n",
            "8\n",
            "8\n",
            "8\n",
            "1\n",
            "0\n",
            "4\n",
            "9\n",
            "4\n",
            "9\n",
            "8\n",
            "5\n",
            "9\n",
            "8\n",
            "8\n",
            "0\n",
            "8\n",
            "5\n",
            "9\n",
            "9\n",
            "9\n",
            "8\n",
            "2\n",
            "8\n",
            "2\n",
            "1\n",
            "3\n",
            "3\n",
            "9\n",
            "8\n",
            "9\n",
            "4\n",
            "0\n",
            "8\n",
            "3\n",
            "2\n",
            "8\n",
            "1\n",
            "0\n",
            "4\n",
            "1\n",
            "0\n",
            "7\n",
            "4\n",
            "9\n",
            "9\n",
            "2\n",
            "8\n",
            "9\n",
            "4\n",
            "8\n",
            "9\n",
            "1\n",
            "2\n",
            "1\n",
            "9\n",
            "4\n",
            "8\n",
            "8\n",
            "8\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "1\n",
            "2\n",
            "4\n",
            "7\n",
            "0\n",
            "1\n",
            "5\n",
            "8\n",
            "2\n",
            "7\n",
            "1\n",
            "9\n",
            "8\n",
            "8\n",
            "1\n",
            "2\n",
            "8\n",
            "0\n",
            "9\n",
            "3\n",
            "8\n",
            "9\n",
            "0\n",
            "8\n",
            "8\n",
            "2\n",
            "7\n",
            "9\n",
            "9\n",
            "4\n",
            "2\n",
            "9\n",
            "1\n",
            "9\n",
            "1\n",
            "8\n",
            "2\n",
            "3\n",
            "8\n",
            "8\n",
            "4\n",
            "0\n",
            "4\n",
            "8\n",
            "8\n",
            "2\n",
            "9\n",
            "3\n",
            "8\n",
            "8\n",
            "0\n",
            "1\n",
            "9\n",
            "9\n",
            "8\n",
            "3\n",
            "8\n",
            "3\n",
            "4\n",
            "8\n",
            "0\n",
            "2\n",
            "1\n",
            "3\n",
            "3\n",
            "0\n",
            "9\n",
            "0\n",
            "8\n",
            "9\n",
            "8\n",
            "9\n",
            "3\n",
            "8\n",
            "9\n",
            "9\n",
            "4\n",
            "8\n",
            "8\n",
            "0\n",
            "1\n",
            "4\n",
            "4\n",
            "0\n",
            "9\n",
            "8\n",
            "1\n",
            "2\n",
            "1\n",
            "0\n",
            "3\n",
            "2\n",
            "8\n",
            "8\n",
            "8\n",
            "0\n",
            "8\n",
            "3\n",
            "4\n",
            "4\n",
            "8\n",
            "8\n",
            "9\n",
            "4\n",
            "8\n",
            "2\n",
            "4\n",
            "7\n",
            "0\n",
            "1\n",
            "8\n",
            "0\n",
            "9\n",
            "9\n",
            "0\n",
            "0\n",
            "3\n",
            "8\n",
            "1\n",
            "1\n",
            "2\n",
            "4\n",
            "2\n",
            "9\n",
            "8\n",
            "1\n",
            "0\n",
            "1\n",
            "8\n",
            "8\n",
            "8\n",
            "8\n",
            "0\n",
            "4\n",
            "0\n",
            "4\n",
            "9\n",
            "3\n",
            "8\n",
            "8\n",
            "4\n",
            "8\n",
            "9\n",
            "2\n",
            "8\n",
            "8\n",
            "4\n",
            "4\n",
            "0\n",
            "8\n",
            "8\n",
            "8\n",
            "2\n",
            "8\n",
            "9\n",
            "2\n",
            "9\n",
            "7\n",
            "9\n",
            "2\n",
            "1\n",
            "0\n",
            "4\n",
            "1\n",
            "9\n",
            "8\n",
            "1\n",
            "8\n",
            "3\n",
            "9\n",
            "8\n",
            "8\n",
            "9\n",
            "8\n",
            "1\n",
            "8\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "4\n",
            "3\n",
            "8\n",
            "0\n",
            "4\n",
            "2\n",
            "8\n",
            "3\n",
            "3\n",
            "8\n",
            "0\n",
            "3\n",
            "0\n",
            "9\n",
            "1\n",
            "8\n",
            "3\n",
            "1\n",
            "8\n",
            "8\n",
            "8\n",
            "4\n",
            "9\n",
            "0\n",
            "3\n",
            "8\n",
            "8\n",
            "1\n",
            "0\n",
            "9\n",
            "0\n",
            "9\n",
            "8\n",
            "1\n",
            "8\n",
            "8\n",
            "8\n",
            "3\n",
            "0\n",
            "8\n",
            "8\n",
            "8\n",
            "5\n",
            "4\n",
            "8\n",
            "8\n",
            "0\n",
            "3\n",
            "8\n",
            "1\n",
            "9\n",
            "9\n",
            "0\n",
            "3\n",
            "1\n",
            "2\n",
            "9\n",
            "8\n",
            "3\n",
            "8\n",
            "9\n",
            "8\n",
            "0\n",
            "9\n",
            "5\n",
            "1\n",
            "7\n",
            "2\n",
            "1\n",
            "8\n",
            "2\n",
            "4\n",
            "3\n",
            "3\n",
            "8\n",
            "9\n",
            "1\n",
            "3\n",
            "9\n",
            "4\n",
            "9\n",
            "8\n",
            "3\n",
            "2\n",
            "8\n",
            "3\n",
            "3\n",
            "8\n",
            "8\n",
            "8\n",
            "9\n",
            "9\n",
            "4\n",
            "9\n",
            "8\n",
            "1\n",
            "2\n",
            "9\n",
            "3\n",
            "0\n",
            "8\n",
            "8\n",
            "2\n",
            "9\n",
            "1\n",
            "4\n",
            "0\n",
            "5\n",
            "9\n",
            "4\n",
            "1\n",
            "4\n",
            "0\n",
            "3\n",
            "3\n",
            "0\n",
            "3\n",
            "0\n",
            "8\n",
            "8\n",
            "8\n",
            "8\n",
            "7\n",
            "8\n",
            "0\n",
            "8\n",
            "9\n",
            "4\n",
            "4\n",
            "9\n",
            "9\n",
            "4\n",
            "9\n",
            "1\n",
            "8\n",
            "8\n",
            "8\n",
            "9\n",
            "8\n",
            "3\n",
            "8\n",
            "9\n",
            "3\n",
            "9\n",
            "0\n",
            "2\n",
            "1\n",
            "8\n",
            "4\n",
            "4\n",
            "0\n",
            "0\n",
            "9\n",
            "8\n",
            "3\n",
            "1\n",
            "9\n",
            "1\n",
            "3\n",
            "7\n",
            "0\n",
            "8\n",
            "2\n",
            "1\n",
            "8\n",
            "3\n",
            "0\n",
            "8\n",
            "2\n",
            "8\n",
            "0\n",
            "9\n",
            "0\n",
            "0\n",
            "4\n",
            "1\n",
            "2\n",
            "2\n",
            "9\n",
            "0\n",
            "1\n",
            "4\n",
            "9\n",
            "8\n",
            "8\n",
            "9\n",
            "2\n",
            "0\n",
            "9\n",
            "8\n",
            "8\n",
            "9\n",
            "0\n",
            "9\n",
            "0\n",
            "0\n",
            "0\n",
            "8\n",
            "1\n",
            "3\n",
            "0\n",
            "8\n",
            "8\n",
            "8\n",
            "2\n",
            "5\n",
            "0\n",
            "8\n",
            "9\n",
            "1\n",
            "9\n",
            "1\n",
            "0\n",
            "1\n",
            "8\n",
            "0.5766 "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_y)"
      ],
      "metadata": {
        "id": "o84TF_u3yVv9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "074c498e-5519-4f15-dc86-049be1c6382a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gradient[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUcuGN-SzSyq",
        "outputId": "f6614617-7859-48d3-dff7-623f7622625c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20, 785)"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in gradient[0]:\n",
        "    print(i, end = \" \")"
      ],
      "metadata": {
        "id": "OY0Zy9psDtk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gradient[0]"
      ],
      "metadata": {
        "id": "I3YdLFqZU8Ci",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52462d9f-b600-46f9-9403-a93bdb09c341"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gradaL = -(Functions.onehot(train_y[1])-h[len(h)-1])"
      ],
      "metadata": {
        "id": "kPDcvGye0lF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gradhL_1 = np.matmul(np.transpose(net[(len(net)-1)]),aL)"
      ],
      "metadata": {
        "id": "PcCtxLxzFKVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gradaL_1 = np.multiply(net[len(net)-1][:,:len(net[len(net)-1][0])-1], Functions.derivative_sigmoid(a[len(net)-2]))"
      ],
      "metadata": {
        "id": "W9xEUE6XKesg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gradW = np.outer(gradaL,h[len(net)-2].T)\n",
        "gradB = gradaL\n"
      ],
      "metadata": {
        "id": "K72jzJOiJSR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gradB.resize((len(gradB),1))"
      ],
      "metadata": {
        "id": "XDbZT58hUOWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gradB.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3-e7uWUUYd_",
        "outputId": "2dccdc5f-1f4f-4de9-ef20-bdc5f9e31910"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.append(gradW,gradB.resize((10,1)),axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdZHmwrRR2rx",
        "outputId": "df26a5dc-df87-4147-c06a-0d3df7c1d70d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.49974422, -0.49987732, -0.50045317, -0.49836884, -0.50235192,\n",
              "        -0.50235109, -0.49928474, -0.49920566, -0.49775972, -0.49974985,\n",
              "        -0.49777699, -0.50029926, -0.49888012, -0.50086685, -0.50025429,\n",
              "        -0.49987588, -0.49888532, -0.49982542, -0.50135421, -0.50049566,\n",
              "        -0.89963249],\n",
              "       [ 0.05646266,  0.0564777 ,  0.05654276,  0.05630726,  0.05675728,\n",
              "         0.05675719,  0.05641074,  0.05640181,  0.05623844,  0.05646329,\n",
              "         0.05624039,  0.05652537,  0.05636503,  0.0565895 ,  0.05652029,\n",
              "         0.05647753,  0.05636562,  0.05647183,  0.05664456,  0.05654756,\n",
              "         0.10164328],\n",
              "       [ 0.05479879,  0.05481338,  0.05487653,  0.05464797,  0.05508473,\n",
              "         0.05508464,  0.05474841,  0.05473974,  0.05458118,  0.05479941,\n",
              "         0.05458308,  0.05485965,  0.05470404,  0.05492189,  0.05485472,\n",
              "         0.05481323,  0.05470461,  0.05480769,  0.05497533,  0.05488119,\n",
              "         0.09864801],\n",
              "       [ 0.05828847,  0.058304  ,  0.05837116,  0.05812805,  0.05859263,\n",
              "         0.05859253,  0.05823488,  0.05822566,  0.05805701,  0.05828913,\n",
              "         0.05805902,  0.05835321,  0.05818769,  0.05841941,  0.05834797,\n",
              "         0.05830383,  0.0581883 ,  0.05829794,  0.05847626,  0.05837612,\n",
              "         0.10493009],\n",
              "       [ 0.05363827,  0.05365255,  0.05371436,  0.05349065,  0.05391816,\n",
              "         0.05391807,  0.05358895,  0.05358046,  0.05342527,  0.05363887,\n",
              "         0.05342712,  0.05369784,  0.05354552,  0.05375876,  0.05369302,\n",
              "         0.0536524 ,  0.05354608,  0.05364698,  0.05381107,  0.05371892,\n",
              "         0.09655885],\n",
              "       [ 0.05534094,  0.05535568,  0.05541945,  0.05518863,  0.05562971,\n",
              "         0.05562962,  0.05529006,  0.0552813 ,  0.05512118,  0.05534156,\n",
              "         0.05512309,  0.05540241,  0.05524525,  0.05546526,  0.05539743,\n",
              "         0.05535552,  0.05524583,  0.05534993,  0.05551923,  0.05542416,\n",
              "         0.09962398],\n",
              "       [ 0.05590346,  0.05591835,  0.05598276,  0.0557496 ,  0.05619517,\n",
              "         0.05619507,  0.05585206,  0.05584321,  0.05568146,  0.05590409,\n",
              "         0.05568339,  0.05596555,  0.0558068 ,  0.05602904,  0.05596052,\n",
              "         0.05591818,  0.05580738,  0.05591254,  0.05608356,  0.05598752,\n",
              "         0.10063661],\n",
              "       [ 0.05581826,  0.05583313,  0.05589745,  0.05566464,  0.05610953,\n",
              "         0.05610943,  0.05576694,  0.05575811,  0.05559661,  0.05581889,\n",
              "         0.05559854,  0.05588026,  0.05572175,  0.05594365,  0.05587524,\n",
              "         0.05583297,  0.05572233,  0.05582733,  0.05599809,  0.0559022 ,\n",
              "         0.10048325],\n",
              "       [ 0.05537143,  0.05538617,  0.05544998,  0.05521904,  0.05566036,\n",
              "         0.05566027,  0.05532052,  0.05531175,  0.05515155,  0.05537205,\n",
              "         0.05515346,  0.05543293,  0.05527569,  0.05549581,  0.05542794,\n",
              "         0.05538601,  0.05527626,  0.05538042,  0.05554981,  0.05545469,\n",
              "         0.09967886],\n",
              "       [ 0.05412194,  0.05413635,  0.05419872,  0.05397299,  0.05440435,\n",
              "         0.05440426,  0.05407218,  0.05406361,  0.05390702,  0.05412255,\n",
              "         0.05390889,  0.05418205,  0.05402836,  0.05424352,  0.05417718,\n",
              "         0.0541362 ,  0.05402892,  0.05413073,  0.0542963 ,  0.05420332,\n",
              "         0.09742955]])"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gradaL.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDIXmbKHNo25",
        "outputId": "6d44560f-8280-4146-b7bd-dedf9486be76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10,)"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a[len(net)-2].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7_71o1X06hg",
        "outputId": "5899546a-1304-47a4-e96b-f0f8edbce0ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20,)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weights = net[0][:,:len(net[0][0])-1]\n",
        "bias = net[0][:,len(net[0][0])-1]\n",
        "temp = np.matmul(weights,train_x[0])+bias\n",
        "temp = temp/np.linalg.norm(temp)\n",
        "a = []\n",
        "a.append(temp)\n",
        "h = []\n",
        "h.append(Functions.sigmoid(a[0]))"
      ],
      "metadata": {
        "id": "k65CaJIefUTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights = net[L][:,:len(net[L][0])-1]\n",
        "bias = net[L][:,len(net[L][0])-1]\n",
        "temp = np.matmul(weights,h[0])+bias\n",
        "temp = temp/np.linalg.norm(temp)"
      ],
      "metadata": {
        "id": "ZafDhQ7LmcUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "L = len(net)-1"
      ],
      "metadata": {
        "id": "epu4dvEji2mG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tg0dnhLfheDu",
        "outputId": "93e6fdc7-9cf7-4236-b55e-550c4459f101"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 20)"
            ]
          },
          "metadata": {},
          "execution_count": 222
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#The class of FeedForwardNeuralNetwor\n",
        "\n",
        "class FFNet:\n",
        "    #constructor\n",
        "    hidden = []\n",
        "    input = []\n",
        "    output = []\n",
        "    def __init__(self,number_of_hidden_layers, number_of_inputs, number_of_outputs):\n",
        "        self.number_of_inputs = number_of_inputs\n",
        "        self.number_of_hidden_layers = number_of_hidden_layers\n",
        "        self.number_of_outputs = number_of_outputs\n",
        "        #At the same time, the layers input layers mus also be initialized.\n",
        "\n",
        "        input = [0 for i in range(number_of_inputs)]\n",
        "        output = [0 for i in range(number_of_outputs)]\n",
        "        hidden = [[]]\n",
        "\n",
        "        #input and output layers are nothing but simple lists\n",
        "    \n",
        "    #Method for creating layers\n",
        "    def add_hidden_layer(number_of_neurons):\n",
        "        temp_weights = [0 for i in range(number_of_neurons+1)] #The +1 is for bias values\n",
        "        hidden.append(temp_weights)\n",
        "    \n",
        "    def backward_propagate(a,h, pred_y):\n",
        "        delthet[L] = -(exp(y) - pred_y) #with respect to output layer\n",
        "        for k in range(0,L-1,-1):\n",
        "            delthetw = np.matmul(delthet[k], h[k-1].T)\n",
        "            delthetb = delthet[k]\n",
        "            deltheth = np.matmul(weights[k].T, delthet[k])\n",
        "            delthet[k-1] = hadamard(deltheth, preac(a)) \n",
        "\n",
        "    def forward_propagate():\n",
        "        #here, we are calculating the preactivations and activations.\n",
        "        #we then store them in an array and return it.\n",
        "        \n",
        "        for k in range(number_of_levels-1):\n",
        "            a[k] = biases[k] + np.matmul(weights[k], h[k-1])\n",
        "            h[k] = g(a[k])\n",
        "        a[number_of_levels-1] = biases[number_of_levels] + np.matmul(weights[number_of_levels],h[number_of_levels-1])\n",
        "        pred_y = output(a[number_of_levels-1])\n",
        "        return a,h, pred_y\n",
        "\n",
        "\n",
        "    def gradient_descent():\n",
        "        a,h, pred_y = forward_propagate()\n",
        "        delthet = backward_propagate(a,h, pred_y)\n",
        "        thet += delthet\n",
        "\n",
        "    def fit(dataset):\n",
        "        for x,y in dataset:\n",
        "            loss = forward(x,y)\n",
        "            delthet = backward(loss)\n",
        "            thet += learn_rate*delthet\n",
        "    "
      ],
      "metadata": {
        "id": "qohv21-jxL88"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}