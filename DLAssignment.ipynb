{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "vlcWrCZGiPRM"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.datasets import fashion_mnist\n",
    "from tqdm import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OLGP4dlAQjyZ",
    "outputId": "5e331d30-d8a9-46fb-f77a-3cf210f38da3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs22m028\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: t1dq6v7h\n",
      "Sweep URL: https://wandb.ai/cs22m028/my-first-sweep/sweeps/t1dq6v7h\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xpimwsia with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tx: 0.02307510392613686\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ty: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/barenya/Documents/DeepLearningAssignment/5thMarch/wandb/run-20230307_005434-xpimwsia</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs22m028/my-first-sweep/runs/xpimwsia' target=\"_blank\">zany-sweep-1</a></strong> to <a href='https://wandb.ai/cs22m028/my-first-sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cs22m028/my-first-sweep/sweeps/t1dq6v7h' target=\"_blank\">https://wandb.ai/cs22m028/my-first-sweep/sweeps/t1dq6v7h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs22m028/my-first-sweep' target=\"_blank\">https://wandb.ai/cs22m028/my-first-sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs22m028/my-first-sweep/sweeps/t1dq6v7h' target=\"_blank\">https://wandb.ai/cs22m028/my-first-sweep/sweeps/t1dq6v7h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs22m028/my-first-sweep/runs/xpimwsia' target=\"_blank\">https://wandb.ai/cs22m028/my-first-sweep/runs/xpimwsia</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a94ab7055404889a1905726fb49cd98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.012 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.060959…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>score</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>score</td><td>1.00001</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">zany-sweep-1</strong> at: <a href='https://wandb.ai/cs22m028/my-first-sweep/runs/xpimwsia' target=\"_blank\">https://wandb.ai/cs22m028/my-first-sweep/runs/xpimwsia</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230307_005434-xpimwsia/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
     ]
    }
   ],
   "source": [
    "# Import the W&B Python Library and log into W&B\n",
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "# 1: Define objective/training function\n",
    "def objective(config):\n",
    "    score = config.x ** 3 + config.y\n",
    "    return score\n",
    "\n",
    "def main():\n",
    "    wandb.init(project='my-first-sweep')\n",
    "    score = objective(wandb.config)\n",
    "    wandb.log({'score': score})\n",
    "\n",
    "# 2: Define the search space\n",
    "sweep_configuration = {\n",
    "    'method': 'random',\n",
    "    'metric': {'goal': 'minimize', 'name': 'score'},\n",
    "    'parameters': \n",
    "    {\n",
    "        'x': {'max': 0.1, 'min': 0.01},\n",
    "        'y': {'values': [1, 3, 7]},\n",
    "     }\n",
    "}\n",
    "\n",
    "# 3: Start the sweep\n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project='my-first-sweep')\n",
    "wandb.agent(sweep_id, function=main, count=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "id": "TfO2B6v6BVWj"
   },
   "outputs": [],
   "source": [
    "class PreProc:\n",
    "    def __init__(self):\n",
    "        (self.trainx,self.trainy),(self.testx, self.testy) = fashion_mnist.load_data()\n",
    "        \n",
    "    def visualize(self,n):\n",
    "        for i in range(n):\n",
    "            plt.subplot(330+1+i) # ask someone why??\n",
    "            plt.imshow(self.trainx[i], cmap = plt.get_cmap('gray'))\n",
    "        plt.show()\n",
    "        \n",
    "    def flattenAndCentralize(self):\n",
    "        trainx_flattened = np.copy(self.trainx).astype('float64')\n",
    "        testx_flattened = np.copy(self.testx).astype('float64')\n",
    "        trainx_flattened -= np.mean(trainx_flattened)\n",
    "        testx_flattened -= np.mean(testx_flattened)\n",
    "        trainx_flattened.shape = (60000,784)\n",
    "        testx_flattened.shape = (10000,784)\n",
    "        return trainx_flattened,testx_flattened\n",
    "    \n",
    "    def getLabels(self):\n",
    "        return self.trainy, self.testy\n",
    "    \n",
    "    def getInputSize(self):\n",
    "        return len(self.trainx[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "4HhzRi0xhOLc"
   },
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "7BwoVy0bPFA0"
   },
   "outputs": [],
   "source": [
    "class Functions:\n",
    "    @staticmethod\n",
    "    def sigmoid(input):\n",
    "        input = np.clip(input, -100,100)\n",
    "        return  1.0/(1.0+np.exp(-input))\n",
    "    \n",
    "    @staticmethod\n",
    "    def reLU(input):\n",
    "        input = np.clip(input, -1e10,1e10)\n",
    "        return np.maximum(0,input)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tanh(input):\n",
    "        return np.tanh(input)\n",
    "    \n",
    "    @staticmethod\n",
    "    def identity(input):\n",
    "        return input\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax(input):\n",
    "        input = np.clip(input, -100,100)\n",
    "        return np.exp(input)/(np.sum(np.exp(input)))\n",
    "    \n",
    "    @staticmethod\n",
    "    def onehot(input):\n",
    "        result = np.zeros(10)\n",
    "        result[input] = 1\n",
    "        return result\n",
    "    \n",
    "    @staticmethod\n",
    "    def crossEntropyLoss(y,yHat):\n",
    "        loss = (-1/10.0) * np.sum(np.multiply(y, np.log(yHat+1e-10)) + np.multiply((1 - y), np.log(1 - (yHat+1e-10))))\n",
    "        return loss\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative_sigmoid(input):\n",
    "        return Functions.sigmoid(input)*(1-Functions.sigmoid(input))\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative_tanh(input):\n",
    "        return (1 - np.tanh(input)**2)\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative_reLU(input):\n",
    "        answer = np.ones_like(input)\n",
    "        for val in input:\n",
    "            if val < 0:\n",
    "                val = 1\n",
    "        return answer\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative_identity(input):\n",
    "        return 1\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot(input):\n",
    "        plt.plot(input)\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Loss over iterations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "wpFxAmhE9t2C",
    "outputId": "fdd02dd5-6147-4d7a-9d86-9cabebea2f81"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [04:05<00:00, 24.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8433\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    data = PreProc()\n",
    "    #data.visualize(5)\n",
    "    train_x, test_x = data.flattenAndCentralize()\n",
    "    trainx = train_x/255\n",
    "    testx = test_x/255\n",
    "    train_y, test_y = data.getLabels()\n",
    "    neuralNet = FFNet(0, len(trainx[0]), 10)\n",
    "    neuralNet.addHiddenLayer(128, \"uniform\")\n",
    "    neuralNet.addHiddenLayer(128, \"uniform\")\n",
    "    neuralNet.addHiddenLayer(128, \"uniform\")\n",
    "    neuralNet.addOutputLayer(10, \"uniform\")\n",
    "    neuralNet.solidify()\n",
    "    #print(trainx.shape)\n",
    "    weights,biases = neuralNet.fit(\"momentum\",100, 0.1, \"sigmoid\", trainx, train_y, 0.0005, 10)\n",
    "    print(Algorithms.evaluateNetwork(weights,biases,testx, test_y))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([  3.06592309,  -3.59871203,  -7.10145594,  -1.65719082,\n",
       "          -4.23911874,   6.36026775,  -7.06349537,  -8.56527349,\n",
       "          -7.72632331, -11.99777088, -23.72939603,  -4.91606961,\n",
       "         -13.96267147,  -6.78856148,  -4.06686118,  -6.60155975,\n",
       "         -11.91679886,  -8.36956248,  -7.60013696,  -9.18511887,\n",
       "         -15.71414727,  -6.69761018, -20.56809728,  -6.01847878,\n",
       "         -20.4594913 ,  -5.59726265,  -5.85155061,  -6.82302913,\n",
       "          -1.86023994,  -6.82124822,  -6.96632972, -10.21821663,\n",
       "          -5.94249069,   1.20126145,  -7.58635885, -12.37719411,\n",
       "          -7.68695842,   6.82559081,  13.13997912, -11.99512254,\n",
       "         -15.08417779,  -4.44854618,  -2.37709665,   3.11046617,\n",
       "         -10.169183  ,   5.28470467,  -9.13061406,  -6.45434648,\n",
       "         -12.49171158,  -3.48510238,  -8.53991666,  -1.70962607,\n",
       "           7.47979876,  -9.87745419,  -5.98635257, -16.55334519,\n",
       "          -7.64518199,  -3.48597844,  -3.48299332,  -6.49491074,\n",
       "          -7.73773738,   0.67273393, -20.30076374,   6.37424459,\n",
       "          -4.03674878,  -6.9390868 ,  -5.14353158,  -4.91870375,\n",
       "          -6.66640236,  -2.30044348,  -4.15938155, -12.69167506,\n",
       "           0.57028234, -14.86831005, -13.29233257, -19.53175726,\n",
       "           2.12160377,  -4.22536135,   1.42507601, -13.47602564,\n",
       "         -11.95860183,  -5.71689525,   8.43786484,   0.92064885,\n",
       "          -8.7971905 ,  -6.49557674,   0.30909092, -11.93432548,\n",
       "          -5.79564186,  -9.54839718,   6.09846136, -10.4214408 ,\n",
       "         -26.07029599,  -3.86953168,  -3.46361752,  -7.05340538,\n",
       "         -15.727575  ,   5.87778913,  -4.522844  ,  -2.1941042 ,\n",
       "          -8.58983061,  -2.14190009,   1.05257082,  -3.38275329,\n",
       "          -9.41873654, -16.20019224,  -2.28227437,  -2.7266855 ,\n",
       "          -9.90231616,  -5.38114156,  -5.20864593,  -3.3586807 ,\n",
       "         -10.85698813, -11.22084768,   7.07665835,  -9.23832476,\n",
       "         -17.23022905,  -6.32240581,   3.7973252 , -13.7113369 ,\n",
       "         -13.40676874,  -2.54364142, -13.1424812 , -18.42835523,\n",
       "          -4.3514225 ,  -0.31820222,   1.75006816, -18.30535114]),\n",
       "  array([ 1.07524304, -0.06851864, -2.23524624, -2.17336602,  2.07157836,\n",
       "          1.0144859 , -0.75169031, -1.527015  , -0.64219034, -0.41648402,\n",
       "          0.86708553,  0.78524601, -1.48176382,  1.6773199 ,  0.73122243,\n",
       "         -1.52041396,  1.10041147,  1.7349137 ,  1.69330247, -0.10502999,\n",
       "          0.19324357, -1.63563717, -0.53600868,  1.48448596, -1.46913546,\n",
       "         -1.65652203,  0.5977483 , -1.12074528, -1.41799097, -1.95823131,\n",
       "         -0.53218636,  0.95266403, -0.36709452, -1.54925859,  0.85590385,\n",
       "          0.615301  ,  0.39164793, -2.8738262 , -1.44246384,  0.22293393,\n",
       "          0.74143822, -1.73355387, -2.01473616, -0.63184059,  1.26969354,\n",
       "         -0.05651777,  0.90490811, -0.99916377,  1.01663983, -1.67991374,\n",
       "          0.83607663,  2.33080512, -0.73430667,  1.2767661 , -0.59520445,\n",
       "          1.07938075, -1.40663835,  0.47726046,  1.49650523, -0.28324423,\n",
       "          0.32679856,  0.24043401,  0.14708308, -2.41583751, -0.59038878,\n",
       "         -0.83979813, -2.36007476, -0.86725432,  1.4374831 ,  1.49023929,\n",
       "         -1.818448  , -0.70139811,  1.5377157 , -1.75703653, -0.60394327,\n",
       "          1.43831305,  0.40371136,  1.19437773,  0.10378605,  2.19879588,\n",
       "         -1.75661474,  0.75175638, -1.8660323 , -1.58720873, -0.25664383,\n",
       "          0.90534019,  0.80419306,  1.06167009, -0.55576973, -0.55428972,\n",
       "          0.44492499, -1.12762206, -1.39776797,  1.2943846 , -0.03567124,\n",
       "          0.97930501,  0.94594279, -1.93738145,  2.60054044, -2.71833214,\n",
       "         -1.16820623, -1.49816272,  0.92224264,  0.70731815,  1.91973306,\n",
       "          1.47192297, -2.01811531,  0.08821444, -1.65434659,  0.46025753,\n",
       "          0.92573933, -1.18635421, -1.47107355,  1.66186725,  1.26959346,\n",
       "          0.97358643, -0.19163123, -1.76554907, -1.5151693 , -2.83972157,\n",
       "         -0.00904214,  0.19138931, -1.7507938 , -1.0295886 , -1.67593352,\n",
       "         -1.42921871, -1.64510584, -1.69402056]),\n",
       "  array([-1.72594802,  2.51015167,  6.65997854,  1.77670194, -2.19642958,\n",
       "         -3.8555898 ,  9.42153017, -6.26016561, -5.46551355,  5.89878876,\n",
       "         -2.78528214,  1.67614114, -2.3343688 ,  1.63966091, -1.38375314,\n",
       "         -3.06074536,  5.45255946,  1.76002755,  4.74799708,  2.5744756 ,\n",
       "         -1.8350976 ,  1.37522041, -0.20366847,  6.14474306,  1.47189682,\n",
       "          1.32286326,  7.10662691,  1.28375494,  7.03891881,  8.47889187,\n",
       "         -1.76980869,  1.28716058,  7.12497839,  5.27512043,  4.36705937,\n",
       "          7.23416578,  8.95366949, -1.74315394,  3.6802918 ,  0.24212401,\n",
       "          2.10045411, -2.49275917,  2.35664145,  7.6258869 , -2.30038189,\n",
       "          7.06546799, -2.15591944, -1.77771847,  6.08724494,  1.13259513,\n",
       "          0.302877  ,  1.82641201, -1.48458736,  3.83670602, -2.45344038,\n",
       "         -1.72700431,  7.87116546,  1.14780984, -2.95377105,  1.7929182 ,\n",
       "         10.61396362,  5.03372638,  6.49232878, -0.73268846,  5.75669616,\n",
       "          4.77697811, -3.77312214,  2.43163412, -1.75612134,  1.75644631,\n",
       "          6.36067238,  0.40788641, -0.86906242,  0.40535873,  1.08707142,\n",
       "          2.19236579,  1.83154034, -2.35238854,  2.56685178, -3.16247281,\n",
       "          4.55549731,  3.36672102,  7.9091502 , -6.09825945,  7.09407148,\n",
       "         -4.44559304, -3.32991538, -4.99218913,  8.1176366 ,  5.42676236,\n",
       "          8.71639494, -6.70251623, -6.62872761, -3.1804963 ,  3.01690677,\n",
       "          1.52404014, -1.82785803,  4.83375095,  0.05029615,  3.85135543,\n",
       "          5.45837411,  1.65224486,  4.78565184,  0.56695648,  1.1651731 ,\n",
       "          3.07876788,  2.5238973 ,  2.96919569, -6.36431659,  1.89071834,\n",
       "          1.73379962, -0.45640251, -2.73952092,  1.28234343,  6.62760208,\n",
       "          1.50821031,  4.71444582,  0.74397928, -1.6028711 ,  3.10798542,\n",
       "          1.21790195, -1.60338672, -6.3288236 ,  2.52639916, -6.47805119,\n",
       "         -5.61472074,  5.34070683,  1.66468236]),\n",
       "  array([-14.89815193,  -4.65438911, -22.44237383, -11.00552186,\n",
       "          -8.26090669,   7.76385986, -12.48693074,  -2.37744527,\n",
       "          -6.26857073,  -4.11026467])],\n",
       " [array([9.55465015e-01, 2.66303590e-02, 8.23226231e-04, 1.60139457e-01,\n",
       "         1.42153053e-02, 9.98274080e-01, 8.55049848e-04, 1.90575125e-04,\n",
       "         4.40868341e-04, 6.15788592e-06, 4.94827978e-11, 7.27456861e-03,\n",
       "         8.63154330e-07, 1.12532045e-03, 1.68425447e-02, 1.35640553e-03,\n",
       "         6.67724197e-06, 2.31763230e-04, 5.00132637e-04, 1.02543709e-04,\n",
       "         1.49772641e-07, 1.23233655e-03, 1.16785480e-09, 2.42746156e-03,\n",
       "         1.30183464e-09, 3.69430147e-03, 2.86719257e-03, 1.08723559e-03,\n",
       "         1.34675087e-01, 1.08917148e-03, 9.42219420e-04, 3.64979986e-05,\n",
       "         2.61860713e-03, 7.68749112e-01, 5.07067693e-04, 4.21357826e-06,\n",
       "         4.58561066e-04, 9.98915543e-01, 9.99998035e-01, 6.17421558e-06,\n",
       "         2.81206075e-07, 1.15603531e-02, 8.49359475e-02, 9.57322406e-01,\n",
       "         3.83321596e-05, 9.94957032e-01, 1.08287335e-04, 1.57119483e-03,\n",
       "         3.75765547e-06, 2.97390984e-02, 1.95468337e-04, 1.53212222e-01,\n",
       "         9.99435947e-01, 5.13161260e-05, 2.50651426e-03, 6.47103126e-08,\n",
       "         4.78114544e-04, 2.97138303e-02, 2.98000151e-02, 1.50883009e-03,\n",
       "         4.35867029e-04, 6.62115061e-01, 1.52577442e-09, 9.98297995e-01,\n",
       "         1.73484950e-02, 9.68215874e-04, 5.80316598e-03, 7.25557045e-03,\n",
       "         1.27135182e-03, 9.10862387e-02, 1.53770665e-02, 3.07662254e-06,\n",
       "         6.38828321e-01, 3.48959350e-07, 1.68737877e-06, 3.29204376e-09,\n",
       "         8.92985287e-01, 1.44093849e-02, 8.06132941e-01, 1.40422214e-06,\n",
       "         6.40386890e-06, 3.27912139e-03, 9.99783535e-01, 7.15174294e-01,\n",
       "         1.51134310e-04, 1.50782705e-03, 5.76663349e-01, 6.56123282e-06,\n",
       "         3.03155976e-03, 7.13103919e-05, 9.97758713e-01, 2.97860442e-05,\n",
       "         4.76227325e-12, 2.04415627e-02, 3.03653405e-02, 8.63713477e-04,\n",
       "         1.47774976e-07, 9.97206853e-01, 1.07414674e-02, 1.00281185e-01,\n",
       "         1.85953005e-04, 1.05090558e-01, 7.41268261e-01, 3.28388367e-02,\n",
       "         8.11819416e-05, 9.21182892e-08, 9.26016680e-02, 6.14169493e-02,\n",
       "         5.00560982e-05, 4.58147827e-03, 5.43932610e-03, 3.36120507e-02,\n",
       "         1.92691074e-05, 1.33918925e-05, 9.99156122e-01, 9.72309000e-05,\n",
       "         3.28856619e-08, 1.79239993e-03, 9.78061409e-01, 1.10979193e-06,\n",
       "         1.50492086e-06, 7.28548236e-02, 1.96016255e-06, 9.92354883e-09,\n",
       "         1.27244669e-02, 4.21113943e-01, 8.51961399e-01, 1.12224326e-08]),\n",
       "  array([0.74559272, 0.48287704, 0.09662971, 0.10216786, 0.8881099 ,\n",
       "         0.73389713, 0.3204531 , 0.17843085, 0.34475158, 0.3973584 ,\n",
       "         0.7041389 , 0.68680965, 0.18516115, 0.84254932, 0.67507347,\n",
       "         0.17940057, 0.75033719, 0.85003986, 0.84465797, 0.47376661,\n",
       "         0.54816111, 0.16305959, 0.36911655, 0.8152492 , 0.18707406,\n",
       "         0.16022943, 0.64514098, 0.24587307, 0.19497673, 0.12365859,\n",
       "         0.3700071 , 0.72165062, 0.40924327, 0.17519338, 0.70180414,\n",
       "         0.64914908, 0.59667934, 0.0534627 , 0.1911641 , 0.5555038 ,\n",
       "         0.67731028, 0.15013357, 0.11766438, 0.34709331, 0.78069028,\n",
       "         0.48587432, 0.71195708, 0.26910587, 0.73431756, 0.15710689,\n",
       "         0.69763827, 0.91139637, 0.32425037, 0.78189879, 0.3554416 ,\n",
       "         0.74637678, 0.19676482, 0.61710076, 0.81705266, 0.42965859,\n",
       "         0.58098021, 0.5598206 , 0.53670462, 0.08197295, 0.35654566,\n",
       "         0.3015773 , 0.0862683 , 0.29582594, 0.80806459, 0.81611419,\n",
       "         0.13962021, 0.33150232, 0.82313241, 0.14716188, 0.35344205,\n",
       "         0.80819328, 0.59957903, 0.7675231 , 0.52592325, 0.90014133,\n",
       "         0.14721483, 0.67956129, 0.13400149, 0.16977697, 0.43618891,\n",
       "         0.71204568, 0.6908707 , 0.74300957, 0.36452683, 0.36486973,\n",
       "         0.60943193, 0.24460021, 0.19817054, 0.7848884 , 0.49108313,\n",
       "         0.72697029, 0.72029851, 0.12593581, 0.93089635, 0.06190025,\n",
       "         0.23717937, 0.18269971, 0.71549884, 0.6698083 , 0.87210866,\n",
       "         0.81334949, 0.11731401, 0.52203932, 0.16052236, 0.61307527,\n",
       "         0.71621009, 0.23391162, 0.18677949, 0.8404885 , 0.78067315,\n",
       "         0.72583377, 0.45223826, 0.14609672, 0.18017397, 0.05521506,\n",
       "         0.49773948, 0.54770181, 0.1479471 , 0.26316387, 0.15763469,\n",
       "         0.19322045, 0.16177151, 0.15524783]),\n",
       "  array([0.15110661, 0.92485043, 0.99872047, 0.85528914, 0.10007157,\n",
       "         0.02072261, 0.99991904, 0.00190728, 0.00421235, 0.99726474,\n",
       "         0.0581247 , 0.84239288, 0.08831627, 0.83748879, 0.20040691,\n",
       "         0.04475583, 0.99573296, 0.85321311, 0.99140547, 0.9292007 ,\n",
       "         0.13763212, 0.79822228, 0.44925816, 0.99785986, 0.81334552,\n",
       "         0.78965768, 0.99918102, 0.78308827, 0.99912369, 0.99979223,\n",
       "         0.14556612, 0.7836662 , 0.9991959 , 0.99490871, 0.98747048,\n",
       "         0.99927901, 0.99987075, 0.14891277, 0.97540457, 0.56023701,\n",
       "         0.89094731, 0.07636735, 0.91346068, 0.99951258, 0.09109134,\n",
       "         0.99914663, 0.10377937, 0.14458508, 0.99773349, 0.7563175 ,\n",
       "         0.57514567, 0.86133374, 0.18473552, 0.97889069, 0.07918732,\n",
       "         0.15097116, 0.99961856, 0.75911065, 0.04955858, 0.85728468,\n",
       "         0.99997543, 0.99352767, 0.99848728, 0.32460504, 0.99684842,\n",
       "         0.99164892, 0.02246398, 0.91920797, 0.14727678, 0.85276403,\n",
       "         0.99827478, 0.60058097, 0.29544943, 0.59997447, 0.74782985,\n",
       "         0.89956186, 0.86194512, 0.08687611, 0.92869751, 0.04060262,\n",
       "         0.98960002, 0.96664814, 0.99963277, 0.00224174, 0.99917068,\n",
       "         0.01159415, 0.03455905, 0.00674498, 0.99970186, 0.99562194,\n",
       "         0.99983615, 0.00122631, 0.0013201 , 0.03990631, 0.9533321 ,\n",
       "         0.82113264, 0.13849364, 0.99210619, 0.51257139, 0.97919129,\n",
       "         0.9957576 , 0.83919422, 0.99172044, 0.6380606 , 0.76227142,\n",
       "         0.9560084 , 0.92580022, 0.95116293, 0.00171896, 0.86883741,\n",
       "         0.84989779, 0.3878396 , 0.0606812 , 0.78284842, 0.99867842,\n",
       "         0.81879582, 0.99111482, 0.6778654 , 0.16758072, 0.95722094,\n",
       "         0.77169412, 0.16750881, 0.00178095, 0.9259719 , 0.00153444,\n",
       "         0.0036306 , 0.99523038, 0.84086555]),\n",
       "  array([1.43876885e-10, 4.04390265e-06, 7.61337263e-14, 7.05567855e-09,\n",
       "         1.09776660e-07, 9.99948649e-01, 1.60387710e-09, 3.94153001e-05,\n",
       "         8.04952998e-07, 6.96804482e-06])])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Algorithms.ForwardProp(weights,biases, Functions.sigmoid, Functions.softmax, trainx[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qHfSx0jHSqtk",
    "outputId": "5e2dae53-035e-468c-98d7-d29b08d821f1"
   },
   "outputs": [],
   "source": [
    "#The class of FeedForwardNeuralNetwor\n",
    "\n",
    "class FFNet:\n",
    "    #constructor\n",
    "    def __init__(self,number_of_hidden_layers, number_of_inputs, number_of_outputs):\n",
    "        self.number_of_inputs = number_of_inputs\n",
    "        self.number_of_hidden_layers = number_of_hidden_layers\n",
    "        self.number_of_outputs = number_of_outputs\n",
    "        self.input = [0 for i in range(number_of_inputs)]\n",
    "        self.output = [0 for i in range(10)]\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        #self.hidden.append(np.random.random((number_of_inputs+1)))\n",
    "    \n",
    "    #Method for creating layers\n",
    "    def addHiddenLayer(self,number_of_neurons, initialization):\n",
    "        if(len(self.weights) == 0):\n",
    "            temp_weights = np.random.uniform(low = -0.5, high = 0.5, size = (number_of_neurons, self.number_of_inputs))\n",
    "            temp_biases = np.random.uniform(low = -0.5, high = 0.5, size = (number_of_neurons))\n",
    "            if initialization == \"xavier\":\n",
    "                temp_weights = np.random.randn(number_of_neurons, self.number_of_inputs)*np.sqrt(2/(self.number_of_inputs+number_of_neurons))\n",
    "                #temp_biases = np.random.randn(number_of_neurons)*np.sqrt(1/(number_of_neurons))\n",
    "        else:\n",
    "            prev_neurons = len(self.weights[len(self.weights) - 1])\n",
    "            temp_weights = np.random.uniform(low = -0.5, high = 0.5, size = (number_of_neurons, prev_neurons))\n",
    "            temp_biases = np.random.uniform(low = -0.5, high = 0.5, size = (number_of_neurons))\n",
    "            if initialization == \"xavier\":\n",
    "                temp_weights = np.random.randn(number_of_neurons, prev_neurons)*np.sqrt(2/(number_of_neurons+prev_neurons))\n",
    "                #temp_biases = np.random.randn(number_of_neurons)*np.sqrt(1/(number_of_neurons))\n",
    "        temp_weights = temp_weights/np.linalg.norm(temp_weights)\n",
    "        temp_biases = temp_biases/np.linalg.norm(temp_biases)\n",
    "        self.weights.append(temp_weights)\n",
    "        self.biases.append(temp_biases)\n",
    "    \n",
    "    def addOutputLayer(self, number_of_outputs, initialization):\n",
    "        if(len(self.weights) == 0):\n",
    "            #print(\"number of inputs: \"+str(self.number_of_inputs))\n",
    "            temp_weights = np.random.uniform(low = -0.5, high = 0.5, size = (number_of_outputs, self.number_of_inputs))\n",
    "            temp_biases = np.random.random(low = -0.5, high = 0.5, size = (number_of_outputs))\n",
    "            if initialization == \"xavier\":\n",
    "                temp_weights = np.random.randn(number_of_outputs, self.number_of_inputs)*np.sqrt(2/(self.number_of_inputs+number_of_outputs))\n",
    "        else:\n",
    "            prev_neurons = len(self.weights[len(self.weights) - 1])\n",
    "            temp_weights = np.random.uniform(low = -0.5, high = 0.5, size = (number_of_outputs, prev_neurons))\n",
    "            temp_biases = np.random.uniform(low = -0.5, high = 0.5, size = (number_of_outputs))\n",
    "            if initialization == \"xavier\":\n",
    "                temp_weights = np.random.randn(number_of_outputs, prev_neurons)*np.sqrt(2/(prev_neurons+number_of_outputs))\n",
    "                \n",
    "        temp_weights = temp_weights/np.linalg.norm(temp_weights)\n",
    "        temp_biases = temp_biases/np.linalg.norm(temp_biases)\n",
    "        self.weights.append(temp_weights)\n",
    "        self.biases.append(temp_biases)\n",
    "\n",
    "    def solidify(self):\n",
    "        self.weights = np.array(self.weights, dtype = object)\n",
    "        self.biases = np.array(self.biases, dtype = object)\n",
    "\n",
    "    def getNetwork(self):\n",
    "        return self.weights,self.biases\n",
    "    \n",
    "    def ForwardProp(self, activate, output, inputLayer):\n",
    "        return Algorithms.ForwardProp(self.network, activate, output, inputLayer)\n",
    "    \n",
    "    def lossCalc(self, lossFunction, Y):\n",
    "        predY = self.historyA[(len(self.historyA)-1)]\n",
    "        return lossFunction(Y,self.predY)\n",
    "\n",
    "    def BackProp(self, a, h, dataPoint, dataLabel):\n",
    "        return Algorithms.BackProp(self.network, a, h, dataPoint, dataLabel)\n",
    "    \n",
    "    def fit(self, optimizer, batchSize, learningRate, activation, trainx, train_y, decay, epochs):\n",
    "        \n",
    "        #break data into training and validation\n",
    "        indices = np.arange(len(trainx))\n",
    "        np.random.shuffle(indices)\n",
    "        trainx = trainx[indices]\n",
    "        train_y = train_y[indices]\n",
    "        \n",
    "        valTest_x = trainx[int(0.9*len(trainx)):]\n",
    "        valTest_y = train_y[int(0.9*len(train_y)):]\n",
    "        \n",
    "        trainx = trainx[:int(0.9*len(trainx))]\n",
    "        train_y = train_y[:int(0.9*len(train_y))]\n",
    "        \n",
    "        if activation == \"relu\":\n",
    "            activate = Functions.reLU\n",
    "            derivative = Functions.derivative_reLU\n",
    "            output = Functions.softmax\n",
    "        elif activation == \"tanh\":\n",
    "            activate = Functions.tanh\n",
    "            derivative = Functions.derivative_tanh\n",
    "            output = Functions.softmax\n",
    "        elif activation == \"identity\":\n",
    "            activate = Functions.identity\n",
    "            derivative = Functions.derivative_identity\n",
    "            output = Functions.softmax\n",
    "        else:\n",
    "            activate = Functions.sigmoid\n",
    "            derivative = Functions.derivative_sigmoid\n",
    "            output = Functions.softmax\n",
    "        \n",
    "        #print(optimizer)\n",
    "        \n",
    "        if optimizer == \"momentum\":\n",
    "            self.weights, self.biases = Algorithms.miniBatchMGD(self.weights,self.biases , batchSize, learningRate, activate, output, derivative,  trainx, train_y, valTest_x, valTest_y, decay, epochs)\n",
    "        elif optimizer == \"nag\":\n",
    "            self.weights, self.biases = Algorithms.miniBatchNAG(self.weights,self.biases , batchSize, learningRate,activate, output, derivative , trainx, train_y, valTest_x, valTest_y, decay, epochs)\n",
    "        elif optimizer == \"rmsprop\":\n",
    "            self.weights, self.biases = Algorithms.RMSProp(self.weights,self.biases , batchSize, learningRate, activate, output, derivative,  trainx, train_y, valTest_x, valTest_y, decay, epochs)\n",
    "        elif optimizer == \"adam\":\n",
    "            self.weights, self.biases = Algorithms.ADAM(self.weights,self.biases , batchSize, learningRate,activate, output, derivative,  trainx, train_y, valTest_x, valTest_y, decay, epochs)\n",
    "        elif optimizer == \"nadam\":\n",
    "            self.weights, self.biases = Algorithms.NADAM(self.weights,self.biases , batchSize, learningRate, activate, output, derivative,  trainx, train_y, valTest_x, valTest_y, decay, epochs)\n",
    "        else:\n",
    "            self.weights, self.biases = Algorithms.miniBatchGD(self.weights,self.biases , batchSize, learningRate, activate, output, derivative , trainx, train_y, valTest_x, valTest_y, decay, epochs)\n",
    "        \n",
    "        return self.weights,self.biases\n",
    "            \n",
    "    def evaluateNetwork(self, testx, tes_ty):\n",
    "        Algorithms.evaluateNetwork(self.weights, self.biases, testx, test_y)        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "id": "Bu5XtsgmjyaH"
   },
   "outputs": [],
   "source": [
    "class Algorithms:\n",
    "    @staticmethod\n",
    "    def ForwardProp(weights, bias, activate, output, inputLayer):\n",
    "        L = len(weights)-1\n",
    "        a = []\n",
    "        h = []\n",
    "        a.append(np.matmul(weights[0],inputLayer)+bias[0])\n",
    "        h.append(activate(a[0]))\n",
    "        for k in range(1,L):\n",
    "            a.append(np.matmul(weights[k],h[k-1].T)+bias[k])\n",
    "            h.append(activate(a[k]))\n",
    "        a.append(np.matmul(weights[L],h[L-1].T)+bias[L])\n",
    "        h.append(output(a[L]))\n",
    "        return a,h\n",
    "    @staticmethod\n",
    "    def BackProp(weights, biases, a, h, derivative, dataPoint, dataLabel):\n",
    "        L = len(weights)-1\n",
    "        gradaL = -(Functions.onehot(dataLabel)-h[len(h)-1])\n",
    "        dw = np.zeros_like(weights)\n",
    "        db = np.zeros_like(biases)\n",
    "        for k in range(L,0,-1):\n",
    "            gradW = np.outer(gradaL, h[k-1].T)\n",
    "            gradB = gradaL\n",
    "            dw[k] = gradW\n",
    "            db[k] = gradB\n",
    "\n",
    "            gradhL_1 = np.matmul(np.transpose(weights[k]),gradaL)\n",
    "            gradaL_1 = np.multiply(gradhL_1, derivative(a[k-1]))\n",
    "            gradaL = gradaL_1\n",
    "        dw[0] = np.outer(gradaL,dataPoint.T)\n",
    "        db[0] = gradaL\n",
    "        return dw, db\n",
    "\n",
    "    @staticmethod\n",
    "    def miniBatchMGD(weights, biases, batchSize, learningRate, activate, output, derivative, dataPoints, dataLabels, valTest_x, valTest_y, decay, epochs):\n",
    "        lossTrack = []\n",
    "        validation = []\n",
    "        beta = 0.9\n",
    "        prevWeights = np.zeros_like(weights)\n",
    "        prevBiases = np.zeros_like(biases)\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            indices = np.arange(len(dataPoints))\n",
    "            np.random.shuffle(indices)\n",
    "            batchX = dataPoints[indices]\n",
    "            batchY = dataLabels[indices]\n",
    "            for i in range(math.ceil(len(dataPoints)/batchSize)):\n",
    "                trainer = batchX[i*batchSize:i*batchSize+batchSize]\n",
    "                labeler = batchY[i*batchSize:i*batchSize+batchSize]\n",
    "                batchLoss = 0.0\n",
    "                dw = np.zeros_like(weights)\n",
    "                db = np.zeros_like(biases)\n",
    "                for data in range(batchSize):\n",
    "                    a,h = Algorithms.ForwardProp(weights, biases , activate, output, trainer[data])\n",
    "                    currWeights, currBiases = Algorithms.BackProp(weights, biases, a, h, derivative, trainer[data], labeler[data])\n",
    "                    batchLoss += Functions.crossEntropyLoss(Functions.onehot(labeler[data]), h[-1])\n",
    "                    dw += currWeights\n",
    "                    db += currBiases\n",
    "                batchLoss /= batchSize\n",
    "                dw /= batchSize\n",
    "                db /= batchSize\n",
    "                momentumWeights = prevWeights*beta + dw*1.0\n",
    "                momentumBiases = prevBiases*beta + db*1.0\n",
    "                weights -= learningRate*(momentumWeights + decay*weights)\n",
    "                biases -= learningRate*(momentumBiases + decay*biases)\n",
    "                prevWeights = momentumWeights\n",
    "                prevBiases = momentumBiases\n",
    "                lossTrack.append(batchLoss)\n",
    "            validation.append(Algorithms.evaluateNetwork(weights, biases, valTest_x, valTest_y))\n",
    "            #print(\"The val_acc after this epoch is: \"+ str(validation))\n",
    "            Functions.plot(validation)\n",
    "        return weights, biases\n",
    "\n",
    "    @staticmethod\n",
    "    def ADAM(weights, biases, batchSize, learningRate, activate, output, derivative, dataPoints, dataLabels, valTest_x, valTest_y, decay, epochs):\n",
    "        validation = []\n",
    "        lossTrack = []\n",
    "        beta1 = 0.9\n",
    "        beta2 = 0.99\n",
    "        epsilon = 0.000001\n",
    "        m_w = np.zeros_like(weights)\n",
    "        v_w = np.zeros_like(weights)\n",
    "        m_b = np.zeros_like(biases)\n",
    "        v_b = np.zeros_like(biases)\n",
    "        learnerRateW = np.full_like(weights, learningRate)\n",
    "        learnerRateB = np.full_like(biases, learningRate)\n",
    "        i = 0\n",
    "        \n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            indices = np.arange(len(dataPoints))\n",
    "            np.random.shuffle(indices)\n",
    "            batchX = dataPoints[indices]\n",
    "            batchY = dataLabels[indices]\n",
    "            for i in range(math.ceil(len(dataPoints)/batchSize)):\n",
    "                trainer = batchX[i*batchSize:i*batchSize+batchSize]\n",
    "                labeler = batchY[i*batchSize:i*batchSize+batchSize]\n",
    "                batchLoss = 0.0\n",
    "                dw = np.zeros_like(weights)\n",
    "                db = np.zeros_like(biases)\n",
    "                for data in range(batchSize):\n",
    "                    a,h = Algorithms.ForwardProp(weights, biases,activate, output, trainer[data])\n",
    "                    currWeights, currBiases = Algorithms.BackProp(weights, biases , a, h, derivative, trainer[data], labeler[data])\n",
    "                    batchLoss += Functions.crossEntropyLoss(Functions.onehot(labeler[data]), h[-1])\n",
    "                    dw += currWeights\n",
    "                    db += currBiases\n",
    "                batchLoss /= batchSize\n",
    "                dw /= batchSize\n",
    "                db /= batchSize\n",
    "                m_w = beta1*m_w + (1-beta1)*dw\n",
    "                m_b = beta1*m_b + (1-beta1)*db\n",
    "                v_w = v_w*beta2 + (1-beta2)*dw**2\n",
    "                v_b = v_b*beta2 + (1-beta2)*db**2\n",
    "                \n",
    "                m_w_hat = m_w/(1 - np.power(beta1, i+1))\n",
    "                m_b_hat = m_b/(1 - np.power(beta1, i+1))\n",
    "                v_w_hat = v_w/(1 - np.power(beta2, i+1))\n",
    "                v_b_hat = v_b/(1 - np.power(beta2, i+1))\n",
    "                \n",
    "                i+=1\n",
    "                \n",
    "                tempW = np.zeros_like(m_w)\n",
    "                tempB = np.zeros_like(m_b)\n",
    "                for i in range(len(dw)):\n",
    "                    tempW[i] = np.sqrt(v_w_hat[i])\n",
    "                    tempB[i] = np.sqrt(v_b_hat[i])\n",
    "                weights = weights - ((learnerRateW*(dw + decay*weights))/(tempW + epsilon))\n",
    "                biases = biases - (learnerRateB*(db + decay*biases))/(tempB+epsilon)\n",
    "                lossTrack.append(batchLoss)\n",
    "            validation.append(Algorithms.evaluateNetwork(weights, biases, valTest_x, valTest_y))\n",
    "            #print(\"The val_acc after this epoch is: \"+ str(validation))\n",
    "            Functions.plot(validation)\n",
    "            #print(\"The loss after this epoch is: \"+ str(batchLoss))\n",
    "        return weights,biases\n",
    "\n",
    "    @staticmethod\n",
    "    def NADAM(weights, biases, batchSize, learningRate, activate, output, derivative, dataPoints, dataLabels, valTest_x, valTest_y, decay, epochs):\n",
    "        lossTrack = []\n",
    "        validation = []\n",
    "        beta1 = 0.9\n",
    "        beta2 = 0.99\n",
    "        epsilon = 0.000001\n",
    "        m_w = np.zeros_like(weights)\n",
    "        v_w = np.zeros_like(weights)\n",
    "        m_b = np.zeros_like(biases)\n",
    "        v_b = np.zeros_like(biases)\n",
    "        learnerRateW = np.full_like(weights, learningRate)\n",
    "        learnerRateB = np.full_like(biases, learningRate)\n",
    "        i = 0\n",
    "        \n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            indices = np.arange(len(dataPoints))\n",
    "            np.random.shuffle(indices)\n",
    "            batchX = dataPoints[indices]\n",
    "            batchY = dataLabels[indices]\n",
    "            for i in range(math.ceil(len(dataPoints)/batchSize)):\n",
    "                trainer = batchX[i*batchSize:i*batchSize+batchSize]\n",
    "                labeler = batchY[i*batchSize:i*batchSize+batchSize]\n",
    "                batchLoss = 0.0\n",
    "                dw = np.zeros_like(weights)\n",
    "                db = np.zeros_like(biases)\n",
    "                for data in range(batchSize):\n",
    "                    a,h = Algorithms.ForwardProp(weights - v_w*(beta1), biases - v_b*(beta1), activate, output, trainer[data])\n",
    "                    currWeights, currBiases = Algorithms.BackProp(weights - v_w*(beta1), biases - v_b*(beta1), a, h, derivative, trainer[data], labeler[data])\n",
    "                    batchLoss += Functions.crossEntropyLoss(Functions.onehot(labeler[data]), h[-1])\n",
    "                    dw += currWeights\n",
    "                    db += currBiases\n",
    "                batchLoss /= batchSize\n",
    "                dw /= batchSize\n",
    "                db /= batchSize\n",
    "                m_w = beta1*m_w + (1-beta1)*dw\n",
    "                m_b = beta1*m_b + (1-beta1)*db\n",
    "                v_w = v_w*beta2 + (1-beta2)*dw**2\n",
    "                v_b = v_b*beta2 + (1-beta2)*db**2\n",
    "                \n",
    "                m_w_hat = m_w/(1 - np.power(beta1, i+1))\n",
    "                m_b_hat = m_b/(1 - np.power(beta1, i+1))\n",
    "                v_w_hat = v_w/(1 - np.power(beta2, i+1))\n",
    "                v_b_hat = v_b/(1 - np.power(beta2, i+1))\n",
    "                \n",
    "                i+=1\n",
    "                \n",
    "                tempW = np.zeros_like(m_w)\n",
    "                tempB = np.zeros_like(m_b)\n",
    "                for j in range(len(dw)):\n",
    "                    tempW[j] = np.sqrt(v_w_hat[j])\n",
    "                    tempB[j] = np.sqrt(v_b_hat[j])\n",
    "                weights = weights - ((learnerRateW*(dw + decay*weights))/(tempW + epsilon))\n",
    "                biases = biases - (learnerRateB*(db + decay*biases))/(tempB+epsilon)\n",
    "                lossTrack.append(batchLoss)\n",
    "            validation.append(Algorithms.evaluateNetwork(weights, biases, valTest_x, valTest_y))\n",
    "            #print(\"The val_acc after this epoch is: \"+ str(validation))\n",
    "            Functions.plot(validation)\n",
    "            #print(\"The loss after this epoch is: \"+ str(batchLoss))\n",
    "        return weights,biases\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def miniBatchNAG(weights, biases, batchSize, learningRate, activate, output, derivative, dataPoints, dataLabels, valTest_x, valTest_y, decay, epochs):\n",
    "        lossTrack = []\n",
    "        validation = []\n",
    "        beta = 0.9\n",
    "        prevWeights = np.zeros_like(weights)\n",
    "        prevBiases = np.zeros_like(biases)\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            indices = np.arange(len(dataPoints))\n",
    "            np.random.shuffle(indices)\n",
    "            batchX = dataPoints[indices]\n",
    "            batchY = dataLabels[indices]\n",
    "            for i in range(math.ceil(len(dataPoints)/batchSize)):\n",
    "                trainer = batchX[i*batchSize:i*batchSize+batchSize]\n",
    "                labeler = batchY[i*batchSize:i*batchSize+batchSize]\n",
    "                batchLoss = 0.0\n",
    "                tempWeights = np.zeros_like(weights)\n",
    "                tempBiases = np.zeros_like(biases)\n",
    "                for data in range(batchSize):\n",
    "                    a,h = Algorithms.ForwardProp(weights-prevWeights, biases-prevBiases, activate, output, trainer[data])\n",
    "                    currWeights, currBiases = Algorithms.BackProp(weights-prevWeights, biases-prevBiases, a, h, derivative, trainer[data], labeler[data])\n",
    "                    batchLoss += Functions.crossEntropyLoss(Functions.onehot(labeler[data]), h[-1])\n",
    "                    tempWeights += currWeights\n",
    "                    tempBiases += currBiases\n",
    "                batchLoss /= batchSize\n",
    "                tempWeights /= batchSize\n",
    "                tempBiases /= batchSize\n",
    "                momentumWeights = beta*prevWeights + tempWeights*1.0\n",
    "                momentumBiases = beta*prevBiases + tempBiases*1.0\n",
    "                weights = weights - learningRate*(momentumWeights + decay*weights) \n",
    "                biases = biases - learningRate*(momentumBiases + decay*biases)\n",
    "                prevWeights = momentumWeights\n",
    "                prevBiases = momentumBiases\n",
    "                lossTrack.append(batchLoss)\n",
    "            validation.append(Algorithms.evaluateNetwork(weights, biases, valTest_x, valTest_y))\n",
    "            #print(\"The val_acc after this epoch is: \"+ str(validation))\n",
    "            Functions.plot(validation)\n",
    "            #print(\"The loss after this epoch is: \"+ str(batchLoss))\n",
    "            Functions.plot(lossTrack)\n",
    "        return net\n",
    "    \n",
    "    @staticmethod\n",
    "    def RMSProp(weights, biases, batchSize, learningRate, activate, output, derivative, dataPoints, dataLabels, valTest_x, valTest_y, decay, epochs):\n",
    "        lossTrack = []\n",
    "        validation = []\n",
    "        beta = 0.5\n",
    "        epsilon = 0.000001\n",
    "        momentumWeights = np.zeros_like(weights)\n",
    "        momentumBiases = np.zeros_like(biases)\n",
    "        learnerRateW = np.full_like(weights, learningRate)\n",
    "        learnerRateB = np.full_like(biases, learningRate)\n",
    "        \n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            indices = np.arange(len(dataPoints))\n",
    "            np.random.shuffle(indices)\n",
    "            batchX = dataPoints[indices]\n",
    "            batchY = dataLabels[indices]\n",
    "            for i in range(math.ceil(len(dataPoints)/batchSize)):\n",
    "                trainer = batchX[i*batchSize:i*batchSize+batchSize]\n",
    "                labeler = batchY[i*batchSize:i*batchSize+batchSize]\n",
    "                batchLoss = 0.0\n",
    "                dw = np.zeros_like(weights)\n",
    "                db = np.zeros_like(biases)\n",
    "                for data in range(batchSize):\n",
    "                    a,h = Algorithms.ForwardProp(weights, biases, activate, output, trainer[data])\n",
    "                    currWeights, currBiases = Algorithms.BackProp(weights, biases , a, h, derivative, trainer[data], labeler[data])\n",
    "                    batchLoss += Functions.crossEntropyLoss(Functions.onehot(labeler[data]), h[-1])\n",
    "                    dw += currWeights\n",
    "                    db += currBiases\n",
    "                batchLoss /= batchSize\n",
    "                dw /= batchSize\n",
    "                db /= batchSize\n",
    "                momentumWeights = momentumWeights*beta + (1-beta)*dw**2\n",
    "                momentumBiases = momentumBiases*beta + (1-beta)*db**2\n",
    "                tempW = np.zeros_like(momentumWeights)\n",
    "                tempB = np.zeros_like(momentumBiases)\n",
    "                for i in range(len(dw)):\n",
    "                    tempW[i] = np.sqrt(momentumWeights[i])\n",
    "                    tempB[i] = np.sqrt(momentumBiases[i])\n",
    "                weights = weights - ((learnerRateW)*(dw + decay*weights)/(tempW + epsilon))\n",
    "                biases = biases - (learnerRateB*(db + decay*biases))/(tempB+epsilon)\n",
    "                lossTrack.append(batchLoss)\n",
    "            validation.append(Algorithms.evaluateNetwork(weights, biases, valTest_x, valTest_y))\n",
    "            #print(\"The val_acc after this epoch is: \"+ str(validation))\n",
    "            Functions.plot(validation)\n",
    "            #print(\"The loss after this epoch is: \"+ str(batchLoss))\n",
    "        return weights, biases\n",
    "\n",
    "    @staticmethod\n",
    "    def miniBatchGD(weights, biases, batchSize, learningRate, activate, output, derivative, dataPoints, dataLabels, valTest_x, valTest_y, decay, epochs):\n",
    "        validation = []\n",
    "        lossTrack = []\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            indices = np.arange(len(dataPoints))\n",
    "            np.random.shuffle(indices)\n",
    "            batchX = dataPoints[indices]\n",
    "            batchY = dataLabels[indices]\n",
    "            for i in range(math.ceil(len(dataPoints)/batchSize)):\n",
    "                trainer = batchX[i*batchSize:i*batchSize+batchSize]\n",
    "                labeler = batchY[i*batchSize:i*batchSize+batchSize]\n",
    "                batchLoss = 0.0\n",
    "                dw = np.zeros_like(weights)\n",
    "                db = np.zeros_like(biases)\n",
    "                for data in range(batchSize):\n",
    "                    a,h = Algorithms.ForwardProp(weights, biases , activate, output, trainer[data])\n",
    "                    tempw,tempb = Algorithms.BackProp(weights, biases, a, h, derivative, trainer[data], labeler[data])\n",
    "                    dw+=tempw\n",
    "                    db+=tempb\n",
    "                    batchLoss += Functions.crossEntropyLoss(Functions.onehot(labeler[data]), h[-1])\n",
    "                batchLoss /= batchSize\n",
    "                dw /= batchSize\n",
    "                db /= batchSize\n",
    "                weights -= learningRate*(dw + decay*weights)\n",
    "                biases -= learningRate*(db + decay*biases)\n",
    "                lossTrack.append(batchLoss)\n",
    "            print(\"The loss after this epoch is: \"+ str(batchLoss))\n",
    "            validation.append(Algorithms.evaluateNetwork(weights, biases, valTest_x, valTest_y))\n",
    "            #print(\"The val_acc after this epoch is: \"+ str(validation))\n",
    "            Functions.plot(validation)\n",
    "        return weights, biases\n",
    "    \n",
    "    @staticmethod\n",
    "    def evaluateNetwork(weights, biases,test_x, test_y):\n",
    "        num_acc = 0\n",
    "        for i in range(len(test_x)):\n",
    "            a,h = Algorithms.ForwardProp(weights, biases, Functions.sigmoid, Functions.softmax, test_x[i])\n",
    "            h = np.array(h, dtype = object)\n",
    "            predY =   np.argmax(h[len(h)-1])\n",
    "            if test_y[i] == predY:\n",
    "                num_acc+=1\n",
    "        return (num_acc/len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = trainx[:int(0.9*len(trainx))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "valTest = trainx[int(0.9*len(trainx)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "R-YOAwnCmO3I",
    "outputId": "a052aca6-18cc-4d10-a3d6-17257b72ab4e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cC3J4fScHacM"
   },
   "outputs": [],
   "source": [
    "a,h = Algorithms.ForwardProp(net, Functions.sigmoid, Functions.softmax, train_x[500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L1KBO1xmaxu8",
    "outputId": "aa351438-35e1-4e7e-99c3-1a86fae078b7"
   },
   "outputs": [],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 454
    },
    "id": "-DhPxwMbl6qX",
    "outputId": "5f0b4ef2-96a9-4a5b-f724-c8a1d521325e"
   },
   "outputs": [],
   "source": [
    "\n",
    "batchSize = 32\n",
    "gradient = np.zeros_like(net)\n",
    "lossTrack = []\n",
    "for epoch in tqdm(range(15)):\n",
    "    indices = np.arange(len(trainx))\n",
    "    np.random.shuffle(indices)\n",
    "    batchX = trainx[indices]\n",
    "    batchY = train_y[indices]\n",
    "    for i in range(math.ceil(len(trainx)/batchSize)):\n",
    "        trainer = batchX[i*batchSize:i*batchSize+batchSize]\n",
    "        labeler = batchY[i*batchSize:i*batchSize+batchSize]\n",
    "        batchLoss = 0.0\n",
    "        for data in range(batchSize):\n",
    "            a,h = Algorithms.ForwardProp(net, Functions.sigmoid, Functions.softmax, trainer[data])\n",
    "            currGrad = Algorithms.BackProp(net, a, h, trainer[data], labeler[data])\n",
    "            batchLoss += Functions.crossEntropyLoss(Functions.onehot(labeler[data]), h[-1])\n",
    "            gradient += currGrad\n",
    "        batchLoss /= 32\n",
    "        gradient /= 32\n",
    "        net = net - 0.01*gradient \n",
    "        lossTrack.append(batchLoss)\n",
    "    print(\"The loss after this epoch is: \"+ str(batchLoss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-hhR7Gex1BK9",
    "outputId": "57a17ae8-adef-4561-8bbb-a0206fa23106"
   },
   "outputs": [],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uckwQu3vbr9b"
   },
   "outputs": [],
   "source": [
    "gradient = neuralNet.BackProp(a,h,train_x[1], train_y[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "trXRsTlNb0_1"
   },
   "outputs": [],
   "source": [
    "net = net - gradient*0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SeXnBQQMbw_U",
    "outputId": "efc6c2b2-af74-4a82-f9e9-3c23fd302cd6"
   },
   "outputs": [],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CvzSP-M4btcN",
    "outputId": "001904ed-f2ec-4f76-b167-2678d1f33835"
   },
   "outputs": [],
   "source": [
    "np.argmax(np.array(h[L]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RcwywNK0blOr",
    "outputId": "d764dc8a-d727-443b-cb91-ba121fe78f26"
   },
   "outputs": [],
   "source": [
    "while np.argmax(np.array(h[L])):\n",
    "    for i in range(10):\n",
    "        a,h = Algorithms.ForwardProp(net,Functions.sigmoid, Functions.softmax, train_x[4])\n",
    "        gradient = neuralNet.BackProp(a,h,train_x[4], train_y[4])\n",
    "        net = net - 0.1*gradient\n",
    "        print(np.argmax(np.array(h[L])), end = \", \"),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-df0oArs006R",
    "outputId": "4218ed61-af21-4c62-cdac-3b2ec63efbe8"
   },
   "outputs": [],
   "source": [
    "train_y[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V0eqMHkIz3to",
    "outputId": "4a35e5fc-75b2-4bb2-de9a-8d103c5bb6c9"
   },
   "outputs": [],
   "source": [
    "num_acc = 0\n",
    "for i in range(len(test_x)):\n",
    "    a,h = Algorithms.ForwardProp(net, Functions.sigmoid, Functions.softmax, test_x[i])\n",
    "    h = np.array(h)\n",
    "    predY =   np.argmax(h[len(h)-1])\n",
    "    print(predY)\n",
    "    if test_y[i] == predY:\n",
    "        num_acc+=1\n",
    "print(num_acc/len(test_y), end = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o84TF_u3yVv9",
    "outputId": "074c498e-5519-4f15-dc86-049be1c6382a"
   },
   "outputs": [],
   "source": [
    "len(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TUcuGN-SzSyq",
    "outputId": "f6614617-7859-48d3-dff7-623f7622625c"
   },
   "outputs": [],
   "source": [
    "gradient[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OY0Zy9psDtk7"
   },
   "outputs": [],
   "source": [
    "for i in gradient[0]:\n",
    "    print(i, end = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I3YdLFqZU8Ci",
    "outputId": "52462d9f-b600-46f9-9403-a93bdb09c341"
   },
   "outputs": [],
   "source": [
    "gradient[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kPDcvGye0lF7"
   },
   "outputs": [],
   "source": [
    "gradaL = -(Functions.onehot(train_y[1])-h[len(h)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PcCtxLxzFKVG"
   },
   "outputs": [],
   "source": [
    "gradhL_1 = np.matmul(np.transpose(net[(len(net)-1)]),aL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W9xEUE6XKesg"
   },
   "outputs": [],
   "source": [
    "gradaL_1 = np.multiply(net[len(net)-1][:,:len(net[len(net)-1][0])-1], Functions.derivative_sigmoid(a[len(net)-2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K72jzJOiJSR3"
   },
   "outputs": [],
   "source": [
    "gradW = np.outer(gradaL,h[len(net)-2].T)\n",
    "gradB = gradaL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XDbZT58hUOWW"
   },
   "outputs": [],
   "source": [
    "gradB.resize((len(gradB),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L3-e7uWUUYd_",
    "outputId": "2dccdc5f-1f4f-4de9-ef20-bdc5f9e31910"
   },
   "outputs": [],
   "source": [
    "gradB.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WdZHmwrRR2rx",
    "outputId": "df26a5dc-df87-4147-c06a-0d3df7c1d70d"
   },
   "outputs": [],
   "source": [
    "np.append(gradW,gradB.resize((10,1)),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RDIXmbKHNo25",
    "outputId": "6d44560f-8280-4146-b7bd-dedf9486be76"
   },
   "outputs": [],
   "source": [
    "gradaL.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d7_71o1X06hg",
    "outputId": "5899546a-1304-47a4-e96b-f0f8edbce0ee"
   },
   "outputs": [],
   "source": [
    "a[len(net)-2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k65CaJIefUTY"
   },
   "outputs": [],
   "source": [
    "weights = net[0][:,:len(net[0][0])-1]\n",
    "bias = net[0][:,len(net[0][0])-1]\n",
    "temp = np.matmul(weights,train_x[0])+bias\n",
    "temp = temp/np.linalg.norm(temp)\n",
    "a = []\n",
    "a.append(temp)\n",
    "h = []\n",
    "h.append(Functions.sigmoid(a[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZafDhQ7LmcUx"
   },
   "outputs": [],
   "source": [
    "weights = net[L][:,:len(net[L][0])-1]\n",
    "bias = net[L][:,len(net[L][0])-1]\n",
    "temp = np.matmul(weights,h[0])+bias\n",
    "temp = temp/np.linalg.norm(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "epu4dvEji2mG"
   },
   "outputs": [],
   "source": [
    "L = len(net)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tg0dnhLfheDu",
    "outputId": "93e6fdc7-9cf7-4236-b55e-550c4459f101"
   },
   "outputs": [],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qohv21-jxL88"
   },
   "outputs": [],
   "source": [
    "#The class of FeedForwardNeuralNetwor\n",
    "\n",
    "class FFNet:\n",
    "    #constructor\n",
    "    hidden = []\n",
    "    input = []\n",
    "    output = []\n",
    "    def __init__(self,number_of_hidden_layers, number_of_inputs, number_of_outputs):\n",
    "        self.number_of_inputs = number_of_inputs\n",
    "        self.number_of_hidden_layers = number_of_hidden_layers\n",
    "        self.number_of_outputs = number_of_outputs\n",
    "        #At the same time, the layers input layers mus also be initialized.\n",
    "\n",
    "        input = [0 for i in range(number_of_inputs)]\n",
    "        output = [0 for i in range(number_of_outputs)]\n",
    "        hidden = [[]]\n",
    "\n",
    "        #input and output layers are nothing but simple lists\n",
    "    \n",
    "    #Method for creating layers\n",
    "    def add_hidden_layer(number_of_neurons):\n",
    "        temp_weights = [0 for i in range(number_of_neurons+1)] #The +1 is for bias values\n",
    "        hidden.append(temp_weights)\n",
    "    \n",
    "    def backward_propagate(a,h, pred_y):\n",
    "        delthet[L] = -(exp(y) - pred_y) #with respect to output layer\n",
    "        for k in range(0,L-1,-1):\n",
    "            delthetw = np.matmul(delthet[k], h[k-1].T)\n",
    "            delthetb = delthet[k]\n",
    "            deltheth = np.matmul(weights[k].T, delthet[k])\n",
    "            delthet[k-1] = hadamard(deltheth, preac(a)) \n",
    "\n",
    "    def forward_propagate():\n",
    "        #here, we are calculating the preactivations and activations.\n",
    "        #we then store them in an array and return it.\n",
    "        \n",
    "        for k in range(number_of_levels-1):\n",
    "            a[k] = biases[k] + np.matmul(weights[k], h[k-1])\n",
    "            h[k] = g(a[k])\n",
    "        a[number_of_levels-1] = biases[number_of_levels] + np.matmul(weights[number_of_levels],h[number_of_levels-1])\n",
    "        pred_y = output(a[number_of_levels-1])\n",
    "        return a,h, pred_y\n",
    "\n",
    "\n",
    "    def gradient_descent():\n",
    "        a,h, pred_y = forward_propagate()\n",
    "        delthet = backward_propagate(a,h, pred_y)\n",
    "        thet += delthet\n",
    "\n",
    "    def fit(dataset):\n",
    "        for x,y in dataset:\n",
    "            loss = forward(x,y)\n",
    "            delthet = backward(loss)\n",
    "            thet += learn_rate*delthet\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
