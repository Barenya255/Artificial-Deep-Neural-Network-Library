{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vlcWrCZGiPRM"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.datasets import fashion_mnist\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import math\n",
    "import cv2 as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OLGP4dlAQjyZ",
    "outputId": "5e331d30-d8a9-46fb-f77a-3cf210f38da3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs22m028\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'bayes'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = {\n",
    "    'name': 'valAcc',\n",
    "    'goal': 'maximize'   \n",
    "    }\n",
    "\n",
    "sweep_config['metric'] = metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_dict = {\n",
    "    'optimizer': {\n",
    "        'values': ['nadam', 'momentum', 'nag', 'rmsprop', 'adam', 'sgd']\n",
    "        },\n",
    "    'fc_layer_size': {\n",
    "        'values': [32, 64, 128]\n",
    "        },\n",
    "    'number_of_layers': {\n",
    "        'values' : [3,4,5]\n",
    "        },\n",
    "    'epochs':{\n",
    "        'values' : [5,10]\n",
    "        },\n",
    "    'decay' : {\n",
    "        'values' : [0 ,0.0005, 0.5]\n",
    "        },\n",
    "    'learningRate' : {\n",
    "        'values' : [1e-1, 1e-3, 1e-4]\n",
    "        },\n",
    "    'batchSize' : {\n",
    "        'values' : [16, 32, 64]\n",
    "        },\n",
    "    'initialization' : {\n",
    "        'values' : ['random', 'xavier']\n",
    "        },\n",
    "    'activation' : {\n",
    "        'values' : ['sigmoid', 'tanh', 'relu']\n",
    "        },\n",
    "    }\n",
    "\n",
    "sweep_config['parameters'] = parameters_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'method': 'bayes',\n",
      " 'metric': {'goal': 'maximize', 'name': 'valAcc'},\n",
      " 'parameters': {'activation': {'values': ['sigmoid', 'tanh', 'relu']},\n",
      "                'batchSize': {'values': [16, 32, 64]},\n",
      "                'decay': {'values': [0, 0.0005, 0.5]},\n",
      "                'epochs': {'values': [5, 10]},\n",
      "                'fc_layer_size': {'values': [32, 64, 128]},\n",
      "                'initialization': {'values': ['random', 'xavier']},\n",
      "                'learningRate': {'values': [0.1, 0.001, 0.0001]},\n",
      "                'number_of_layers': {'values': [3, 4, 5]},\n",
      "                'optimizer': {'values': ['nadam',\n",
      "                                         'momentum',\n",
      "                                         'nag',\n",
      "                                         'rmsprop',\n",
      "                                         'adam',\n",
      "                                         'sgd']}}}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "pprint.pprint(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: g7h4nvmy\n",
      "Sweep URL: https://wandb.ai/cs22m028/CS6910Assignment1/sweeps/g7h4nvmy\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"CS6910Assignment1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config = None):\n",
    "    with wandb.init(config = config):\n",
    "        #pprint.pprint(sweep_config) \n",
    "        config = wandb.config\n",
    "        wandb.run.name = 'op_{}_act{}_lr{}_layer{}_bth{}_num{}'.format(config.optimizer,config.activation,config.learningRate,config.number_of_layers,config.batchSize, config.fc_layer_size)\n",
    "        neuralNet = FFNet(0, len(trainx[0]), 10)\n",
    "        for layer in range(config.number_of_layers):\n",
    "            neuralNet.addHiddenLayer(config.fc_layer_size, config.initialization)\n",
    "        neuralNet.addOutputLayer(10, config.initialization)\n",
    "        neuralNet.solidify()\n",
    "        weights, biases = neuralNet.fit(config.optimizer,config.batchSize, config.learningRate, config.activation, trainx, train_y, config.decay, config.epochs)\n",
    "        #print(Algorithms.evaluateNetwork(weights,biases,config.activation, Functions.softmax, testx, test_y))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = {\n",
    "    'optimizer': {\n",
    "        'values': ['nadam']\n",
    "        },\n",
    "    'fc_layer_size': {\n",
    "        'values': [128]\n",
    "        },\n",
    "    'number_of_layers': {\n",
    "        'values' : [5]\n",
    "        },\n",
    "    'epochs':{\n",
    "        'values' : [10]\n",
    "        },\n",
    "    'decay' : {\n",
    "        'values' : [0.0005]\n",
    "        },\n",
    "    'learningRate' : {\n",
    "        'values' : [1e-4]\n",
    "        },\n",
    "    'batchSize' : {\n",
    "        'values' : [32]\n",
    "        },\n",
    "    'initialization' : {\n",
    "        'values' : ['xavier']\n",
    "        },\n",
    "    'activation' : {\n",
    "        'values' : ['tanh']\n",
    "        },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    data = PreProc()\n",
    "    train_x, test_x = data.flattenAndCentralize()\n",
    "    trainx = train_x/255.0\n",
    "    testx = test_x/255.0\n",
    "    train_y, test_y = data.getLabels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(config):\n",
    "    \n",
    "    wandb.init(entity = \"cs22m028\", project = \"CS6910Assignment1\", name = \"confmat\")\n",
    "    neuralNet = FFNet(0, len(trainx[0]), 10)\n",
    "    print(config)\n",
    "    for layer in range(config['number_of_layers']['values'][0]):\n",
    "        neuralNet.addHiddenLayer(config['fc_layer_size']['values'][0], config['initialization']['values'][0])\n",
    "    neuralNet.addOutputLayer(10, config['initialization']['values'][0])\n",
    "    neuralNet.solidify()\n",
    "    weights, biases = neuralNet.fit(config['optimizer']['values'][0],config['batchSize']['values'][0], config['learningRate']['values'][0], config['activation']['values'][0], trainx, train_y, config['decay']['values'][0], config['epochs']['values'][0])\n",
    "    predictions = []\n",
    "    true = []\n",
    "    activate = Functions.tanh\n",
    "    for i in tqdm(range(len(test_x))):\n",
    "        a,h = Algorithms.ForwardProp(weights, biases, activate, Functions.softmax, test_x[i])\n",
    "        h = np.array(h, dtype = object)\n",
    "        predY =   np.argmax(h[len(h)-1])\n",
    "        predictions.append(predY)\n",
    "    predictions = np.array(predictions)\n",
    "    \n",
    "    wandb.log({\"conf_mat\" : wandb.plot.confusion_matrix(probs=None,\n",
    "                        y_true=test_y, preds=predictions,\n",
    "                        class_names= ['top', 'lower', 'Pullover', 'Dress', 'blazzer','Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'])})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:do2q6doe) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf6f2f0fbe8d4105872f6c8892cdd289",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.007 MB of 0.007 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, maxâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">confmat</strong> at: <a href='https://wandb.ai/cs22m028/CS6910Assigment1/runs/do2q6doe' target=\"_blank\">https://wandb.ai/cs22m028/CS6910Assigment1/runs/do2q6doe</a><br/>Synced 6 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230317_205653-do2q6doe/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:do2q6doe). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b15f2fb30204f0aa3d0c2145d639167",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016752127783062558, max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/barenya/Documents/GitHub/CS6910Assignment1/wandb/run-20230317_211202-51ppkpbe</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs22m028/CS6910Assignment1/runs/51ppkpbe' target=\"_blank\">confmat</a></strong> to <a href='https://wandb.ai/cs22m028/CS6910Assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs22m028/CS6910Assignment1' target=\"_blank\">https://wandb.ai/cs22m028/CS6910Assignment1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs22m028/CS6910Assignment1/runs/51ppkpbe' target=\"_blank\">https://wandb.ai/cs22m028/CS6910Assignment1/runs/51ppkpbe</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'optimizer': {'values': ['nadam']}, 'fc_layer_size': {'values': [128]}, 'number_of_layers': {'values': [5]}, 'epochs': {'values': [10]}, 'decay': {'values': [0.0005]}, 'learningRate': {'values': [0.0001]}, 'batchSize': {'values': [32]}, 'initialization': {'values': ['xavier']}, 'activation': {'values': ['tanh']}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hm/9gm9jdm90q5fz1jnmjxxsxh40000gn/T/ipykernel_83016/991237929.py:212: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for epoch in tqdm(range(epochs)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1da1617f803540ce8b69650069948d26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hm/9gm9jdm90q5fz1jnmjxxsxh40000gn/T/ipykernel_83016/1384539133.py:14: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for i in tqdm(range(len(test_x))):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f82b4081bc6f4a2a8c02268e8e6e3eb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best['optimizer']['values'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wandb.agent(sweep_id, train, count =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "TfO2B6v6BVWj"
   },
   "outputs": [],
   "source": [
    "class PreProc:\n",
    "    def __init__(self):\n",
    "        (self.trainx,self.trainy),(self.testx, self.testy) = fashion_mnist.load_data()\n",
    "        \n",
    "    def visualize(self,n):\n",
    "        for i in range(n):\n",
    "            plt.subplot(330+1+i) # ask someone why??\n",
    "            plt.imshow(self.trainx[i], cmap = plt.get_cmap('gray'))\n",
    "        plt.show()\n",
    "            \n",
    "        \n",
    "    def flattenAndCentralize(self):\n",
    "        trainx_flattened = np.copy(self.trainx).astype('float64')\n",
    "        testx_flattened = np.copy(self.testx).astype('float64')\n",
    "        trainx_flattened -= np.mean(trainx_flattened, axis = 0)\n",
    "        testx_flattened -= np.mean(testx_flattened, axis = 0)\n",
    "        for image in trainx_flattened:\n",
    "            image = cv.GaussianBlur(image,(3,3),cv.BORDER_DEFAULT)\n",
    "        trainx_flattened.shape = (len(trainx_flattened),784)\n",
    "        testx_flattened.shape = (10000,784)\n",
    "        return trainx_flattened,testx_flattened\n",
    "    \n",
    "\n",
    "    \n",
    "    def getLabels(self):\n",
    "        return self.trainy, self.testy\n",
    "    \n",
    "    def getInputSize(self):\n",
    "        return len(self.trainx[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "7BwoVy0bPFA0"
   },
   "outputs": [],
   "source": [
    "class Functions:\n",
    "    @staticmethod\n",
    "    def sigmoid(input):\n",
    "        input = np.clip(input, -100,100)\n",
    "        return  1.0/(1.0+np.exp(-input))\n",
    "    \n",
    "    @staticmethod\n",
    "    def reLU(input):\n",
    "        return np.maximum(0.01*input,input)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tanh(input):\n",
    "        return np.tanh(input)\n",
    "    \n",
    "    @staticmethod\n",
    "    def identity(input):\n",
    "        return input\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax(input):\n",
    "        input = np.clip(input, -100,100)\n",
    "        return np.exp(input)/(np.sum(np.exp(input)))\n",
    "    \n",
    "    @staticmethod\n",
    "    def onehot(input):\n",
    "        result = np.zeros(10)\n",
    "        result[input] = 1\n",
    "        return result\n",
    "    \n",
    "    @staticmethod\n",
    "    def crossEntropyLoss(y,yHat):\n",
    "        loss = (-1/10.0) * np.sum(np.multiply(y, np.log(yHat+1e-10)) + np.multiply((1 - y), np.log(1 - (yHat+1e-10))))\n",
    "        return loss\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative_sigmoid(input):\n",
    "        return Functions.sigmoid(input)*(1-Functions.sigmoid(input))\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative_tanh(input):\n",
    "        return (1 - (np.tanh(input)**2))\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative_reLU(input):\n",
    "        return np.where(input > 0, 1, 0.01)\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative_identity(input):\n",
    "        return 1\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot(input):\n",
    "        plt.plot(input)\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Loss over iterations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Bu5XtsgmjyaH"
   },
   "outputs": [],
   "source": [
    "class Algorithms:\n",
    "    @staticmethod\n",
    "    def uniLoss(weights, biases, activate, output, t1, t2):\n",
    "        Loss = 0.0\n",
    "        for index in range(len(t1)):\n",
    "            a,h = Algorithms.ForwardProp(weights, biases, activate, output, t1[index])\n",
    "            pred = h[-1]\n",
    "            true = t2[index]\n",
    "            Loss += Functions.crossEntropyLoss(true, pred)\n",
    "        return (Loss/len(t1))\n",
    "    \n",
    "    @staticmethod\n",
    "    def ForwardProp(weights, bias, activate, output, inputLayer):\n",
    "        L = len(weights)-1\n",
    "        a = []\n",
    "        h = []\n",
    "        a.append(np.matmul(weights[0],inputLayer)+bias[0])\n",
    "        h.append(activate(a[0]))\n",
    "        for k in range(1,L):\n",
    "            a.append(np.matmul(weights[k],h[k-1].T)+bias[k])\n",
    "            h.append(activate(a[k]))\n",
    "        a.append(np.matmul(weights[L],h[L-1].T)+bias[L])\n",
    "        h.append(output(a[L]))\n",
    "        return a,h\n",
    "    @staticmethod\n",
    "    def BackProp(weights, biases, a, h, derivative, dataPoint, dataLabel):\n",
    "        L = len(weights)-1\n",
    "        gradaL = -(Functions.onehot(dataLabel)-h[len(h)-1])\n",
    "        dw = np.zeros_like(weights)\n",
    "        db = np.zeros_like(biases)\n",
    "        for k in range(L,0,-1):\n",
    "            gradW = np.outer(gradaL, h[k-1].T)\n",
    "            gradB = gradaL\n",
    "            dw[k] = gradW\n",
    "            db[k] = gradB\n",
    "\n",
    "            gradhL_1 = np.matmul(np.transpose(weights[k]),gradaL)\n",
    "            gradaL_1 = np.multiply(gradhL_1, derivative(a[k-1]))\n",
    "            gradaL = gradaL_1\n",
    "        dw[0] = np.outer(gradaL,dataPoint.T)\n",
    "        db[0] = gradaL\n",
    "        return dw, db\n",
    "\n",
    "    @staticmethod\n",
    "    def miniBatchMGD(weights, biases, batchSize, learningRate, activate, output, derivative, dataPoints, dataLabels, valTest_x, valTest_y, decay, epochs):\n",
    "        validation = []\n",
    "        validationLoss = []\n",
    "        trainAccuracy = []\n",
    "        loss = []\n",
    "        lossTrack = []\n",
    "        LOSS = 0.0\n",
    "        beta = 0.9\n",
    "        prevWeights = np.zeros_like(weights)\n",
    "        prevBiases = np.zeros_like(biases)\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            indices = np.arange(len(dataPoints))\n",
    "            np.random.shuffle(indices)\n",
    "            batchX = dataPoints[indices]\n",
    "            batchY = dataLabels[indices]\n",
    "            Loss = 0.0\n",
    "            for i in range(math.ceil(len(dataPoints)/batchSize)):\n",
    "                if (i+1)*batchSize < len(dataPoints):\n",
    "                    trainer = batchX[i*batchSize:i*batchSize+batchSize]\n",
    "                    labeler = batchY[i*batchSize:i*batchSize+batchSize]\n",
    "                else:\n",
    "                    trainer = batchX[i*batchSize:len(dataPoints)]\n",
    "                    labeler = batchY[i*batchSize:len(dataPoints)]\n",
    "                batchLoss = 0.0\n",
    "                dw = np.zeros_like(weights)\n",
    "                db = np.zeros_like(biases)\n",
    "                for data in range(len(trainer)):\n",
    "                    a,h = Algorithms.ForwardProp(weights, biases , activate, output, trainer[data])\n",
    "                    currWeights, currBiases = Algorithms.BackProp(weights, biases, a, h, derivative, trainer[data], labeler[data])\n",
    "                    batchLoss += Functions.crossEntropyLoss(Functions.onehot(labeler[data]), h[-1])\n",
    "                    dw += currWeights\n",
    "                    db += currBiases\n",
    "                batchLoss /= batchSize\n",
    "                dw /= batchSize\n",
    "                db /= batchSize\n",
    "                momentumWeights = prevWeights*beta + dw*1.0\n",
    "                momentumBiases = prevBiases*beta + db*1.0\n",
    "                weights -= learningRate*(momentumWeights + decay*weights)\n",
    "                biases -= learningRate*(momentumBiases + decay*biases)\n",
    "                prevWeights = momentumWeights\n",
    "                prevBiases = momentumBiases\n",
    "                lossTrack.append(batchLoss)\n",
    "                \n",
    "            validation.append(Algorithms.evaluateNetwork(weights, biases, activate, output, valTest_x, valTest_y))\n",
    "            validationLoss.append((Algorithms.uniLoss(weights, biases, activate, output, valTest_x, valTest_y)))\n",
    "            trainAccuracy.append(Algorithms.evaluateNetwork(weights, biases, activate, output, testx, test_y))\n",
    "            #wandb.log({\"valAcc\":validation[-1], \"epoch\":epoch, \"loss\":lossTrack[-1], \"valLoss\":validationLoss[-1], \"accuracy\":trainAccuracy[-1]})\n",
    "        return weights, biases\n",
    "\n",
    "    @staticmethod\n",
    "    def ADAM(weights, biases, batchSize, learningRate, activate, output, derivative, dataPoints, dataLabels, valTest_x, valTest_y, decay, epochs):\n",
    "        validation = []\n",
    "        validationLoss = []\n",
    "        trainAccuracy = []\n",
    "        loss = []\n",
    "        lossTrack = []\n",
    "        prev_val = 0.0\n",
    "        beta1 = 0.9\n",
    "        beta2 = 0.99\n",
    "        epsilon = 0.000001\n",
    "        patience = 3\n",
    "        m_w = np.zeros_like(weights)\n",
    "        v_w = np.zeros_like(weights)\n",
    "        m_b = np.zeros_like(biases)\n",
    "        v_b = np.zeros_like(biases)\n",
    "        learnerRateW = np.full_like(weights, learningRate)\n",
    "        learnerRateB = np.full_like(biases, learningRate)\n",
    "        i = 0\n",
    "        \n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            indices = np.arange(len(dataPoints))\n",
    "            np.random.shuffle(indices)\n",
    "            batchX = dataPoints[indices]\n",
    "            batchY = dataLabels[indices]\n",
    "            Loss = 0.0\n",
    "            for i in range(math.ceil(len(dataPoints)/batchSize)):\n",
    "                if (i+1)*batchSize < len(dataPoints):\n",
    "                    trainer = batchX[i*batchSize:i*batchSize+batchSize]\n",
    "                    labeler = batchY[i*batchSize:i*batchSize+batchSize]\n",
    "                else:\n",
    "                    trainer = batchX[i*batchSize:len(dataPoints)]\n",
    "                    labeler = batchY[i*batchSize:len(dataPoints)]\n",
    "                batchLoss = 0.0\n",
    "                dw = np.zeros_like(weights)\n",
    "                db = np.zeros_like(biases)\n",
    "                for data in range(len(trainer)):\n",
    "                    a,h = Algorithms.ForwardProp(weights, biases,activate, output, trainer[data])\n",
    "                    currWeights, currBiases = Algorithms.BackProp(weights, biases , a, h, derivative, trainer[data], labeler[data])\n",
    "                    batchLoss += Functions.crossEntropyLoss(Functions.onehot(labeler[data]), h[-1])\n",
    "                    \n",
    "                    dw += currWeights\n",
    "                    db += currBiases\n",
    "                batchLoss /= batchSize\n",
    "                dw /= batchSize\n",
    "                db /= batchSize\n",
    "                m_w = beta1*m_w + (1-beta1)*dw\n",
    "                m_b = beta1*m_b + (1-beta1)*db\n",
    "                v_w = v_w*beta2 + (1-beta2)*dw**2\n",
    "                v_b = v_b*beta2 + (1-beta2)*db**2\n",
    "                \n",
    "                m_w_hat = m_w/(1 - np.power(beta1, i+1))\n",
    "                m_b_hat = m_b/(1 - np.power(beta1, i+1))\n",
    "                v_w_hat = v_w/(1 - np.power(beta2, i+1))\n",
    "                v_b_hat = v_b/(1 - np.power(beta2, i+1))\n",
    "                \n",
    "                i+=1\n",
    "                \n",
    "                tempW = np.zeros_like(m_w)\n",
    "                tempB = np.zeros_like(m_b)\n",
    "                for i in range(len(dw)):\n",
    "                    tempW[i] = np.sqrt(v_w_hat[i])\n",
    "                    tempB[i] = np.sqrt(v_b_hat[i])\n",
    "                weights = weights - ((learnerRateW*(dw + decay*weights))/(tempW + epsilon))\n",
    "                biases = biases - (learnerRateB*(db + decay*biases))/(tempB+epsilon)\n",
    "\n",
    "                lossTrack.append(batchLoss)\n",
    "                \n",
    "            Loss /= len(dataPoints)\n",
    "            loss.append(Loss)\n",
    "            validation.append(Algorithms.evaluateNetwork(weights, biases, activate, output, valTest_x, valTest_y))\n",
    "            validationLoss.append((Algorithms.uniLoss(weights, biases, activate, output, valTest_x, valTest_y)))\n",
    "            trainAccuracy.append(Algorithms.evaluateNetwork(weights, biases, activate, output, trainx, train_y))\n",
    "            #wandb.log({\"valAcc\":validation[-1], \"epoch\":epoch, \"loss\":loss[-1], \"valLoss\": validationLoss[-1], \"train Accuracy\":trainAccuracy[-1]})\n",
    "            print(validation[-1])   \n",
    "            if validation[-1] <= prev_val+1e-2:\n",
    "                patience -= 1\n",
    "            else:\n",
    "                patience = 3\n",
    "                prev_weights = weights\n",
    "                prev_biases = biases\n",
    "                prev_val = validation[-1]\n",
    "            \n",
    "            if patience == 0:\n",
    "                weights = prev_weights\n",
    "                biases = prev_biases\n",
    "            Loss /= len(dataPoints)\n",
    "            loss.append(Loss)\n",
    "            \n",
    "        if prev_val > validation[-1]:\n",
    "            return prev_weights, prev_biases\n",
    "            \n",
    "        return weights,biases\n",
    "\n",
    "    @staticmethod\n",
    "    def NADAM(weights, biases, batchSize, learningRate, activate, output, derivative, dataPoints, dataLabels, valTest_x, valTest_y, decay, epochs):\n",
    "        patience = 3\n",
    "        validation = []\n",
    "        validationLoss = []\n",
    "        trainAccuracy = []\n",
    "        loss = []\n",
    "        lossTrack = []\n",
    "        prev_val = 0.0\n",
    "        prevWeights = np.zeros_like(weights)\n",
    "        prevBiases = np.zeros_like(biases)\n",
    "        beta1 = 0.9\n",
    "        beta2 = 0.99\n",
    "        epsilon = 0.000001\n",
    "        prev_weights = np.zeros_like(weights)\n",
    "        prev_biases = np.zeros_like(biases)\n",
    "        m_w = np.zeros_like(weights)\n",
    "        v_w = np.zeros_like(weights)\n",
    "        m_b = np.zeros_like(biases)\n",
    "        v_b = np.zeros_like(biases)\n",
    "        learnerRateW = np.full_like(weights, learningRate)\n",
    "        learnerRateB = np.full_like(biases, learningRate)\n",
    "        i = 0\n",
    "        \n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            indices = np.arange(len(dataPoints))\n",
    "            np.random.shuffle(indices)\n",
    "            batchX = dataPoints[indices]\n",
    "            batchY = dataLabels[indices]\n",
    "            Loss = 0.0\n",
    "            for i in range(math.ceil(len(dataPoints)/batchSize)):\n",
    "                if (i+1)*batchSize < len(dataPoints):\n",
    "                    trainer = batchX[i*batchSize:i*batchSize+batchSize]\n",
    "                    labeler = batchY[i*batchSize:i*batchSize+batchSize]\n",
    "                else:\n",
    "                    trainer = batchX[i*batchSize:len(dataPoints)]\n",
    "                    labeler = batchY[i*batchSize:len(dataPoints)]\n",
    "                batchLoss = 0.0\n",
    "                dw = np.zeros_like(weights)\n",
    "                db = np.zeros_like(biases)\n",
    "                for data in range(len(trainer)):\n",
    "                    a,h = Algorithms.ForwardProp(weights - v_w*(beta1), biases - v_b*(beta1), activate, output, trainer[data])\n",
    "                    currWeights, currBiases = Algorithms.BackProp(weights - v_w*(beta1), biases - v_b*(beta1), a, h, derivative, trainer[data], labeler[data])\n",
    "                    batchLoss += Functions.crossEntropyLoss(Functions.onehot(labeler[data]), h[-1])\n",
    "                    \n",
    "                    dw += currWeights\n",
    "                    db += currBiases\n",
    "                batchLoss /= batchSize\n",
    "                dw /= batchSize\n",
    "                db /= batchSize\n",
    "                m_w = beta1*m_w + (1-beta1)*dw\n",
    "                m_b = beta1*m_b + (1-beta1)*db\n",
    "                v_w = v_w*beta2 + (1-beta2)*dw**2\n",
    "                v_b = v_b*beta2 + (1-beta2)*db**2\n",
    "                \n",
    "                m_w_hat = m_w/(1 - np.power(beta1, i+1))\n",
    "                m_b_hat = m_b/(1 - np.power(beta1, i+1))\n",
    "                v_w_hat = v_w/(1 - np.power(beta2, i+1))\n",
    "                v_b_hat = v_b/(1 - np.power(beta2, i+1))\n",
    "                \n",
    "                i+=1\n",
    "                \n",
    "                tempW = np.zeros_like(m_w)\n",
    "                tempB = np.zeros_like(m_b)\n",
    "                for j in range(len(dw)):\n",
    "                    tempW[j] = np.sqrt(v_w_hat[j])\n",
    "                    tempB[j] = np.sqrt(v_b_hat[j])\n",
    "                weights = weights - ((learnerRateW*(dw + decay*weights))/(tempW + epsilon))\n",
    "                biases = biases - (learnerRateB*(db + decay*biases))/(tempB+epsilon)\n",
    "                lossTrack.append(batchLoss)\n",
    "            #wandb.log({'valAcc':(Algorithms.evaluateNetwork(weights, biases, valTest_x, valTest_y)), 'epoch':epoch})\n",
    "            validation.append(Algorithms.evaluateNetwork(weights, biases, activate, output, valTest_x, valTest_y))\n",
    "            if validation[-1] <= prev_val:\n",
    "                patience -= 1\n",
    "            else:\n",
    "                patience = 3\n",
    "                prev_weights = weights\n",
    "                prev_biases = biases\n",
    "                prev_val = validation[-1]\n",
    "            \n",
    "            if patience <= 0:\n",
    "                weights = prev_weights\n",
    "                biases = prev_biases\n",
    "                #learningRate /= 2\n",
    "            \n",
    "            Loss /= len(dataPoints)\n",
    "            loss.append(Loss)\n",
    "            validation.append(Algorithms.evaluateNetwork(weights, biases, activate, output, valTest_x, valTest_y))\n",
    "            validationLoss.append((Algorithms.uniLoss(weights, biases, activate, output, valTest_x, valTest_y)))\n",
    "            trainAccuracy.append(Algorithms.evaluateNetwork(weights, biases, activate, output, trainx, train_y))\n",
    "            #wandb.log({\"valAcc\":validation[-1], \"epoch\":epoch, \"loss\":loss[-1], \"valLoss\": validationLoss[-1], \"train Accuracy\":trainAccuracy[-1]})\n",
    "            \n",
    "        return prev_weights, prev_biases\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def miniBatchNAG(weights, biases, batchSize, learningRate, activate, output, derivative, dataPoints, dataLabels, valTest_x, valTest_y, decay, epochs):\n",
    "        validation = []\n",
    "        validationLoss = []\n",
    "        trainAccuracy = []\n",
    "        loss = []\n",
    "        lossTrack = []\n",
    "        beta = 0.9\n",
    "        prevWeights = np.zeros_like(weights)\n",
    "        prevBiases = np.zeros_like(biases)\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            indices = np.arange(len(dataPoints))\n",
    "            np.random.shuffle(indices)\n",
    "            batchX = dataPoints[indices]\n",
    "            batchY = dataLabels[indices]\n",
    "            Loss = 0.0\n",
    "            for i in range(math.ceil(len(dataPoints)/batchSize)):\n",
    "                if (i+1)*batchSize < len(dataPoints):\n",
    "                    trainer = batchX[i*batchSize:i*batchSize+batchSize]\n",
    "                    labeler = batchY[i*batchSize:i*batchSize+batchSize]\n",
    "                else:\n",
    "                    trainer = batchX[i*batchSize:len(dataPoints)]\n",
    "                    labeler = batchY[i*batchSize:len(dataPoints)]\n",
    "                batchLoss = 0.0\n",
    "                tempWeights = np.zeros_like(weights)\n",
    "                tempBiases = np.zeros_like(biases)\n",
    "                for data in range(len(trainer)):\n",
    "                    a,h = Algorithms.ForwardProp(weights-prevWeights, biases-prevBiases, activate, output, trainer[data])\n",
    "                    currWeights, currBiases = Algorithms.BackProp(weights-prevWeights, biases-prevBiases, a, h, derivative, trainer[data], labeler[data])\n",
    "                    batchLoss += Functions.crossEntropyLoss(Functions.onehot(labeler[data]), h[-1])\n",
    "                    tempWeights += currWeights\n",
    "                    tempBiases += currBiases\n",
    "                batchLoss /= batchSize\n",
    "                tempWeights /= batchSize\n",
    "                tempBiases /= batchSize\n",
    "                momentumWeights = beta*prevWeights + tempWeights*1.0\n",
    "                momentumBiases = beta*prevBiases + tempBiases*1.0\n",
    "                weights = weights - learningRate*(momentumWeights + decay*weights) \n",
    "                biases = biases - learningRate*(momentumBiases + decay*biases)\n",
    "                prevWeights = momentumWeights\n",
    "                prevBiases = momentumBiases\n",
    "                lossTrack.append(batchLoss)\n",
    "            Loss /= len(dataPoints)\n",
    "            loss.append(Loss)\n",
    "            validation.append(Algorithms.evaluateNetwork(weights, biases, activate, output, valTest_x, valTest_y))\n",
    "            validationLoss.append((Algorithms.uniLoss(weights, biases, activate, output, valTest_x, valTest_y)))\n",
    "            trainAccuracy.append(Algorithms.evaluateNetwork(weights, biases, activate, output, trainx, train_y))\n",
    "            #wandb.log({\"valAcc\":validation[-1], \"epoch\":epoch, \"loss\":loss[-1], \"valLoss\": validationLoss[-1], \"train Accuracy\":trainAccuracy[-1]})\n",
    "            \n",
    "        return weights, biases\n",
    "    \n",
    "    @staticmethod\n",
    "    def RMSProp(weights, biases, batchSize, learningRate, activate, output, derivative, dataPoints, dataLabels, valTest_x, valTest_y, decay, epochs):\n",
    "        validation = []\n",
    "        validationLoss = []\n",
    "        trainAccuracy = []\n",
    "        loss = []\n",
    "        lossTrack = []\n",
    "        beta = 0.5\n",
    "        epsilon = 0.000001\n",
    "        momentumWeights = np.zeros_like(weights)\n",
    "        momentumBiases = np.zeros_like(biases)\n",
    "        learnerRateW = np.full_like(weights, learningRate)\n",
    "        learnerRateB = np.full_like(biases, learningRate)\n",
    "        \n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            indices = np.arange(len(dataPoints))\n",
    "            np.random.shuffle(indices)\n",
    "            batchX = dataPoints[indices]\n",
    "            batchY = dataLabels[indices]\n",
    "            Loss = 0.0\n",
    "            for i in range(math.ceil(len(dataPoints)/batchSize)):\n",
    "                if (i+1)*batchSize < len(dataPoints):\n",
    "                    trainer = batchX[i*batchSize:i*batchSize+batchSize]\n",
    "                    labeler = batchY[i*batchSize:i*batchSize+batchSize]\n",
    "                else:\n",
    "                    trainer = batchX[i*batchSize:len(dataPoints)]\n",
    "                    labeler = batchY[i*batchSize:len(dataPoints)]\n",
    "                batchLoss = 0.0\n",
    "                dw = np.zeros_like(weights)\n",
    "                db = np.zeros_like(biases)\n",
    "                for data in range(len(trainer)):\n",
    "                    a,h = Algorithms.ForwardProp(weights, biases, activate, output, trainer[data])\n",
    "                    currWeights, currBiases = Algorithms.BackProp(weights, biases , a, h, derivative, trainer[data], labeler[data])\n",
    "                    batchLoss += Functions.crossEntropyLoss(Functions.onehot(labeler[data]), h[-1])\n",
    "                    dw += currWeights\n",
    "                    db += currBiases\n",
    "                batchLoss /= batchSize\n",
    "                dw /= batchSize\n",
    "                db /= batchSize\n",
    "                momentumWeights = momentumWeights*beta + (1-beta)*dw**2\n",
    "                momentumBiases = momentumBiases*beta + (1-beta)*db**2\n",
    "                tempW = np.zeros_like(momentumWeights)\n",
    "                tempB = np.zeros_like(momentumBiases)\n",
    "                for i in range(len(dw)):\n",
    "                    tempW[i] = np.sqrt(momentumWeights[i])\n",
    "                    tempB[i] = np.sqrt(momentumBiases[i])\n",
    "                weights = weights - ((learnerRateW)*(dw + decay*weights)/(tempW + epsilon))\n",
    "                biases = biases - (learnerRateB*(db + decay*biases))/(tempB+epsilon)\n",
    "                lossTrack.append(batchLoss)\n",
    "            Loss /= len(dataPoints)\n",
    "            loss.append(Loss)\n",
    "            validation.append(Algorithms.evaluateNetwork(weights, biases, activate, output, valTest_x, valTest_y))\n",
    "            validationLoss.append((Algorithms.uniLoss(weights, biases, activate, output, valTest_x, valTest_y)))\n",
    "            trainAccuracy.append(Algorithms.evaluateNetwork(weights, biases, activate, output, trainx, train_y))\n",
    "            wandb.log({\"valAcc\":validation[-1], \"epoch\":epoch, \"loss\":loss[-1], \"valLoss\": validationLoss[-1], \"train Accuracy\":trainAccuracy[-1]})\n",
    "            \n",
    "        return weights, biases\n",
    "\n",
    "    @staticmethod\n",
    "    def miniBatchGD(weights, biases, batchSize, learningRate, activate, output, derivative, dataPoints, dataLabels, valTest_x, valTest_y, decay, epochs):\n",
    "        validation = []\n",
    "        validationLoss = []\n",
    "        trainAccuracy = []\n",
    "        loss = []\n",
    "        lossTrack = []\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            indices = np.arange(len(dataPoints))\n",
    "            np.random.shuffle(indices)\n",
    "            batchX = dataPoints[indices]\n",
    "            batchY = dataLabels[indices]\n",
    "            Loss = 0.0\n",
    "            for i in range(math.ceil(len(dataPoints)/batchSize)):\n",
    "                if (i+1)*batchSize < len(dataPoints):\n",
    "                    trainer = batchX[i*batchSize:i*batchSize+batchSize]\n",
    "                    labeler = batchY[i*batchSize:i*batchSize+batchSize]\n",
    "                else:\n",
    "                    trainer = batchX[i*batchSize:len(dataPoints)]\n",
    "                    labeler = batchY[i*batchSize:len(dataPoints)]\n",
    "                batchLoss = 0.0\n",
    "                dw = np.zeros_like(weights)\n",
    "                db = np.zeros_like(biases)\n",
    "                for data in range(len(trainer)):\n",
    "                    a,h = Algorithms.ForwardProp(weights, biases , activate, output, trainer[data])\n",
    "                    tempw,tempb = Algorithms.BackProp(weights, biases, a, h, derivative, trainer[data], labeler[data])\n",
    "                    dw+=tempw\n",
    "                    db+=tempb\n",
    "                    batchLoss += Functions.crossEntropyLoss(Functions.onehot(labeler[data]), h[-1])\n",
    "                batchLoss /= batchSize\n",
    "                dw /= batchSize\n",
    "                db /= batchSize\n",
    "                weights -= learningRate*(dw + decay*weights)\n",
    "                biases -= learningRate*(db + decay*biases)\n",
    "                lossTrack.append(batchLoss)\n",
    "            Loss /= len(dataPoints)\n",
    "            loss.append(Loss)\n",
    "            validation.append(Algorithms.evaluateNetwork(weights, biases, activate, output, valTest_x, valTest_y))\n",
    "            validationLoss.append((Algorithms.uniLoss(weights, biases, activate, output, valTest_x, valTest_y)))\n",
    "            trainAccuracy.append(Algorithms.evaluateNetwork(weights, biases, activate, output, trainx, train_y))\n",
    "            #wandb.log({\"valAcc\":validation[-1], \"epoch\":epoch, \"loss\":loss[-1], \"valLoss\": validationLoss[-1], \"train Accuracy\":trainAccuracy[-1]})\n",
    "            \n",
    "        return weights, biases\n",
    "    \n",
    "    @staticmethod\n",
    "    def logConfusion(weights, biases,activate, output, test_x, test_y):\n",
    "        predicitons = []\n",
    "        for i in range(len(test_x)):\n",
    "            a,h = Algorithms.ForwardProp(weights, biases, activate, output, test_x[i])\n",
    "            h = np.array(h, dtype = object)\n",
    "            predY =   np.argmax(h[len(h)-1])\n",
    "            predictions.append(predY)\n",
    "        predictions = np.array(predictions)\n",
    "        wandb.log({\"conf_mat\" : wandb.plot.confusion_matrix(probs=None,\n",
    "                        y_true=test_y, preds=predictions,\n",
    "                        class_names= ['top', 'lower', 'Pullover', 'Dress', 'blazzer','Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'])})\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def evaluateNetwork(weights, biases,activate, output, test_x, test_y):\n",
    "        num_acc = 0\n",
    "        for i in range(len(test_x)):\n",
    "            a,h = Algorithms.ForwardProp(weights, biases, activate, output, test_x[i])\n",
    "            h = np.array(h, dtype = object)\n",
    "            predY =   np.argmax(h[len(h)-1])\n",
    "            if test_y[i] == predY:\n",
    "                num_acc+=1\n",
    "        \n",
    "        return (num_acc/len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "wpFxAmhE9t2C",
    "outputId": "fdd02dd5-6147-4d7a-9d86-9cabebea2f81",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    data = PreProc()\n",
    "    #data.visualize(5)\n",
    "    train_x, test_x = data.flattenAndCentralize()\n",
    "    trainx = train_x/255.0\n",
    "    testx = test_x/255.0\n",
    "    train_y, test_y = data.getLabels()\n",
    "    neuralNet = FFNet(0, len(trainx[0]), 10)\n",
    "\n",
    "    neuralNet.addHiddenLayer(32, \"xavier\")\n",
    "    neuralNet.addHiddenLayer(32, \"xavier\")#drastic decrease in accuracy\n",
    "    neuralNet.addHiddenLayer(32, \"xavier\")#drastic decrease in accuracy\n",
    "    neuralNet.addOutputLayer(10, \"xavier\")\n",
    "    neuralNet.solidify()\n",
    "    #print(trainx.shape)\n",
    "    #wandb.init()\n",
    "    weights,biases = neuralNet.fit(\"adam\",16, 0.001, \"relu\", trainx, train_y, 0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FFNet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m testx \u001b[38;5;241m=\u001b[39m test_x\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255.0\u001b[39m\n\u001b[1;32m      7\u001b[0m train_y, test_y \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mgetLabels()\n\u001b[0;32m----> 8\u001b[0m neuralNet \u001b[38;5;241m=\u001b[39m \u001b[43mFFNet\u001b[49m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(trainx[\u001b[38;5;241m0\u001b[39m]), \u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     10\u001b[0m neuralNet\u001b[38;5;241m.\u001b[39maddHiddenLayer(\u001b[38;5;241m32\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxavier\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m neuralNet\u001b[38;5;241m.\u001b[39maddHiddenLayer(\u001b[38;5;241m32\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxavier\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;66;03m#drastic decrease in accuracy\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'FFNet' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    data = PreProc()\n",
    "    #data.visualize(5)\n",
    "    train_x, test_x = data.flattenAndCentralize()\n",
    "    trainx = train_x/255.0\n",
    "    testx = test_x/255.0\n",
    "    train_y, test_y = data.getLabels()\n",
    "    neuralNet = FFNet(0, len(trainx[0]), 10)\n",
    "\n",
    "    neuralNet.addHiddenLayer(32, \"xavier\")\n",
    "    neuralNet.addHiddenLayer(32, \"xavier\")#drastic decrease in accuracy\n",
    "    neuralNet.addHiddenLayer(32, \"xavier\")#drastic decrease in accuracy\n",
    "    neuralNet.addOutputLayer(10, \"xavier\")\n",
    "    neuralNet.solidify()\n",
    "    weights,biases = neuralNet.fit(\"adam\",16, 0.001, \"relu\", trainx, train_y, 0, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Algorithms.evaluateNetwork(weights, biases, Functions.reLU, Functions.softmax, testx, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qHfSx0jHSqtk",
    "outputId": "5e2dae53-035e-468c-98d7-d29b08d821f1"
   },
   "outputs": [],
   "source": [
    "#The class of FeedForwardNeuralNetwor\n",
    "\n",
    "class FFNet:\n",
    "    #constructor\n",
    "    def __init__(self,number_of_hidden_layers, number_of_inputs, number_of_outputs):\n",
    "        self.number_of_inputs = number_of_inputs\n",
    "        self.number_of_hidden_layers = number_of_hidden_layers\n",
    "        self.number_of_outputs = number_of_outputs\n",
    "        self.input = [0 for i in range(number_of_inputs)]\n",
    "        self.output = [0 for i in range(10)]\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        #self.hidden.append(np.random.random((number_of_inputs+1)))\n",
    "    \n",
    "    #Method for creating layers\n",
    "    def addHiddenLayer(self,number_of_neurons, initialization):\n",
    "        if(len(self.weights) == 0):\n",
    "            temp_weights = np.random.randn(number_of_neurons, self.number_of_inputs)*0.01\n",
    "            temp_biases = np.full((number_of_neurons), 0.01)\n",
    "            if initialization == \"xavier\":\n",
    "                temp_weights = np.random.randn(number_of_neurons, self.number_of_inputs)/np.sqrt((self.number_of_inputs)/2)\n",
    "                #temp_biases = np.random.randn(number_of_neurons)*np.sqrt(1/(number_of_neurons))\n",
    "        else:\n",
    "            prev_neurons = len(self.weights[-1])\n",
    "            temp_weights = np.random.randn(number_of_neurons, prev_neurons)*0.01\n",
    "            temp_biases = np.full((number_of_neurons), 0.01)\n",
    "            if initialization == \"xavier\":\n",
    "                temp_weights = np.random.randn(number_of_neurons, prev_neurons)/np.sqrt((prev_neurons)/2)\n",
    "                #temp_biases = np.random.randn(number_of_neurons)*np.sqrt(1/(number_of_neurons))\n",
    "\n",
    "        self.weights.append(temp_weights)\n",
    "        self.biases.append(temp_biases)\n",
    "    \n",
    "    def addOutputLayer(self, number_of_outputs, initialization):\n",
    "        if(len(self.weights) == 0):\n",
    "            #print(\"number of inputs: \"+str(self.number_of_inputs))\n",
    "            temp_weights = np.random.randn(number_of_outputs, self.number_of_inputs)*0.01\n",
    "            temp_biases = np.full((number_of_outputs), 0.01)\n",
    "            if initialization == \"xavier\":\n",
    "                temp_weights = np.random.randn(number_of_outputs, self.number_of_inputs)/np.sqrt((prev_neurons)/2)\n",
    "        else:\n",
    "            prev_neurons = len(self.weights[-1])\n",
    "            temp_weights = np.random.randn(number_of_outputs, prev_neurons)*0.01\n",
    "            temp_biases = np.full((number_of_outputs), 0.01)\n",
    "            if initialization == \"xavier\":\n",
    "                temp_weights = np.random.randn(number_of_outputs, prev_neurons)/np.sqrt((prev_neurons)/2)\n",
    "                \n",
    "        \n",
    "        self.weights.append(temp_weights)\n",
    "        self.biases.append(temp_biases)\n",
    "\n",
    "    def solidify(self):\n",
    "        self.weights = np.array(self.weights, dtype = object)\n",
    "        self.biases = np.array(self.biases, dtype = object)\n",
    "\n",
    "    def getNetwork(self):\n",
    "        return self.weights,self.biases\n",
    "    \n",
    "    def ForwardProp(self, activate, output, inputLayer):\n",
    "        return Algorithms.ForwardProp(self.network, activate, output, inputLayer)\n",
    "    \n",
    "    def lossCalc(self, lossFunction, Y):\n",
    "        predY = self.historyA[(len(self.historyA)-1)]\n",
    "        return lossFunction(Y,self.predY)\n",
    "\n",
    "    def BackProp(self, a, h, dataPoint, dataLabel):\n",
    "        return Algorithms.BackProp(self.network, a, h, dataPoint, dataLabel)\n",
    "    \n",
    "    def fit(self, optimizer, batchSize, learningRate, activation, trainx, train_y, decay, epochs):\n",
    "        \n",
    "        #break data into training and validation\n",
    "        indices = np.arange(len(trainx))\n",
    "        np.random.shuffle(indices)\n",
    "        trainx = trainx[indices]\n",
    "        train_y = train_y[indices]\n",
    "        \n",
    "        valTest_x = trainx[int(0.9*len(trainx)):]\n",
    "        valTest_y = train_y[int(0.9*len(train_y)):]\n",
    "        \n",
    "        trainx = trainx[:int(0.9*len(trainx))]\n",
    "        train_y = train_y[:int(0.9*len(train_y))]\n",
    "        \n",
    "        if activation == \"relu\":\n",
    "            activate = Functions.reLU\n",
    "            derivative = Functions.derivative_reLU\n",
    "            output = Functions.softmax\n",
    "        elif activation == \"tanh\":\n",
    "            activate = Functions.tanh\n",
    "            derivative = Functions.derivative_tanh\n",
    "            output = Functions.softmax\n",
    "        elif activation == \"identity\":\n",
    "            activate = Functions.identity\n",
    "            derivative = Functions.derivative_identity\n",
    "            output = Functions.softmax\n",
    "        else:\n",
    "            activate = Functions.sigmoid\n",
    "            derivative = Functions.derivative_sigmoid\n",
    "            output = Functions.softmax\n",
    "        \n",
    "        #print(optimizer)\n",
    "        \n",
    "        if optimizer == \"momentum\":\n",
    "            self.weights, self.biases = Algorithms.miniBatchMGD(self.weights,self.biases , batchSize, learningRate, activate, output, derivative,  trainx, train_y, valTest_x, valTest_y, decay, epochs)\n",
    "        elif optimizer == \"nag\":\n",
    "            self.weights, self.biases = Algorithms.miniBatchNAG(self.weights,self.biases , batchSize, learningRate,activate, output, derivative , trainx, train_y, valTest_x, valTest_y, decay, epochs)\n",
    "        elif optimizer == \"rmsprop\":\n",
    "            self.weights, self.biases = Algorithms.RMSProp(self.weights,self.biases , batchSize, learningRate, activate, output, derivative,  trainx, train_y, valTest_x, valTest_y, decay, epochs)\n",
    "        elif optimizer == \"adam\":\n",
    "            self.weights, self.biases = Algorithms.ADAM(self.weights,self.biases , batchSize, learningRate,activate, output, derivative,  trainx, train_y, valTest_x, valTest_y, decay, epochs)\n",
    "        elif optimizer == \"nadam\":\n",
    "            self.weights, self.biases = Algorithms.NADAM(self.weights,self.biases , batchSize, learningRate, activate, output, derivative,  trainx, train_y, valTest_x, valTest_y, decay, epochs)\n",
    "        else:\n",
    "            self.weights, self.biases = Algorithms.miniBatchGD(self.weights,self.biases , batchSize, learningRate, activate, output, derivative , trainx, train_y, valTest_x, valTest_y, decay, epochs)\n",
    "        print(Algorithms.evaluateNetwork(self.weights, self.biases, activate, output, testx, test_y))       \n",
    "        \n",
    "        return self.weights,self.biases\n",
    "    \n",
    "    def logConfusion(weights, biases, testx, test_y):\n",
    "        Algorithms.logConfusion(weights, biases, activate, output, testx, test_y)\n",
    "    def evaluateNetwork(self, testx, test_y):\n",
    "        Algorithms.evaluateNetwork(self.weights, self.biases, testx, test_y)        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
