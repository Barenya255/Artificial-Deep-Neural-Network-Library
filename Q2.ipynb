{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "wpFxAmhE9t2C",
    "outputId": "fdd02dd5-6147-4d7a-9d86-9cabebea2f81",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    data = PreProc()\n",
    "    #data.visualize(5)\n",
    "    train_x, test_x = data.flattenAndCentralize()\n",
    "    trainx = train_x/255.0\n",
    "    testx = test_x/255.0\n",
    "    train_y, test_y = data.getLabels()\n",
    "    neuralNet = FFNet(0, len(trainx[0]), 10)\n",
    "\n",
    "    neuralNet.addHiddenLayer(128, \"xavier\")\n",
    "    neuralNet.addHiddenLayer(128, \"xavier\")\n",
    "    neuralNet.addHiddenLayer(128, \"xavier\")\n",
    "    neuralNet.addHiddenLayer(128, \"xavier\")\n",
    "    neuralNet.addHiddenLayer(128, \"xavier\")\n",
    "    neuralNet.addOutputLayer(10, \"xavier\")\n",
    "    neuralNet.solidify()\n",
    "    weights, biases = neuralNet.getNetwork()\n",
    "    prediction = []\n",
    "    for image in trainx:\n",
    "        a,h = Algorithms.ForwardProp(weights, biases, Functions.sigmoid, Functions.softmax, image)\n",
    "        prediction.append(np.argmax(h[-1]))\n",
    "    prediction = np.array(prediction)\n",
    "    print(prediction)\n",
    "    Algorithms.evaluateNetwork(weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 9, 9, ..., 9, 9, 9])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vlcWrCZGiPRM"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.datasets import fashion_mnist\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import math\n",
    "import cv2 as cv\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "TfO2B6v6BVWj"
   },
   "outputs": [],
   "source": [
    "class PreProc:\n",
    "    def __init__(self):\n",
    "        (self.trainx,self.trainy),(self.testx, self.testy) = fashion_mnist.load_data()\n",
    "        \n",
    "    def visualize(self,n):\n",
    "        for i in range(n):\n",
    "            plt.subplot(330+1+i) # ask someone why??\n",
    "            plt.imshow(self.trainx[i], cmap = plt.get_cmap('gray'))\n",
    "        plt.show()\n",
    "            \n",
    "        \n",
    "    def flattenAndCentralize(self):\n",
    "        trainx_flattened = np.copy(self.trainx).astype('float64')\n",
    "        testx_flattened = np.copy(self.testx).astype('float64')\n",
    "        trainx_flattened -= np.mean(trainx_flattened, axis = 0)\n",
    "        testx_flattened -= np.mean(testx_flattened, axis = 0)\n",
    "        for image in trainx_flattened:\n",
    "            image = cv.GaussianBlur(image,(3,3),cv.BORDER_DEFAULT)\n",
    "        trainx_flattened.shape = (len(trainx_flattened),784)\n",
    "        testx_flattened.shape = (10000,784)\n",
    "        return trainx_flattened,testx_flattened\n",
    "    \n",
    "\n",
    "    \n",
    "    def getLabels(self):\n",
    "        return self.trainy, self.testy\n",
    "    \n",
    "    def getInputSize(self):\n",
    "        return len(self.trainx[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "7BwoVy0bPFA0"
   },
   "outputs": [],
   "source": [
    "class Functions:\n",
    "    @staticmethod\n",
    "    def sigmoid(input):\n",
    "        input = np.clip(input, -100,100)\n",
    "        return  1.0/(1.0+np.exp(-input))\n",
    "    \n",
    "    @staticmethod\n",
    "    def reLU(input):\n",
    "        return np.maximum(0.01*input,input)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tanh(input):\n",
    "        return np.tanh(input)\n",
    "    \n",
    "    @staticmethod\n",
    "    def identity(input):\n",
    "        return input\n",
    "    \n",
    "    @staticmethod\n",
    "    def mse(pred, true):\n",
    "        return pred*true\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax(input):\n",
    "        input = np.clip(input, -100,100)\n",
    "        return np.exp(input)/(np.sum(np.exp(input)))\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative_softmax(input):\n",
    "        return Functions.softmax(input)*(1-Functions.softmax(input))\n",
    "    \n",
    "    @staticmethod\n",
    "    def onehot(input):\n",
    "        result = np.zeros(10)\n",
    "        result[input] = 1\n",
    "        return result\n",
    "    \n",
    "    @staticmethod\n",
    "    def crossEntropyLoss(y,yHat):\n",
    "        loss = (-1/10.0) * np.sum(np.multiply(y, np.log(yHat+1e-10)) + np.multiply((1 - y), np.log(1 - (yHat+1e-10))))\n",
    "        return loss\n",
    "    \n",
    "    @staticmethod\n",
    "    def squaredErrorLoss(y,yHat):\n",
    "        return np.dot(y,yHat)\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative_sigmoid(input):\n",
    "        return Functions.sigmoid(input)*(1-Functions.sigmoid(input))\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative_tanh(input):\n",
    "        return (1 - (np.tanh(input)**2))\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative_reLU(input):\n",
    "        return np.where(input > 0, 1, 0.01)\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative_identity(input):\n",
    "        return 1\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot(input):\n",
    "        plt.plot(input)\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Loss over iterations\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def plotAccuract(input):\n",
    "        plt.plot(input)\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"val accuracy\")\n",
    "        plt.title(\"Train over iterations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Bu5XtsgmjyaH"
   },
   "outputs": [],
   "source": [
    "class Algorithms:\n",
    "    @staticmethod\n",
    "    def uniLoss(weights, biases, activate, output, t1, t2, lossFunc):\n",
    "        Loss = 0.0\n",
    "        if lossFunc == \"mse\":\n",
    "            for index in range(len(t1)):\n",
    "                a,h = Algorithms.ForwardProp(weights, biases, activate, output, t1[index])\n",
    "                pred = h[-1]\n",
    "                true = t2[index]\n",
    "                Loss += (Functions.mse(true, pred))\n",
    "        else:\n",
    "            for index in range(len(t1)):\n",
    "                a,h = Algorithms.ForwardProp(weights, biases, activate, output, t1[index])\n",
    "                pred = h[-1]\n",
    "                true = t2[index]\n",
    "                Loss += (Functions.crossEntropyLoss(Functions.onehot(true), pred))\n",
    "        return (Loss/len(t1))\n",
    "    \n",
    "    @staticmethod\n",
    "    def ForwardProp(weights, bias, activate, output, inputLayer):\n",
    "        L = len(weights)-1\n",
    "        a = []\n",
    "        h = []\n",
    "        a.append(np.matmul(weights[0],inputLayer)+bias[0])\n",
    "        h.append(activate(a[0]))\n",
    "        for k in range(1,L):\n",
    "            a.append(np.matmul(weights[k],h[k-1].T)+bias[k])\n",
    "            h.append(activate(a[k]))\n",
    "        a.append(np.matmul(weights[L],h[L-1].T)+bias[L])\n",
    "        h.append(output(a[L]))\n",
    "        return a,h\n",
    "    @staticmethod\n",
    "    def BackProp(weights, biases, a, h, derivative, dataPoint, dataLabel, lossFunc):\n",
    "        L = len(weights)-1\n",
    "        if lossFunc == \"mse\":\n",
    "            gradaL = -((Functions.onehot(dataLabel) - h[-1])*(Functions.derivative_softmax(a[-1])))\n",
    "        else:\n",
    "            gradaL = -(Functions.onehot(dataLabel)-h[len(h)-1])\n",
    "        dw = np.zeros_like(weights)\n",
    "        db = np.zeros_like(biases)\n",
    "        for k in range(L,0,-1):\n",
    "            gradW = np.outer(gradaL, h[k-1].T)\n",
    "            gradB = gradaL\n",
    "            dw[k] = gradW\n",
    "            db[k] = gradB\n",
    "\n",
    "            gradhL_1 = np.matmul(np.transpose(weights[k]),gradaL)\n",
    "            gradaL_1 = np.multiply(gradhL_1, derivative(a[k-1]))\n",
    "            gradaL = gradaL_1\n",
    "        dw[0] = np.outer(gradaL,dataPoint.T)\n",
    "        db[0] = gradaL\n",
    "        return dw, db\n",
    "\n",
    "    @staticmethod\n",
    "    def miniBatchMGD(weights, biases, batchSize, learningRate, activate, output, derivative, dataPoints, dataLabels, valTest_x, valTest_y, decay, epochs, lossFunc):\n",
    "        validation = []\n",
    "        validationLoss = []\n",
    "        trainAccuracy = []\n",
    "        loss = []\n",
    "        lossTrack = []\n",
    "        LOSS = 0.0\n",
    "        beta = 0.9\n",
    "        prevWeights = np.zeros_like(weights)\n",
    "        prevBiases = np.zeros_like(biases)\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            indices = np.arange(len(dataPoints))\n",
    "            np.random.shuffle(indices)\n",
    "            batchX = dataPoints[indices]\n",
    "            batchY = dataLabels[indices]\n",
    "            Loss = 0.0\n",
    "            for i in range(math.ceil(len(dataPoints)/batchSize)):\n",
    "                if (i+1)*batchSize < len(dataPoints):\n",
    "                    trainer = batchX[i*batchSize:i*batchSize+batchSize]\n",
    "                    labeler = batchY[i*batchSize:i*batchSize+batchSize]\n",
    "                else:\n",
    "                    trainer = batchX[i*batchSize:len(dataPoints)]\n",
    "                    labeler = batchY[i*batchSize:len(dataPoints)]\n",
    "                batchLoss = 0.0\n",
    "                dw = np.zeros_like(weights)\n",
    "                db = np.zeros_like(biases)\n",
    "                for data in range(len(trainer)):\n",
    "                    a,h = Algorithms.ForwardProp(weights, biases , activate, output, trainer[data])\n",
    "                    currWeights, currBiases = Algorithms.BackProp(weights, biases, a, h, derivative, trainer[data], labeler[data], lossFunc)\n",
    "                    if lossFunc == \"mse\":\n",
    "                        batchLoss += Functions.mse(Functions.onehot(labeler[data]), h[-1])\n",
    "                    else:\n",
    "                        batchLoss += Functions.crossEntropyLoss(Functions.onehot(labeler[data]), h[-1])\n",
    "                    dw += currWeights\n",
    "                    db += currBiases\n",
    "                batchLoss /= batchSize\n",
    "                dw /= batchSize\n",
    "                db /= batchSize\n",
    "                momentumWeights = prevWeights*beta + dw*1.0\n",
    "                momentumBiases = prevBiases*beta + db*1.0\n",
    "                weights -= learningRate*(momentumWeights + decay*weights)\n",
    "                biases -= learningRate*(momentumBiases + decay*biases)\n",
    "                prevWeights = momentumWeights\n",
    "                prevBiases = momentumBiases\n",
    "                lossTrack.append(batchLoss)\n",
    "                \n",
    "            validation.append(Algorithms.evaluateNetwork(weights, biases, activate, output, valTest_x, valTest_y))\n",
    "            validationLoss.append((Algorithms.uniLoss(weights, biases, activate, output, valTest_x, valTest_y, lossFunc)))\n",
    "            trainAccuracy.append(Algorithms.evaluateNetwork(weights, biases, activate, output, testx, test_y))\n",
    "            wandb.log({\"Train accuracy\": trainAccuracy[-1]}, {\"Validation Accuracy\":validation[-1]}, {\"Validation Loss\":validationLoss[-1]})\n",
    "            print(validationLoss)\n",
    "            \n",
    "        return weights, biases\n",
    "\n",
    "    @staticmethod\n",
    "    def ADAM(weights, biases, batchSize, learningRate, activate, output, derivative, dataPoints, dataLabels, valTest_x, valTest_y, decay, epochs, lossFunc):\n",
    "        validation = []\n",
    "        validationLoss = []\n",
    "        trainAccuracy = []\n",
    "        loss = []\n",
    "        lossTrack = []\n",
    "        prev_val = 0.0\n",
    "        beta1 = 0.9\n",
    "        beta2 = 0.99\n",
    "        epsilon = 0.000001\n",
    "        patience = 3\n",
    "        m_w = np.zeros_like(weights)\n",
    "        v_w = np.zeros_like(weights)\n",
    "        m_b = np.zeros_like(biases)\n",
    "        v_b = np.zeros_like(biases)\n",
    "        learnerRateW = np.full_like(weights, learningRate)\n",
    "        learnerRateB = np.full_like(biases, learningRate)\n",
    "        i = 0\n",
    "        \n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            indices = np.arange(len(dataPoints))\n",
    "            np.random.shuffle(indices)\n",
    "            batchX = dataPoints[indices]\n",
    "            batchY = dataLabels[indices]\n",
    "            Loss = 0.0\n",
    "            for i in range(math.ceil(len(dataPoints)/batchSize)):\n",
    "                if (i+1)*batchSize < len(dataPoints):\n",
    "                    trainer = batchX[i*batchSize:i*batchSize+batchSize]\n",
    "                    labeler = batchY[i*batchSize:i*batchSize+batchSize]\n",
    "                else:\n",
    "                    trainer = batchX[i*batchSize:len(dataPoints)]\n",
    "                    labeler = batchY[i*batchSize:len(dataPoints)]\n",
    "                batchLoss = 0.0\n",
    "                dw = np.zeros_like(weights)\n",
    "                db = np.zeros_like(biases)\n",
    "                for data in range(len(trainer)):\n",
    "                    a,h = Algorithms.ForwardProp(weights, biases,activate, output, trainer[data])\n",
    "                    currWeights, currBiases = Algorithms.BackProp(weights, biases , a, h, derivative, trainer[data], labeler[data], lossFunc)\n",
    "                    if lossFunc == \"mse\":\n",
    "                        batchLoss += Functions.mse(Functions.onehot(labeler[data]), h[-1])\n",
    "                    else:\n",
    "                        batchLoss += Functions.crossEntropyLoss(Functions.onehot(labeler[data]), h[-1])\n",
    "                    \n",
    "                    dw += currWeights\n",
    "                    db += currBiases\n",
    "                batchLoss /= batchSize\n",
    "                dw /= batchSize\n",
    "                db /= batchSize\n",
    "                m_w = beta1*m_w + (1-beta1)*dw\n",
    "                m_b = beta1*m_b + (1-beta1)*db\n",
    "                v_w = v_w*beta2 + (1-beta2)*dw**2\n",
    "                v_b = v_b*beta2 + (1-beta2)*db**2\n",
    "                \n",
    "                m_w_hat = m_w/(1 - np.power(beta1, i+1))\n",
    "                m_b_hat = m_b/(1 - np.power(beta1, i+1))\n",
    "                v_w_hat = v_w/(1 - np.power(beta2, i+1))\n",
    "                v_b_hat = v_b/(1 - np.power(beta2, i+1))\n",
    "                \n",
    "                i+=1\n",
    "                \n",
    "                tempW = np.zeros_like(m_w)\n",
    "                tempB = np.zeros_like(m_b)\n",
    "                for i in range(len(dw)):\n",
    "                    tempW[i] = np.sqrt(v_w_hat[i])\n",
    "                    tempB[i] = np.sqrt(v_b_hat[i])\n",
    "                weights = weights - ((learnerRateW*(dw + decay*weights))/(tempW + epsilon))\n",
    "                biases = biases - (learnerRateB*(db + decay*biases))/(tempB+epsilon)\n",
    "\n",
    "                lossTrack.append(batchLoss)\n",
    "                \n",
    "            Loss /= len(dataPoints)\n",
    "            loss.append(Loss)\n",
    "            validation.append(Algorithms.evaluateNetwork(weights, biases, activate, output, valTest_x, valTest_y))\n",
    "            validationLoss.append((Algorithms.uniLoss(weights, biases, activate, output, valTest_x, valTest_y, lossFunc)))\n",
    "            trainAccuracy.append(Algorithms.evaluateNetwork(weights, biases, activate, output, trainx, train_y))\n",
    "            wandb.log({\"Train accuracy\": trainAccuracy[-1]}, {\"Validation Accuracy\":validation[-1]}, {\"Validation Loss\":validationLoss[-1]})\n",
    "            print(trainAccuracy[-1])\n",
    "                \n",
    "            if validation[-1] <= prev_val+1e-2:\n",
    "                patience -= 1\n",
    "            else:\n",
    "                patience = 3\n",
    "                prev_weights = weights\n",
    "                prev_biases = biases\n",
    "                prev_val = validation[-1]\n",
    "            \n",
    "            if patience == 0:\n",
    "                weights = prev_weights\n",
    "                biases = prev_biases\n",
    "            Loss /= len(dataPoints)\n",
    "            loss.append(Loss)\n",
    "            \n",
    "        if prev_val > validation[-1]:\n",
    "            return prev_weights, prev_biases\n",
    "            \n",
    "        return weights,biases\n",
    "\n",
    "    @staticmethod\n",
    "    def NADAM(weights, biases, batchSize, learningRate, activate, output, derivative, dataPoints, dataLabels, valTest_x, valTest_y, decay, epochs, lossFunc):\n",
    "        patience = 3\n",
    "        validation = []\n",
    "        validationLoss = []\n",
    "        trainAccuracy = []\n",
    "        loss = []\n",
    "        lossTrack = []\n",
    "        prev_val = 0.0\n",
    "        prevWeights = np.zeros_like(weights)\n",
    "        prevBiases = np.zeros_like(biases)\n",
    "        beta1 = 0.9\n",
    "        beta2 = 0.99\n",
    "        epsilon = 0.000001\n",
    "        prev_weights = np.zeros_like(weights)\n",
    "        prev_biases = np.zeros_like(biases)\n",
    "        m_w = np.zeros_like(weights)\n",
    "        v_w = np.zeros_like(weights)\n",
    "        m_b = np.zeros_like(biases)\n",
    "        v_b = np.zeros_like(biases)\n",
    "        learnerRateW = np.full_like(weights, learningRate)\n",
    "        learnerRateB = np.full_like(biases, learningRate)\n",
    "        i = 0\n",
    "        \n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            indices = np.arange(len(dataPoints))\n",
    "            np.random.shuffle(indices)\n",
    "            batchX = dataPoints[indices]\n",
    "            batchY = dataLabels[indices]\n",
    "            Loss = 0.0\n",
    "            for i in range(math.ceil(len(dataPoints)/batchSize)):\n",
    "                if (i+1)*batchSize < len(dataPoints):\n",
    "                    trainer = batchX[i*batchSize:i*batchSize+batchSize]\n",
    "                    labeler = batchY[i*batchSize:i*batchSize+batchSize]\n",
    "                else:\n",
    "                    trainer = batchX[i*batchSize:len(dataPoints)]\n",
    "                    labeler = batchY[i*batchSize:len(dataPoints)]\n",
    "                batchLoss = 0.0\n",
    "                dw = np.zeros_like(weights)\n",
    "                db = np.zeros_like(biases)\n",
    "                for data in range(len(trainer)):\n",
    "                    a,h = Algorithms.ForwardProp(weights - v_w*(beta1), biases - v_b*(beta1), activate, output, trainer[data])\n",
    "                    currWeights, currBiases = Algorithms.BackProp(weights - v_w*(beta1), biases - v_b*(beta1), a, h, derivative, trainer[data], labeler[data], lossFunc)\n",
    "                    if lossFunc == \"mse\":\n",
    "                        batchLoss += Functions.mse(Functions.onehot(labeler[data]), h[-1])\n",
    "                    else:\n",
    "                        batchLoss += Functions.crossEntropyLoss(Functions.onehot(labeler[data]), h[-1])\n",
    "                    \n",
    "                    dw += currWeights\n",
    "                    db += currBiases\n",
    "                batchLoss /= batchSize\n",
    "                dw /= batchSize\n",
    "                db /= batchSize\n",
    "                m_w = beta1*m_w + (1-beta1)*dw\n",
    "                m_b = beta1*m_b + (1-beta1)*db\n",
    "                v_w = v_w*beta2 + (1-beta2)*dw**2\n",
    "                v_b = v_b*beta2 + (1-beta2)*db**2\n",
    "                \n",
    "                m_w_hat = m_w/(1 - np.power(beta1, i+1))\n",
    "                m_b_hat = m_b/(1 - np.power(beta1, i+1))\n",
    "                v_w_hat = v_w/(1 - np.power(beta2, i+1))\n",
    "                v_b_hat = v_b/(1 - np.power(beta2, i+1))\n",
    "                \n",
    "                i+=1\n",
    "                \n",
    "                tempW = np.zeros_like(m_w)\n",
    "                tempB = np.zeros_like(m_b)\n",
    "                for j in range(len(dw)):\n",
    "                    tempW[j] = np.sqrt(v_w_hat[j])\n",
    "                    tempB[j] = np.sqrt(v_b_hat[j])\n",
    "                weights = weights - ((learnerRateW*(dw + decay*weights))/(tempW + epsilon))\n",
    "                biases = biases - (learnerRateB*(db + decay*biases))/(tempB+epsilon)\n",
    "                lossTrack.append(batchLoss)\n",
    "            #wandb.log({'valAcc':(Algorithms.evaluateNetwork(weights, biases, valTest_x, valTest_y)), 'epoch':epoch})\n",
    "            validation.append(Algorithms.evaluateNetwork(weights, biases, activate, output, valTest_x, valTest_y))\n",
    "            if validation[-1] <= prev_val - 2*1e-2:\n",
    "                patience -= 1\n",
    "            else:\n",
    "                patience = 3\n",
    "                prev_weights = weights\n",
    "                prev_biases = biases\n",
    "                prev_val = validation[-1]\n",
    "            \n",
    "            Loss /= len(dataPoints)\n",
    "            loss.append(Loss)\n",
    "            validationLoss.append((Algorithms.uniLoss(weights, biases, activate, output, valTest_x, valTest_y, lossFunc)))\n",
    "            trainAccuracy.append(Algorithms.evaluateNetwork(weights, biases, activate, output, trainx, train_y))\n",
    "            wandb.log({\"Train accuracy\": trainAccuracy[-1]}, {\"Validation Accuracy\":validation[-1]}, {\"Validation Loss\":validationLoss[-1]})\n",
    "            print(trainAccuracy[-1])\n",
    "            \n",
    "            if patience <= 0:\n",
    "                return prev_weights, prev_biases\n",
    "            \n",
    "        return prev_weights, prev_biases\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def miniBatchNAG(weights, biases, batchSize, learningRate, activate, output, derivative, dataPoints, dataLabels, valTest_x, valTest_y, decay, epochs, lossFunc):\n",
    "        validation = []\n",
    "        validationLoss = []\n",
    "        trainAccuracy = []\n",
    "        loss = []\n",
    "        lossTrack = []\n",
    "        beta = 0.9\n",
    "        prevWeights = np.zeros_like(weights)\n",
    "        prevBiases = np.zeros_like(biases)\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            indices = np.arange(len(dataPoints))\n",
    "            np.random.shuffle(indices)\n",
    "            batchX = dataPoints[indices]\n",
    "            batchY = dataLabels[indices]\n",
    "            Loss = 0.0\n",
    "            for i in range(math.ceil(len(dataPoints)/batchSize)):\n",
    "                if (i+1)*batchSize < len(dataPoints):\n",
    "                    trainer = batchX[i*batchSize:i*batchSize+batchSize]\n",
    "                    labeler = batchY[i*batchSize:i*batchSize+batchSize]\n",
    "                else:\n",
    "                    trainer = batchX[i*batchSize:len(dataPoints)]\n",
    "                    labeler = batchY[i*batchSize:len(dataPoints)]\n",
    "                batchLoss = 0.0\n",
    "                tempWeights = np.zeros_like(weights)\n",
    "                tempBiases = np.zeros_like(biases)\n",
    "                for data in range(len(trainer)):\n",
    "                    a,h = Algorithms.ForwardProp(weights-prevWeights, biases-prevBiases, activate, output, trainer[data])\n",
    "                    currWeights, currBiases = Algorithms.BackProp(weights-prevWeights, biases-prevBiases, a, h, derivative, trainer[data], labeler[data], lossFunc)\n",
    "                    if lossFunc == \"mse\":\n",
    "                        batchLoss += Functions.mse(Functions.onehot(labeler[data]), h[-1])\n",
    "                    else:\n",
    "                        batchLoss += Functions.crossEntropyLoss(Functions.onehot(labeler[data]), h[-1])\n",
    "                    tempWeights += currWeights\n",
    "                    tempBiases += currBiases\n",
    "                batchLoss /= batchSize\n",
    "                tempWeights /= batchSize\n",
    "                tempBiases /= batchSize\n",
    "                momentumWeights = beta*prevWeights + tempWeights*1.0\n",
    "                momentumBiases = beta*prevBiases + tempBiases*1.0\n",
    "                weights = weights - learningRate*(momentumWeights + decay*weights) \n",
    "                biases = biases - learningRate*(momentumBiases + decay*biases)\n",
    "                prevWeights = momentumWeights\n",
    "                prevBiases = momentumBiases\n",
    "                lossTrack.append(batchLoss)\n",
    "            Loss /= len(dataPoints)\n",
    "            loss.append(Loss)\n",
    "            validation.append(Algorithms.evaluateNetwork(weights, biases, activate, output, valTest_x, valTest_y))\n",
    "            validationLoss.append((Algorithms.uniLoss(weights, biases, activate, output, valTest_x, valTest_y, lossFunc)))\n",
    "            trainAccuracy.append(Algorithms.evaluateNetwork(weights, biases, activate, output, trainx, train_y))\n",
    "            wandb.log({\"Train accuracy\": trainAccuracy[-1]}, {\"Validation Accuracy\":validation[-1]}, {\"Validation Loss\":validationLoss[-1]})\n",
    "            print(trainAccuracy[-1])\n",
    "            \n",
    "        return weights, biases\n",
    "    \n",
    "    @staticmethod\n",
    "    def RMSProp(weights, biases, batchSize, learningRate, activate, output, derivative, dataPoints, dataLabels, valTest_x, valTest_y, decay, epochs, lossFunc):\n",
    "        validation = []\n",
    "        validationLoss = []\n",
    "        trainAccuracy = []\n",
    "        loss = []\n",
    "        lossTrack = []\n",
    "        beta = 0.5\n",
    "        epsilon = 0.000001\n",
    "        momentumWeights = np.zeros_like(weights)\n",
    "        momentumBiases = np.zeros_like(biases)\n",
    "        learnerRateW = np.full_like(weights, learningRate)\n",
    "        learnerRateB = np.full_like(biases, learningRate)\n",
    "        \n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            indices = np.arange(len(dataPoints))\n",
    "            np.random.shuffle(indices)\n",
    "            batchX = dataPoints[indices]\n",
    "            batchY = dataLabels[indices]\n",
    "            Loss = 0.0\n",
    "            for i in range(math.ceil(len(dataPoints)/batchSize)):\n",
    "                if (i+1)*batchSize < len(dataPoints):\n",
    "                    trainer = batchX[i*batchSize:i*batchSize+batchSize]\n",
    "                    labeler = batchY[i*batchSize:i*batchSize+batchSize]\n",
    "                else:\n",
    "                    trainer = batchX[i*batchSize:len(dataPoints)]\n",
    "                    labeler = batchY[i*batchSize:len(dataPoints)]\n",
    "                batchLoss = 0.0\n",
    "                dw = np.zeros_like(weights)\n",
    "                db = np.zeros_like(biases)\n",
    "                for data in range(len(trainer)):\n",
    "                    a,h = Algorithms.ForwardProp(weights, biases, activate, output, trainer[data])\n",
    "                    currWeights, currBiases = Algorithms.BackProp(weights, biases , a, h, derivative, trainer[data], labeler[data], lossFunc)\n",
    "                    if lossFunc == \"mse\":\n",
    "                        batchLoss += Functions.mse(Functions.onehot(labeler[data]), h[-1])\n",
    "                    else:\n",
    "                        batchLoss += Functions.crossEntropyLoss(Functions.onehot(labeler[data]), h[-1])\n",
    "                    dw += currWeights\n",
    "                    db += currBiases\n",
    "                batchLoss /= batchSize\n",
    "                dw /= batchSize\n",
    "                db /= batchSize\n",
    "                momentumWeights = momentumWeights*beta + (1-beta)*dw**2\n",
    "                momentumBiases = momentumBiases*beta + (1-beta)*db**2\n",
    "                tempW = np.zeros_like(momentumWeights)\n",
    "                tempB = np.zeros_like(momentumBiases)\n",
    "                for i in range(len(dw)):\n",
    "                    tempW[i] = np.sqrt(momentumWeights[i])\n",
    "                    tempB[i] = np.sqrt(momentumBiases[i])\n",
    "                weights = weights - ((learnerRateW)*(dw + decay*weights)/(tempW + epsilon))\n",
    "                biases = biases - (learnerRateB*(db + decay*biases))/(tempB+epsilon)\n",
    "                lossTrack.append(batchLoss)\n",
    "            Loss /= len(dataPoints)\n",
    "            loss.append(Loss)\n",
    "            validation.append(Algorithms.evaluateNetwork(weights, biases, activate, output, valTest_x, valTest_y))\n",
    "            validationLoss.append((Algorithms.uniLoss(weights, biases, activate, output, valTest_x, valTest_y, lossFunc)))\n",
    "            trainAccuracy.append(Algorithms.evaluateNetwork(weights, biases, activate, output, trainx, train_y))\n",
    "            wandb.log({\"Train accuracy\": trainAccuracy[-1]}, {\"Validation Accuracy\":validation[-1]}, {\"Validation Loss\":validationLoss[-1]})\n",
    "            print(trainAccuracy[-1])\n",
    "        return weights, biases\n",
    "\n",
    "    @staticmethod\n",
    "    def miniBatchGD(weights, biases, batchSize, learningRate, activate, output, derivative, dataPoints, dataLabels, valTest_x, valTest_y, decay, epochs, lossFunc):\n",
    "        validation = []\n",
    "        validationLoss = []\n",
    "        trainAccuracy = []\n",
    "        loss = []\n",
    "        lossTrack = []\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            indices = np.arange(len(dataPoints))\n",
    "            np.random.shuffle(indices)\n",
    "            batchX = dataPoints[indices]\n",
    "            batchY = dataLabels[indices]\n",
    "            Loss = 0.0\n",
    "            for i in range(math.ceil(len(dataPoints)/batchSize)):\n",
    "                if (i+1)*batchSize < len(dataPoints):\n",
    "                    trainer = batchX[i*batchSize:i*batchSize+batchSize]\n",
    "                    labeler = batchY[i*batchSize:i*batchSize+batchSize]\n",
    "                else:\n",
    "                    trainer = batchX[i*batchSize:len(dataPoints)]\n",
    "                    labeler = batchY[i*batchSize:len(dataPoints)]\n",
    "                batchLoss = 0.0\n",
    "                dw = np.zeros_like(weights)\n",
    "                db = np.zeros_like(biases)\n",
    "                for data in range(len(trainer)):\n",
    "                    a,h = Algorithms.ForwardProp(weights, biases , activate, output, trainer[data])\n",
    "                    tempw,tempb = Algorithms.BackProp(weights, biases, a, h, derivative, trainer[data], labeler[data], lossFunc)\n",
    "                    dw+=tempw\n",
    "                    db+=tempb\n",
    "                    if lossFunc == \"mse\":\n",
    "                        batchLoss += Functions.mse(Functions.onehot(labeler[data]), h[-1])\n",
    "                    else:\n",
    "                        batchLoss += Functions.crossEntropyLoss(Functions.onehot(labeler[data]), h[-1])\n",
    "                batchLoss /= batchSize\n",
    "                dw /= batchSize\n",
    "                db /= batchSize\n",
    "                weights -= learningRate*(dw + decay*weights)\n",
    "                biases -= learningRate*(db + decay*biases)\n",
    "                lossTrack.append(batchLoss)\n",
    "            Loss /= len(dataPoints)\n",
    "            loss.append(Loss)\n",
    "            validation.append(Algorithms.evaluateNetwork(weights, biases, activate, output, valTest_x, valTest_y))\n",
    "            validationLoss.append((Algorithms.uniLoss(weights, biases, activate, output, valTest_x, valTest_y, lossFunc)))\n",
    "            trainAccuracy.append(Algorithms.evaluateNetwork(weights, biases, activate, output, trainx, train_y))\n",
    "            wandb.log({\"Train accuracy\": trainAccuracy[-1]}, {\"Validation Accuracy\":validation[-1]}, {\"Validation Loss\":validationLoss[-1]})\n",
    "            print(trainAccuracy[-1])\n",
    "        return weights, biases\n",
    "    \n",
    "    @staticmethod\n",
    "    def evaluateNetwork(weights, biases,activate, output, test_x, test_y):\n",
    "        num_acc = 0\n",
    "        for i in range(len(test_x)):\n",
    "            a,h = Algorithms.ForwardProp(weights, biases, activate, output, test_x[i])\n",
    "            h = np.array(h, dtype = object)\n",
    "            predY =   np.argmax(h[len(h)-1])\n",
    "            if test_y[i] == predY:\n",
    "                num_acc+=1\n",
    "        return (num_acc/len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qHfSx0jHSqtk",
    "outputId": "5e2dae53-035e-468c-98d7-d29b08d821f1"
   },
   "outputs": [],
   "source": [
    "#The class of FeedForwardNeuralNetwor\n",
    "\n",
    "class FFNet:\n",
    "    #constructor\n",
    "    def __init__(self,number_of_hidden_layers, number_of_inputs, number_of_outputs):\n",
    "        self.number_of_inputs = number_of_inputs\n",
    "        self.number_of_hidden_layers = number_of_hidden_layers\n",
    "        self.number_of_outputs = number_of_outputs\n",
    "        self.input = [0 for i in range(number_of_inputs)]\n",
    "        self.output = [0 for i in range(10)]\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "    \n",
    "    #Method for creating layers\n",
    "    def addHiddenLayer(self,number_of_neurons, initialization):\n",
    "        if(len(self.weights) == 0):\n",
    "            temp_weights = np.random.randn(number_of_neurons, self.number_of_inputs)*0.01\n",
    "            temp_biases = np.full((number_of_neurons), 0.01)\n",
    "            if initialization == \"xavier\":\n",
    "                temp_weights = np.random.randn(number_of_neurons, self.number_of_inputs)/np.sqrt((self.number_of_inputs)/2)\n",
    "                #temp_biases = np.random.randn(number_of_neurons)*np.sqrt(1/(number_of_neurons))\n",
    "        else:\n",
    "            prev_neurons = len(self.weights[-1])\n",
    "            temp_weights = np.random.randn(number_of_neurons, prev_neurons)*0.01\n",
    "            temp_biases = np.full((number_of_neurons), 0.01)\n",
    "            if initialization == \"xavier\":\n",
    "                temp_weights = np.random.randn(number_of_neurons, prev_neurons)/np.sqrt((prev_neurons)/2)\n",
    "                #temp_biases = np.random.randn(number_of_neurons)*np.sqrt(1/(number_of_neurons))\n",
    "\n",
    "        self.weights.append(temp_weights)\n",
    "        self.biases.append(temp_biases)\n",
    "    \n",
    "    def addOutputLayer(self, number_of_outputs, initialization):\n",
    "        if(len(self.weights) == 0):\n",
    "            #print(\"number of inputs: \"+str(self.number_of_inputs))\n",
    "            temp_weights = np.random.randn(number_of_outputs, self.number_of_inputs)*0.01\n",
    "            temp_biases = np.full((number_of_outputs), 0.01)\n",
    "            if initialization == \"xavier\":\n",
    "                temp_weights = np.random.randn(number_of_outputs, self.number_of_inputs)/np.sqrt((prev_neurons)/2)\n",
    "        else:\n",
    "            prev_neurons = len(self.weights[-1])\n",
    "            temp_weights = np.random.randn(number_of_outputs, prev_neurons)*0.01\n",
    "            temp_biases = np.full((number_of_outputs), 0.01)\n",
    "            if initialization == \"xavier\":\n",
    "                temp_weights = np.random.randn(number_of_outputs, prev_neurons)/np.sqrt((prev_neurons)/2)\n",
    "                \n",
    "        \n",
    "        self.weights.append(temp_weights)\n",
    "        self.biases.append(temp_biases)\n",
    "\n",
    "    def solidify(self):\n",
    "        self.weights = np.array(self.weights, dtype = object)\n",
    "        self.biases = np.array(self.biases, dtype = object)\n",
    "\n",
    "    def getNetwork(self):\n",
    "        return self.weights,self.biases\n",
    "    \n",
    "    def ForwardProp(self, activate, output, inputLayer):\n",
    "        return Algorithms.ForwardProp(self.network, activate, output, inputLayer)\n",
    "    \n",
    "    def lossCalc(self, lossFunction, Y):\n",
    "        predY = self.historyA[(len(self.historyA)-1)]\n",
    "        return lossFunction(Y,self.predY)\n",
    "\n",
    "    def BackProp(self, a, h, dataPoint, dataLabel):\n",
    "        return Algorithms.BackProp(self.network, a, h, dataPoint, dataLabel)\n",
    "    \n",
    "    def fit(self, optimizer, batchSize, learningRate, activation, trainx, train_y, decay, epochs, lossFunc):\n",
    "        \n",
    "        #break data into training and validation\n",
    "        indices = np.arange(len(trainx))\n",
    "        np.random.shuffle(indices)\n",
    "        trainx = trainx[indices]\n",
    "        train_y = train_y[indices]\n",
    "        \n",
    "        valTest_x = trainx[int(0.9*len(trainx)):]\n",
    "        valTest_y = train_y[int(0.9*len(train_y)):]\n",
    "        \n",
    "        trainx = trainx[:int(0.9*len(trainx))]\n",
    "        train_y = train_y[:int(0.9*len(train_y))]\n",
    "        \n",
    "        if activation == \"relu\":\n",
    "            activate = Functions.reLU\n",
    "            derivative = Functions.derivative_reLU\n",
    "            output = Functions.softmax\n",
    "        elif activation == \"tanh\":\n",
    "            activate = Functions.tanh\n",
    "            derivative = Functions.derivative_tanh\n",
    "            output = Functions.softmax\n",
    "        elif activation == \"identity\":\n",
    "            activate = Functions.identity\n",
    "            derivative = Functions.derivative_identity\n",
    "            output = Functions.softmax\n",
    "        else:\n",
    "            activate = Functions.sigmoid\n",
    "            derivative = Functions.derivative_sigmoid\n",
    "            output = Functions.softmax\n",
    "        \n",
    "        #print(optimizer)\n",
    "        if optimizer == \"momentum\":\n",
    "            self.weights, self.biases = Algorithms.miniBatchMGD(self.weights,self.biases , batchSize, learningRate, activate, output, derivative,  trainx, train_y, valTest_x, valTest_y, decay, epochs, lossFunc)\n",
    "        elif optimizer == \"nag\":\n",
    "            self.weights, self.biases = Algorithms.miniBatchNAG(self.weights,self.biases , batchSize, learningRate,activate, output, derivative , trainx, train_y, valTest_x, valTest_y, decay, epochs, lossFunc)\n",
    "        elif optimizer == \"rmsprop\":\n",
    "            self.weights, self.biases = Algorithms.RMSProp(self.weights,self.biases , batchSize, learningRate, activate, output, derivative,  trainx, train_y, valTest_x, valTest_y, decay, epochs, lossFunc)\n",
    "        elif optimizer == \"adam\":\n",
    "            self.weights, self.biases = Algorithms.ADAM(self.weights,self.biases , batchSize, learningRate,activate, output, derivative,  trainx, train_y, valTest_x, valTest_y, decay, epochs, lossFunc)\n",
    "        elif optimizer == \"nadam\":\n",
    "            self.weights, self.biases = Algorithms.NADAM(self.weights,self.biases , batchSize, learningRate, activate, output, derivative,  trainx, train_y, valTest_x, valTest_y, decay, epochs, lossFunc)\n",
    "        else:\n",
    "            self.weights, self.biases = Algorithms.miniBatchGD(self.weights,self.biases , batchSize, learningRate, activate, output, derivative , trainx, train_y, valTest_x, valTest_y, decay, epochs, lossFunc)\n",
    "        print(Algorithms.evaluateNetwork(self.weights, self.biases, activate, output, testx, test_y))       \n",
    "        \n",
    "        return self.weights,self.biases\n",
    "            \n",
    "    def evaluateNetwork(self, testx, tes_ty):\n",
    "        Algorithms.evaluateNetwork(self.weights, self.biases, testx, test_y)        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
