{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "vlcWrCZGiPRM"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.datasets import fashion_mnist\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import math\n",
    "import cv2 as cv\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "TfO2B6v6BVWj"
   },
   "outputs": [],
   "source": [
    "class PreProc:\n",
    "    '''Class used for preprocessing all images. \n",
    "        making a constructor of this class immediately loads in desired dataset\n",
    "        \n",
    "        visualize(n) logs into wandb 10 images each belonging to a separate class.\n",
    "        \n",
    "        flattenAndCentralize() makes the mean of the image arrays 0. This helps increasing the \n",
    "        training accuracy quicker per epoch\n",
    "        \n",
    "        getLabels() return labels in corresponding index fashion\n",
    "        \n",
    "        getInputsize returns the number of images present in the training sample\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        (self.trainx,self.trainy),(self.testx, self.testy) = fashion_mnist.load_data()\n",
    "        \n",
    "    def visualize(self,n):\n",
    "        ''' args -> n :: The number of images desired to be visualized\n",
    "            returns-> null\n",
    "            \n",
    "            shows the images via matplotlib\n",
    "        '''\n",
    "        for i in range(n):\n",
    "            plt.subplot(330+1+i) # ask someone why??\n",
    "            plt.imshow(self.trainx[i], cmap = plt.get_cmap('gray'))\n",
    "        plt.show()\n",
    "            \n",
    "        \n",
    "    def flattenAndCentralize(self):\n",
    "        ''' args -> none\n",
    "            returns -> trainx_flattened :: The training images, mean centered and flattened\n",
    "                        into a 1 dimensional array\n",
    "                    -> testx_flattened :: The testing images, mean centered and flattened\n",
    "                        into a 1 dimensional array\n",
    "        '''\n",
    "        trainx_flattened = np.copy(self.trainx).astype('float64')\n",
    "        testx_flattened = np.copy(self.testx).astype('float64')\n",
    "        trainx_flattened -= np.mean(trainx_flattened, axis = 0)\n",
    "        testx_flattened -= np.mean(testx_flattened, axis = 0)\n",
    "        for image in trainx_flattened:\n",
    "            image = cv.GaussianBlur(image,(3,3),cv.BORDER_DEFAULT)\n",
    "        trainx_flattened.shape = (len(trainx_flattened),784)\n",
    "        testx_flattened.shape = (10000,784)\n",
    "        return trainx_flattened,testx_flattened\n",
    "    \n",
    "\n",
    "    \n",
    "    def getLabels(self):\n",
    "        ''' args -> none\n",
    "            returns -> self.trainy :: The labels of the training data\n",
    "                    -> self.testy :: The labels of the testing data\n",
    "        '''\n",
    "        return self.trainy, self.testy\n",
    "    \n",
    "    def getInputSize(self):\n",
    "        return len(self.trainx[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "7BwoVy0bPFA0"
   },
   "outputs": [],
   "source": [
    "class Functions:\n",
    "    ''' The Functions class/ Library stores static methods corresponding to all the functions\n",
    "        To be used in the program/training/testing.\n",
    "        The correct implementation of these is vital to the correct working of the neural net\n",
    "        model\n",
    "    '''\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(input):\n",
    "        ''' args -> input :: the input value, a numpy array type to the sigmoid function\n",
    "            return -> np.array :: the np array containing calculated sigmoid values (per input[i])\n",
    "        '''\n",
    "        input = np.clip(input, -100,100)\n",
    "        return  1.0/(1.0+np.exp(-input))\n",
    "    \n",
    "    @staticmethod\n",
    "    def reLU(input):\n",
    "        ''' args -> input :: the input value, a numpy array type to the reLU function\n",
    "            return -> np.array :: the np array containing calculated relu values (per input[i])\n",
    "        '''\n",
    "        return np.maximum(0.01*input,input)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tanh(input):\n",
    "        ''' args -> input :: the input value, a numpy array type to the tanh function\n",
    "            return -> np.array :: the np array containing calculated tanh values (per input[i])\n",
    "        '''\n",
    "        return np.tanh(input)\n",
    "    \n",
    "    @staticmethod\n",
    "    def identity(input):\n",
    "        ''' args -> input :: the input value, a numpy array type to the identity function\n",
    "            return -> np.array :: the np array containing calculated same values (per input[i])\n",
    "        '''\n",
    "        return input\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax(input):\n",
    "        ''' args -> input :: the input value, a numpy array type to the softmax function\n",
    "            return -> np.array :: the np array containing calculated softmax values (per input[i])\n",
    "        '''\n",
    "        input = np.clip(input, -100,100)\n",
    "        return np.exp(input)/(np.sum(np.exp(input)))\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative_softmax(input):\n",
    "        ''' args -> input :: the input value, a numpy array type to the derivative of the softmax function\n",
    "            return -> np.array :: the np array containing calculated derivative of softmax values (per input[i])\n",
    "        '''\n",
    "        return Functions.softmax(input)*(1-Functions.softmax(input))\n",
    "    \n",
    "    @staticmethod\n",
    "    def onehot(input):\n",
    "        \n",
    "        result = np.zeros(10)\n",
    "        result[input] = 1\n",
    "        return result\n",
    "    \n",
    "    @staticmethod\n",
    "    def crossEntropyLoss(y,yHat):\n",
    "        loss = (-1/10.0) * np.sum(np.multiply(y, np.log(yHat+1e-10)) + np.multiply((1 - y), np.log(1 - (yHat+1e-10))))\n",
    "        return loss\n",
    "    \n",
    "    @staticmethod\n",
    "    def mse(y,yHat):\n",
    "        return np.mean(np.dot((y - yHat).T, (y - yHat)))\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative_sigmoid(input):\n",
    "        ''' args -> input :: the input value, a numpy array type to the derivative of the sigmoid function\n",
    "            return -> np.array :: the np array containing calculated derivative of sigmoid values (per input[i])\n",
    "        '''\n",
    "        return Functions.sigmoid(input)*(1-Functions.sigmoid(input))\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative_tanh(input):\n",
    "        ''' args -> input :: the input value, a numpy array type to the derivative of the tanh function\n",
    "            return -> np.array :: the np array containing calculated derivative of tanh values (per input[i])\n",
    "        '''\n",
    "        return (1 - (np.tanh(input)**2))\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative_reLU(input):\n",
    "        ''' args -> input :: the input value, a numpy array type to the derivative of the reLU function\n",
    "            return -> np.array :: the np array containing calculated derivative of reLU values (per input[i])\n",
    "        '''\n",
    "        return np.where(input > 0, 1, 0.01)\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative_identity(input):\n",
    "        ''' args -> input :: the input value, a numpy array type to the derivative of the identity function\n",
    "            return -> np.array :: the np array containing calculated derivative of identity values (per input[i])\n",
    "        '''\n",
    "        return 1\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot(input):\n",
    "        ''' args -> input :: the loss list to be plotted\n",
    "            return -> null \n",
    "            Just show the matplotlib plots for the loss\n",
    "        '''\n",
    "        plt.plot(input)\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Loss over iterations\")\n",
    "        plt.show()\n",
    "    \n",
    "    @staticmethod\n",
    "    def plotAccuracy(input):\n",
    "        ''' args -> input :: the accuracy list to be plotted\n",
    "            return -> null \n",
    "            Just show the matplotlib plots for the accuracy\n",
    "        '''\n",
    "        plt.plot(input)\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"val accuracy\")\n",
    "        plt.title(\"Train over iterations\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "Bu5XtsgmjyaH"
   },
   "outputs": [],
   "source": [
    "class Algorithms:\n",
    "    ''' The Algorithms class/ libarary contains several functions and optimizers crucial for \n",
    "        the implementation of training and testing of the neural networks\n",
    "        \n",
    "        All these functions are static methods and therefore creation of an object instance\n",
    "        of algorithms is unnecessary\n",
    "    '''\n",
    "    @staticmethod\n",
    "    def ForwardProp(weights, bias, activate, output, inputLayer):\n",
    "        '''\n",
    "            args -> weights,biases :: The model on which loss is to be calculated\n",
    "            args -> activate :: The activation Function to be used\n",
    "            args -> output :: usually the softmax function\n",
    "            args -> inputLayer :: The image upon which to Forward Prop\n",
    "            \n",
    "            return -> a,h :: The preactivation and activation lists for every layer of the model.'''\n",
    "        \n",
    "        L = len(weights)-1\n",
    "        a = []\n",
    "        h = []\n",
    "        a.append(np.matmul(weights[0],inputLayer)+bias[0])\n",
    "        h.append(activate(a[0]))\n",
    "        for k in range(1,L):\n",
    "            a.append(np.matmul(weights[k],h[k-1].T)+bias[k])\n",
    "            h.append(activate(a[k]))\n",
    "        a.append(np.matmul(weights[L],h[L-1].T)+bias[L])\n",
    "        h.append(output(a[L]))\n",
    "        return a,h\n",
    "    \n",
    "    @staticmethod\n",
    "    def evaluateNetwork(weights, biases,activate, output, test_x, test_y):\n",
    "        ''' \n",
    "            args -> weights,biases :: The model on which loss is to be calculated\n",
    "            args -> activate :: The activation Function to be used\n",
    "            args -> output :: usually the softmax function\n",
    "            args -> inputLayer :: The image upon which to Forward Prop\n",
    "            \n",
    "            return -> double :: the accuracy of the model on the given images and labels split\n",
    "        '''\n",
    "        num_acc = 0\n",
    "        for i in range(len(test_x)):\n",
    "            a,h = Algorithms.ForwardProp(weights, biases, activate, output, test_x[i])\n",
    "            h = np.array(h, dtype = object)\n",
    "            predY =   np.argmax(h[len(h)-1])\n",
    "            if test_y[i] == predY:\n",
    "                num_acc+=1\n",
    "        return (num_acc/len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qHfSx0jHSqtk",
    "outputId": "5e2dae53-035e-468c-98d7-d29b08d821f1"
   },
   "outputs": [],
   "source": [
    "#The class of FeedForwardNeuralNetwor\n",
    "\n",
    "class FFNet:\n",
    "    ''' The neural Network class/library, has functions crucial to implementing the neural Network\n",
    "        constructor initializes the network to adapt to the input layer size and also initializaes the output layer size\n",
    "        \n",
    "    '''\n",
    "    #constructor\n",
    "    def __init__(self,number_of_hidden_layers, number_of_inputs, number_of_outputs):\n",
    "        self.number_of_inputs = number_of_inputs\n",
    "        self.number_of_hidden_layers = number_of_hidden_layers\n",
    "        self.number_of_outputs = number_of_outputs\n",
    "        self.input = [0 for i in range(number_of_inputs)]\n",
    "        self.output = [0 for i in range(10)]\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "    \n",
    "    #Method for creating layers\n",
    "    def addHiddenLayer(self,number_of_neurons, initialization):\n",
    "        ''' args -> number_of_neurons :: The number of neurons to be added for this layer of the network\n",
    "            args -> initialization :: The type of initialization used\n",
    "            \n",
    "            return -> null\n",
    "        '''\n",
    "        if(len(self.weights) == 0):\n",
    "            temp_weights = np.random.randn(number_of_neurons, self.number_of_inputs)*0.01\n",
    "            temp_biases = np.full((number_of_neurons), 0.01)\n",
    "            if initialization == \"xavier\":\n",
    "                temp_weights = np.random.randn(number_of_neurons, self.number_of_inputs)/np.sqrt((self.number_of_inputs)/2)\n",
    "                #temp_biases = np.random.randn(number_of_neurons)*np.sqrt(1/(number_of_neurons))\n",
    "        else:\n",
    "            prev_neurons = len(self.weights[-1])\n",
    "            temp_weights = np.random.randn(number_of_neurons, prev_neurons)*0.01\n",
    "            temp_biases = np.full((number_of_neurons), 0.01)\n",
    "            if initialization == \"xavier\":\n",
    "                temp_weights = np.random.randn(number_of_neurons, prev_neurons)/np.sqrt((prev_neurons)/2)\n",
    "                #temp_biases = np.random.randn(number_of_neurons)*np.sqrt(1/(number_of_neurons))\n",
    "\n",
    "        self.weights.append(temp_weights)\n",
    "        self.biases.append(temp_biases)\n",
    "    \n",
    "    def addOutputLayer(self, number_of_outputs, initialization):\n",
    "        ''' To add the output layer\n",
    "            args -> number_of_outputs :: The number of neurons in the output layer of the network\n",
    "            args -> initialization :: The type of initialization used for this network layer\n",
    "        '''\n",
    "        if(len(self.weights) == 0):\n",
    "            #print(\"number of inputs: \"+str(self.number_of_inputs))\n",
    "            temp_weights = np.random.randn(number_of_outputs, self.number_of_inputs)*0.01\n",
    "            temp_biases = np.full((number_of_outputs), 0.01)\n",
    "            if initialization == \"xavier\":\n",
    "                temp_weights = np.random.randn(number_of_outputs, self.number_of_inputs)/np.sqrt((prev_neurons)/2)\n",
    "        else:\n",
    "            prev_neurons = len(self.weights[-1])\n",
    "            temp_weights = np.random.randn(number_of_outputs, prev_neurons)*0.01\n",
    "            temp_biases = np.full((number_of_outputs), 0.01)\n",
    "            if initialization == \"xavier\":\n",
    "                temp_weights = np.random.randn(number_of_outputs, prev_neurons)/np.sqrt((prev_neurons)/2)\n",
    "                \n",
    "        \n",
    "        self.weights.append(temp_weights)\n",
    "        self.biases.append(temp_biases)\n",
    "\n",
    "    def solidify(self):\n",
    "        ''' convert the entire list into a numpy array'''\n",
    "        self.weights = np.array(self.weights, dtype = object)\n",
    "        self.biases = np.array(self.biases, dtype = object)\n",
    "\n",
    "    def getNetwork(self):\n",
    "        ''' returns the weights, biases of the network'''\n",
    "        return self.weights,self.biases\n",
    "    \n",
    "    def ForwardProp(self, activate, output, inputLayer):\n",
    "        ''' Forward Propagate the network on the given activation function, output function, and input layer'''\n",
    "        return Algorithms.ForwardProp(self.network, activate, output, inputLayer)\n",
    "    \n",
    "    def lossCalc(self, lossFunction, Y):\n",
    "        ''' calulate the loss fucntion'''\n",
    "        predY = self.historyA[(len(self.historyA)-1)]\n",
    "        return lossFunction(Y,self.predY)\n",
    "\n",
    "    def BackProp(self, a, h, dataPoint, dataLabel):\n",
    "        '''call the back propagation'''\n",
    "        return Algorithms.BackProp(self.network, a, h, dataPoint, dataLabel)\n",
    "    \n",
    "    def fit(self, optimizer, batchSize, learningRate, activation, trainx, train_y, decay, epochs, lossFunc):\n",
    "        ''' the fit method basically trains the model for the given configuration'''\n",
    "        #break data into training and validation\n",
    "        indices = np.arange(len(trainx))\n",
    "        np.random.shuffle(indices)\n",
    "        trainx = trainx[indices]\n",
    "        train_y = train_y[indices]\n",
    "        \n",
    "        valTest_x = trainx[int(0.9*len(trainx)):]\n",
    "        valTest_y = train_y[int(0.9*len(train_y)):]\n",
    "        \n",
    "        trainx = trainx[:int(0.9*len(trainx))]\n",
    "        train_y = train_y[:int(0.9*len(train_y))]\n",
    "        \n",
    "        ''' the selector if else blocks to choose the activation function and output function'''\n",
    "        if activation == \"relu\":\n",
    "            activate = Functions.reLU\n",
    "            derivative = Functions.derivative_reLU\n",
    "            output = Functions.softmax\n",
    "        elif activation == \"tanh\":\n",
    "            activate = Functions.tanh\n",
    "            derivative = Functions.derivative_tanh\n",
    "            output = Functions.softmax\n",
    "        elif activation == \"identity\":\n",
    "            activate = Functions.identity\n",
    "            derivative = Functions.derivative_identity\n",
    "            output = Functions.softmax\n",
    "        else:\n",
    "            activate = Functions.sigmoid\n",
    "            derivative = Functions.derivative_sigmoid\n",
    "            output = Functions.softmax\n",
    "        \n",
    "        #print(optimizer)\n",
    "        ''' The if else block for selecting the appropriate optimizer'''\n",
    "        if optimizer == \"momentum\":\n",
    "            self.weights, self.biases = Algorithms.miniBatchMGD(self.weights,self.biases , batchSize, learningRate, activate, output, derivative,  trainx, train_y, valTest_x, valTest_y, decay, epochs, lossFunc)\n",
    "        elif optimizer == \"nag\":\n",
    "            self.weights, self.biases = Algorithms.miniBatchNAG(self.weights,self.biases , batchSize, learningRate,activate, output, derivative , trainx, train_y, valTest_x, valTest_y, decay, epochs, lossFunc)\n",
    "        elif optimizer == \"rmsprop\":\n",
    "            self.weights, self.biases = Algorithms.RMSProp(self.weights,self.biases , batchSize, learningRate, activate, output, derivative,  trainx, train_y, valTest_x, valTest_y, decay, epochs, lossFunc)\n",
    "        elif optimizer == \"adam\":\n",
    "            self.weights, self.biases = Algorithms.ADAM(self.weights,self.biases , batchSize, learningRate,activate, output, derivative,  trainx, train_y, valTest_x, valTest_y, decay, epochs, lossFunc)\n",
    "        elif optimizer == \"nadam\":\n",
    "            self.weights, self.biases = Algorithms.NADAM(self.weights,self.biases , batchSize, learningRate, activate, output, derivative,  trainx, train_y, valTest_x, valTest_y, decay, epochs, lossFunc)\n",
    "        else:\n",
    "            self.weights, self.biases = Algorithms.miniBatchGD(self.weights,self.biases , batchSize, learningRate, activate, output, derivative , trainx, train_y, valTest_x, valTest_y, decay, epochs, lossFunc)\n",
    "        print(Algorithms.evaluateNetwork(self.weights, self.biases, activate, output, testx, test_y))       \n",
    "        \n",
    "        return self.weights,self.biases\n",
    "            \n",
    "    def evaluateNetwork(self, testx, tes_ty):\n",
    "        ''' To evaluate Network on the given images and labels set.'''\n",
    "        Algorithms.evaluateNetwork(self.weights, self.biases, testx, test_y)        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "wpFxAmhE9t2C",
    "outputId": "fdd02dd5-6147-4d7a-9d86-9cabebea2f81",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    #preprocessing of the data.\n",
    "    data = PreProc()\n",
    "    train_x, test_x = data.flattenAndCentralize()\n",
    "    trainx = train_x/255.0\n",
    "    testx = test_x/255.0\n",
    "    train_y, test_y = data.getLabels()\n",
    "    \n",
    "    #create the neural network\n",
    "    neuralNet = FFNet(0, len(trainx[0]), 10)\n",
    "    \n",
    "    #adding hidden layers\n",
    "    neuralNet.addHiddenLayer(128, \"xavier\")\n",
    "    neuralNet.addHiddenLayer(128, \"xavier\")\n",
    "    neuralNet.addHiddenLayer(128, \"xavier\")\n",
    "    neuralNet.addHiddenLayer(128, \"xavier\")\n",
    "    neuralNet.addHiddenLayer(128, \"xavier\")\n",
    "    neuralNet.addOutputLayer(10, \"xavier\")\n",
    "    \n",
    "    #adding output layer\n",
    "    neuralNet.solidify()\n",
    "    weights, biases = neuralNet.getNetwork()\n",
    "    prediction = []\n",
    "    \n",
    "    #Forward Prop\n",
    "    for image in trainx:\n",
    "        a,h = Algorithms.ForwardProp(weights, biases, Functions.sigmoid, Functions.softmax, image)\n",
    "        prediction.append(np.argmax(h[-1]))\n",
    "    prediction = np.array(prediction)\n",
    "    \n",
    "    #print predictions\n",
    "    print(prediction)\n",
    "    \n",
    "    #print accuracy on test\n",
    "    print(Algorithms.evaluateNetwork(weights, biases, Functions.sigmoid, Functions.softmax, testx, test_y))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
