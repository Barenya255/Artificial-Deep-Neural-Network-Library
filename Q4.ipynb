{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vlcWrCZGiPRM"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.datasets import fashion_mnist\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import math\n",
    "import cv2 as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    '''This function is used to log using wandb and train the model'''\n",
    "    \n",
    "    #initialie a new wandb configuraiton\n",
    "    wandb.init()\n",
    "    \n",
    "    #get the configuration\n",
    "    config = wandb.config\n",
    "    \n",
    "    #give the run a meaningful name\n",
    "    wandb.run.name = \"config_\"+ str(wandb.config.optimizer) + \"_\" + str(wandb.config.batchSize) + \"_\"+ str(wandb.config.learningRate) + \"_\" + str(wandb.config.activation) + \"_\"+str(wandb.config.decay) + \"_\" + str(config.epochs) + \"_\" + str(config.fc_layer_size)+str(wandb.config.initialization)\n",
    "    \n",
    "    #make the neural Net model according to specifications\n",
    "    neuralNet = FFNet(0, len(trainx[0]), 10)\n",
    "    for layer in range(config.number_of_layers):\n",
    "        neuralNet.addHiddenLayer(config.fc_layer_size, config.initialization)\n",
    "    neuralNet.addOutputLayer(10, config.initialization)\n",
    "    \n",
    "    #compile the model\n",
    "    neuralNet.solidify()\n",
    "    \n",
    "    #train the model\n",
    "    weights, biases = neuralNet.fit(config.optimizer,config.batchSize, config.learningRate, config.activation, trainx, train_y, config.decay, config.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "TfO2B6v6BVWj"
   },
   "outputs": [],
   "source": [
    "class PreProc:\n",
    "    '''Class used for preprocessing all images. \n",
    "        making a constructor of this class immediately loads in desired dataset\n",
    "        \n",
    "        visualize(n) logs into wandb 10 images each belonging to a separate class.\n",
    "        \n",
    "        flattenAndCentralize() makes the mean of the image arrays 0. This helps increasing the \n",
    "        training accuracy quicker per epoch\n",
    "        \n",
    "        getLabels() return labels in corresponding index fashion\n",
    "        \n",
    "        getInputsize returns the number of images present in the training sample\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        (self.trainx,self.trainy),(self.testx, self.testy) = fashion_mnist.load_data()\n",
    "        \n",
    "    def visualize(self,n):\n",
    "        ''' args -> n :: The number of images desired to be visualized\n",
    "            returns-> null\n",
    "            \n",
    "            shows the images via matplotlib\n",
    "        '''\n",
    "        for i in range(n):\n",
    "            plt.subplot(330+1+i) # ask someone why??\n",
    "            plt.imshow(self.trainx[i], cmap = plt.get_cmap('gray'))\n",
    "        plt.show()\n",
    "            \n",
    "        \n",
    "    def flattenAndCentralize(self):\n",
    "        ''' args -> none\n",
    "            returns -> trainx_flattened :: The training images, mean centered and flattened\n",
    "                        into a 1 dimensional array\n",
    "                    -> testx_flattened :: The testing images, mean centered and flattened\n",
    "                        into a 1 dimensional array\n",
    "        '''\n",
    "        trainx_flattened = np.copy(self.trainx).astype('float64')\n",
    "        testx_flattened = np.copy(self.testx).astype('float64')\n",
    "        trainx_flattened -= np.mean(trainx_flattened, axis = 0)\n",
    "        testx_flattened -= np.mean(testx_flattened, axis = 0)\n",
    "        for image in trainx_flattened:\n",
    "            image = cv.GaussianBlur(image,(3,3),cv.BORDER_DEFAULT)\n",
    "        trainx_flattened.shape = (len(trainx_flattened),784)\n",
    "        testx_flattened.shape = (10000,784)\n",
    "        return trainx_flattened,testx_flattened\n",
    "    \n",
    "\n",
    "    \n",
    "    def getLabels(self):\n",
    "        ''' args -> none\n",
    "            returns -> self.trainy :: The labels of the training data\n",
    "                    -> self.testy :: The labels of the testing data\n",
    "        '''\n",
    "        return self.trainy, self.testy\n",
    "    \n",
    "    def getInputSize(self):\n",
    "        return len(self.trainx[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4HhzRi0xhOLc"
   },
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "7BwoVy0bPFA0"
   },
   "outputs": [],
   "source": [
    "class Functions:\n",
    "    ''' The Functions class/ Library stores static methods corresponding to all the functions\n",
    "        To be used in the program/training/testing.\n",
    "        The correct implementation of these is vital to the correct working of the neural net\n",
    "        model\n",
    "    '''\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(input):\n",
    "        ''' args -> input :: the input value, a numpy array type to the sigmoid function\n",
    "            return -> np.array :: the np array containing calculated sigmoid values (per input[i])\n",
    "        '''\n",
    "        input = np.clip(input, -100,100)\n",
    "        return  1.0/(1.0+np.exp(-input))\n",
    "    \n",
    "    @staticmethod\n",
    "    def reLU(input):\n",
    "        ''' args -> input :: the input value, a numpy array type to the reLU function\n",
    "            return -> np.array :: the np array containing calculated relu values (per input[i])\n",
    "        '''\n",
    "        return np.maximum(np.random.randint(low=1, high=10)*1e-5, input)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tanh(input):\n",
    "        ''' args -> input :: the input value, a numpy array type to the tanh function\n",
    "            return -> np.array :: the np array containing calculated tanh values (per input[i])\n",
    "        '''\n",
    "        return np.tanh(input)\n",
    "    \n",
    "    @staticmethod\n",
    "    def identity(input):\n",
    "        ''' args -> input :: the input value, a numpy array type to the identity function\n",
    "            return -> np.array :: the np array containing calculated same values (per input[i])\n",
    "        '''\n",
    "        return input\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax(input):\n",
    "        ''' args -> input :: the input value, a numpy array type to the softmax function\n",
    "            return -> np.array :: the np array containing calculated softmax values (per input[i])\n",
    "        '''\n",
    "        input = np.clip(input, -100,100)\n",
    "        return np.exp(input)/(np.sum(np.exp(input)))\n",
    "    \n",
    "    @staticmethod\n",
    "    def onehot(input):\n",
    "        ''' args -> input :: the input value, a numpy array type to the derivative of the softmax function\n",
    "            return -> np.array :: the np array containing calculated derivative of softmax values (per input[i])\n",
    "        '''\n",
    "        result = np.zeros(10)\n",
    "        result[input] = 1\n",
    "        return result\n",
    "    \n",
    "    @staticmethod\n",
    "    def crossEntropyLoss(y,yHat):\n",
    "        loss = (-1/10.0) * np.sum(np.multiply(y, np.log(yHat+1e-10)) + np.multiply((1 - y), np.log(1 - (yHat+1e-10))))\n",
    "        return loss\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative_sigmoid(input):\n",
    "        ''' args -> input :: the input value, a numpy array type to the derivative of the sigmoid function\n",
    "            return -> np.array :: the np array containing calculated derivative of sigmoid values (per input[i])\n",
    "        '''\n",
    "        return Functions.sigmoid(input)*(1-Functions.sigmoid(input))\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative_tanh(input):\n",
    "        ''' args -> input :: the input value, a numpy array type to the derivative of the tanh function\n",
    "            return -> np.array :: the np array containing calculated derivative of tanh values (per input[i])\n",
    "        '''\n",
    "        return (1 - (np.tanh(input)**2))\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative_reLU(input):\n",
    "        ''' args -> input :: the input value, a numpy array type to the derivative of the reLU function\n",
    "            return -> np.array :: the np array containing calculated derivative of reLU values (per input[i])\n",
    "        '''\n",
    "        return np.where(input > 0, 1, 0)\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative_identity(input):\n",
    "        ''' args -> input :: the input value, a numpy array type to the derivative of the identity function\n",
    "            return -> np.array :: the np array containing calculated derivative of identity values (per input[i])\n",
    "        '''\n",
    "        return 1\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot(input):\n",
    "        ''' args -> input :: the loss list to be plotted\n",
    "            return -> null \n",
    "            Just show the matplotlib plots for the loss\n",
    "        '''\n",
    "        plt.plot(input)\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Loss over iterations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Bu5XtsgmjyaH"
   },
   "outputs": [],
   "source": [
    "class Algorithms:\n",
    "    ''' The Algorithms class/ libarary contains several functions and optimizers crucial for \n",
    "        the implementation of training and testing of the neural networks\n",
    "        \n",
    "        All these functions are static methods and therefore creation of an object instance\n",
    "        of algorithms is unnecessary\n",
    "    '''\n",
    "    \n",
    "    @staticmethod\n",
    "    def uniLoss(weights, biases, activate, output, t1, t2):\n",
    "        '''\n",
    "            args -> weights,biases :: The model on which loss is to be calculated\n",
    "            args -> activate :: The activation Function to be used\n",
    "            args -> t1,t2 :: images and labels respectively\n",
    "            args -> lossFunc :: which loss function to be used for the purpose of evaluation\n",
    "            \n",
    "            return -> float :: the loss calculated on the given data by the model.\n",
    "        '''\n",
    "        Loss = 0.0\n",
    "        for index in range(len(t1)):\n",
    "            a,h = Algorithms.ForwardProp(weights, biases, activate, output, t1[index])\n",
    "            pred = h[-1]\n",
    "            true = t2[index]\n",
    "            Loss += Functions.crossEntropyLoss(Functions.onehot(true), pred)\n",
    "        return (Loss/len(t1))\n",
    "    \n",
    "    @staticmethod\n",
    "    def ForwardProp(weights, bias, activate, output, inputLayer):\n",
    "        '''\n",
    "            args -> weights,biases :: The model on which loss is to be calculated\n",
    "            args -> activate :: The activation Function to be used\n",
    "            args -> output :: usually the softmax function\n",
    "            args -> inputLayer :: The image upon which to Forward Prop\n",
    "            \n",
    "            return -> a,h :: The preactivation and activation lists for every layer of the model.\n",
    "        '''\n",
    "        L = len(weights)-1\n",
    "        a = []\n",
    "        h = []\n",
    "        a.append(np.matmul(weights[0],inputLayer)+bias[0])\n",
    "        h.append(activate(a[0]))\n",
    "        for k in range(1,L):\n",
    "            a.append(np.matmul(weights[k],h[k-1].T)+bias[k])\n",
    "            h.append(activate(a[k]))\n",
    "        a.append(np.matmul(weights[L],h[L-1].T)+bias[L])\n",
    "        h.append(output(a[L]))\n",
    "        return a,h\n",
    "    @staticmethod\n",
    "    def BackProp(weights, biases, a, h, derivative, dataPoint, dataLabel):\n",
    "        '''\n",
    "            args -> weights,biases :: The model on which loss is to be calculated\n",
    "            args -> a,h :: The preactivation and activation lists for every layer of the model.\n",
    "            args -> derivative :: the derivative of the function used in corresponding Forward Prop\n",
    "            args -> dataPoint :: The image upon which to BackwardProp\n",
    "            args -> dataLabel :: The label of the corresponding image\n",
    "            args -> lossFunc :: The loss function on which to operate on\n",
    "            \n",
    "            return -> weights,biases :: The updated model\n",
    "        '''\n",
    "        L = len(weights)-1\n",
    "        gradaL = -(Functions.onehot(dataLabel)-h[len(h)-1])\n",
    "        dw = np.zeros_like(weights)\n",
    "        db = np.zeros_like(biases)\n",
    "        for k in range(L,0,-1):\n",
    "            gradW = np.outer(gradaL, h[k-1].T)\n",
    "            gradB = gradaL\n",
    "            dw[k] = gradW\n",
    "            db[k] = gradB\n",
    "\n",
    "            gradhL_1 = np.matmul(np.transpose(weights[k]),gradaL)\n",
    "            gradaL_1 = np.multiply(gradhL_1, derivative(a[k-1]))\n",
    "            gradaL = gradaL_1\n",
    "        dw[0] = np.outer(gradaL,dataPoint.T)\n",
    "        db[0] = gradaL\n",
    "        return dw, db\n",
    "\n",
    "    @staticmethod\n",
    "    def miniBatchMGD(weights, biases, batchSize, learningRate, activate, output, derivative, dataPoints, dataLabels, valTest_x, valTest_y, decay, epochs):\n",
    "        '''\n",
    "            args -> weights,biases :: The model on which loss is to be calculated\n",
    "            args -> a,h :: The preactivation and activation lists for every layer of the model.\n",
    "            args -> activate :: The activation Function to be used\n",
    "            args -> output :: usually the softmax function\n",
    "            args -> derivative :: the derivative of the function used in corresponding Forward Prop\n",
    "            args -> dataPoint :: The image upon which to BackwardProp\n",
    "            args -> dataLabel :: The label of the corresponding image\n",
    "            args -> lossFunc :: The loss function on which to operate on\n",
    "            args -> valTestx :: The validation image set\n",
    "            args -> valTesty :: The validation labels\n",
    "            args -> batchSize :: batch size \n",
    "            args -> learningRate :: the lower the better, but too low and you have no training at all.\n",
    "            args -> decay :: the amount of L2 regularization (small is good)\n",
    "            args -> epochs :: The number of epochs for which this will run \n",
    "            return -> weights,biases :: The updated model\n",
    "        '''\n",
    "        validation = []\n",
    "        validationLoss = []\n",
    "        trainAccuracy = []\n",
    "        loss = []\n",
    "        lossTrack = []\n",
    "        LOSS = 0.0\n",
    "        beta = 0.9\n",
    "        prevWeights = np.zeros_like(weights)\n",
    "        prevBiases = np.zeros_like(biases)\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            indices = np.arange(len(dataPoints))\n",
    "            np.random.shuffle(indices)\n",
    "            batchX = dataPoints[indices]\n",
    "            batchY = dataLabels[indices]\n",
    "            Loss = 0.0\n",
    "            for i in range(math.ceil(len(dataPoints)/batchSize)):\n",
    "                if (i+1)*batchSize < len(dataPoints):\n",
    "                    trainer = batchX[i*batchSize:i*batchSize+batchSize]\n",
    "                    labeler = batchY[i*batchSize:i*batchSize+batchSize]\n",
    "                else:\n",
    "                    trainer = batchX[i*batchSize:len(dataPoints)]\n",
    "                    labeler = batchY[i*batchSize:len(dataPoints)]\n",
    "                batchLoss = 0.0\n",
    "                dw = np.zeros_like(weights)\n",
    "                db = np.zeros_like(biases)\n",
    "                for data in range(len(trainer)):\n",
    "                    a,h = Algorithms.ForwardProp(weights, biases , activate, output, trainer[data])\n",
    "                    currWeights, currBiases = Algorithms.BackProp(weights, biases, a, h, derivative, trainer[data], labeler[data])\n",
    "                    batchLoss += Functions.crossEntropyLoss(Functions.onehot(labeler[data]), h[-1])\n",
    "                    dw += currWeights\n",
    "                    db += currBiases\n",
    "                Loss += batchLoss\n",
    "                batchLoss /= batchSize\n",
    "                dw /= batchSize\n",
    "                db /= batchSize\n",
    "                momentumWeights = prevWeights*beta + dw*1.0\n",
    "                momentumBiases = prevBiases*beta + db*1.0\n",
    "                weights -= learningRate*(momentumWeights + decay*weights)\n",
    "                biases -= learningRate*(momentumBiases + decay*biases)\n",
    "                prevWeights = momentumWeights\n",
    "                prevBiases = momentumBiases\n",
    "                lossTrack.append(batchLoss)\n",
    "            Loss /= len(dataPoints)\n",
    "            loss.append(Loss)\n",
    "            validation.append(Algorithms.evaluateNetwork(weights, biases, activate, output, valTest_x, valTest_y))\n",
    "            validationLoss.append((Algorithms.uniLoss(weights, biases, activate, output, valTest_x, valTest_y)))\n",
    "            trainAccuracy.append(Algorithms.evaluateNetwork(weights, biases, activate, output, trainx, train_y))\n",
    "            wandb.log({\"valAcc\":validation[-1], \"epoch\":epoch, \"loss\":lossTrack[-1], \"valLoss\":validationLoss[-1], \"train Accuracy\":trainAccuracy[-1]})\n",
    "        return weights, biases\n",
    "\n",
    "    @staticmethod\n",
    "    def ADAM(weights, biases, batchSize, learningRate, activate, output, derivative, dataPoints, dataLabels, valTest_x, valTest_y, decay, epochs):\n",
    "        '''\n",
    "            args -> weights,biases :: The model on which loss is to be calculated\n",
    "            args -> a,h :: The preactivation and activation lists for every layer of the model.\n",
    "            args -> activate :: The activation Function to be used\n",
    "            args -> output :: usually the softmax function\n",
    "            args -> derivative :: the derivative of the function used in corresponding Forward Prop\n",
    "            args -> dataPoint :: The image upon which to BackwardProp\n",
    "            args -> dataLabel :: The label of the corresponding image\n",
    "            args -> lossFunc :: The loss function on which to operate on\n",
    "            args -> valTestx :: The validation image set\n",
    "            args -> valTesty :: The validation labels\n",
    "            args -> batchSize :: batch size \n",
    "            args -> learningRate :: the lower the better, but too low and you have no training at all.\n",
    "            args -> decay :: the amount of L2 regularization (small is good)\n",
    "            args -> epochs :: The number of epochs for which this will run \n",
    "            \n",
    "            updated :: \n",
    "            args -> beta1 :: value for momentum calculation\n",
    "            args -> beta2 :: value for velocity calculation\n",
    "            args -> epsilon :: small noise to avoid nans and other runtime errors/warnings\n",
    "            \n",
    "            return -> weights,biases :: The updated model\n",
    "        '''\n",
    "        validation = []\n",
    "        validationLoss = []\n",
    "        trainAccuracy = []\n",
    "        loss = []\n",
    "        lossTrack = []\n",
    "        prev_val = 0.0\n",
    "        beta1 = 0.9\n",
    "        beta2 = 0.99\n",
    "        epsilon = 0.000001\n",
    "        patience = 3\n",
    "        m_w = np.zeros_like(weights)\n",
    "        v_w = np.zeros_like(weights)\n",
    "        m_b = np.zeros_like(biases)\n",
    "        v_b = np.zeros_like(biases)\n",
    "        learnerRateW = np.full_like(weights, learningRate)\n",
    "        learnerRateB = np.full_like(biases, learningRate)\n",
    "        i = 0\n",
    "        \n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            indices = np.arange(len(dataPoints))\n",
    "            np.random.shuffle(indices)\n",
    "            batchX = dataPoints[indices]\n",
    "            batchY = dataLabels[indices]\n",
    "            Loss = 0.0\n",
    "            for i in range(math.ceil(len(dataPoints)/batchSize)):\n",
    "                if (i+1)*batchSize < len(dataPoints):\n",
    "                    trainer = batchX[i*batchSize:i*batchSize+batchSize]\n",
    "                    labeler = batchY[i*batchSize:i*batchSize+batchSize]\n",
    "                else:\n",
    "                    trainer = batchX[i*batchSize:len(dataPoints)]\n",
    "                    labeler = batchY[i*batchSize:len(dataPoints)]\n",
    "                batchLoss = 0.0\n",
    "                dw = np.zeros_like(weights)\n",
    "                db = np.zeros_like(biases)\n",
    "                for data in range(len(trainer)):\n",
    "                    a,h = Algorithms.ForwardProp(weights, biases,activate, output, trainer[data])\n",
    "                    currWeights, currBiases = Algorithms.BackProp(weights, biases , a, h, derivative, trainer[data], labeler[data])\n",
    "                    batchLoss += Functions.crossEntropyLoss(Functions.onehot(labeler[data]), h[-1])\n",
    "                    \n",
    "                    dw += currWeights\n",
    "                    db += currBiases\n",
    "                Loss += batchLoss\n",
    "                batchLoss /= batchSize\n",
    "                dw /= batchSize\n",
    "                db /= batchSize\n",
    "                m_w = beta1*m_w + (1-beta1)*dw\n",
    "                m_b = beta1*m_b + (1-beta1)*db\n",
    "                v_w = v_w*beta2 + (1-beta2)*dw**2\n",
    "                v_b = v_b*beta2 + (1-beta2)*db**2\n",
    "                \n",
    "                m_w_hat = m_w/(1 - np.power(beta1, i+1))\n",
    "                m_b_hat = m_b/(1 - np.power(beta1, i+1))\n",
    "                v_w_hat = v_w/(1 - np.power(beta2, i+1))\n",
    "                v_b_hat = v_b/(1 - np.power(beta2, i+1))\n",
    "                \n",
    "                i+=1\n",
    "                \n",
    "                tempW = np.zeros_like(m_w)\n",
    "                tempB = np.zeros_like(m_b)\n",
    "                for i in range(len(dw)):\n",
    "                    tempW[i] = np.sqrt(v_w_hat[i])\n",
    "                    tempB[i] = np.sqrt(v_b_hat[i])\n",
    "                weights = weights - ((learnerRateW*(dw + decay*weights))/(tempW + epsilon))\n",
    "                biases = biases - (learnerRateB*(db + decay*biases))/(tempB+epsilon)\n",
    "\n",
    "                lossTrack.append(batchLoss)\n",
    "                \n",
    "            Loss /= len(dataPoints)\n",
    "            loss.append(Loss)\n",
    "            validation.append(Algorithms.evaluateNetwork(weights, biases, activate, output, valTest_x, valTest_y))\n",
    "            validationLoss.append((Algorithms.uniLoss(weights, biases, activate, output, valTest_x, valTest_y)))\n",
    "            trainAccuracy.append(Algorithms.evaluateNetwork(weights, biases, activate, output, trainx, train_y))\n",
    "            wandb.log({\"valAcc\":validation[-1], \"epoch\":epoch, \"loss\":loss[-1], \"valLoss\": validationLoss[-1], \"train Accuracy\":trainAccuracy[-1]})\n",
    "                \n",
    "            if validation[-1] <= prev_val+1e-2:\n",
    "                patience -= 1\n",
    "            else:\n",
    "                patience = 3\n",
    "                prev_weights = weights\n",
    "                prev_biases = biases\n",
    "                prev_val = validation[-1]\n",
    "            \n",
    "            if patience == 0:\n",
    "                weights = prev_weights\n",
    "                biases = prev_biases\n",
    "            Loss /= len(dataPoints)\n",
    "            loss.append(Loss)\n",
    "            \n",
    "        if prev_val > validation[-1]:\n",
    "            return prev_weights, prev_biases\n",
    "            \n",
    "        return weights,biases\n",
    "\n",
    "    @staticmethod\n",
    "    def NADAM(weights, biases, batchSize, learningRate, activate, output, derivative, dataPoints, dataLabels, valTest_x, valTest_y, decay, epochs):\n",
    "        '''\n",
    "            args -> weights,biases :: The model on which loss is to be calculated\n",
    "            args -> a,h :: The preactivation and activation lists for every layer of the model.\n",
    "            args -> activate :: The activation Function to be used\n",
    "            args -> output :: usually the softmax function\n",
    "            args -> derivative :: the derivative of the function used in corresponding Forward Prop\n",
    "            args -> dataPoint :: The image upon which to BackwardProp\n",
    "            args -> dataLabel :: The label of the corresponding image\n",
    "            args -> lossFunc :: The loss function on which to operate on\n",
    "            args -> valTestx :: The validation image set\n",
    "            args -> valTesty :: The validation labels\n",
    "            args -> batchSize :: batch size \n",
    "            args -> learningRate :: the lower the better, but too low and you have no training at all.\n",
    "            args -> decay :: the amount of L2 regularization (small is good)\n",
    "            args -> epochs :: The number of epochs for which this will run \n",
    "            \n",
    "            updated :: \n",
    "            args -> beta1 :: value for momentum calculation\n",
    "            args -> beta2 :: value for velocity calculation\n",
    "            args -> epsilon :: small noise to avoid nans and other runtime errors/warnings\n",
    "            \n",
    "            return -> weights,biases :: The updated model\n",
    "        '''\n",
    "        patience = 3\n",
    "        validation = []\n",
    "        validationLoss = []\n",
    "        trainAccuracy = []\n",
    "        loss = []\n",
    "        lossTrack = []\n",
    "        prev_val = 0.0\n",
    "        prevWeights = np.zeros_like(weights)\n",
    "        prevBiases = np.zeros_like(biases)\n",
    "        beta1 = 0.9\n",
    "        beta2 = 0.99\n",
    "        epsilon = 0.000001\n",
    "        prev_weights = np.zeros_like(weights)\n",
    "        prev_biases = np.zeros_like(biases)\n",
    "        m_w = np.zeros_like(weights)\n",
    "        v_w = np.zeros_like(weights)\n",
    "        m_b = np.zeros_like(biases)\n",
    "        v_b = np.zeros_like(biases)\n",
    "        learnerRateW = np.full_like(weights, learningRate)\n",
    "        learnerRateB = np.full_like(biases, learningRate)\n",
    "        i = 0\n",
    "        \n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            indices = np.arange(len(dataPoints))\n",
    "            np.random.shuffle(indices)\n",
    "            batchX = dataPoints[indices]\n",
    "            batchY = dataLabels[indices]\n",
    "            Loss = 0.0\n",
    "            for i in range(math.ceil(len(dataPoints)/batchSize)):\n",
    "                if (i+1)*batchSize < len(dataPoints):\n",
    "                    trainer = batchX[i*batchSize:i*batchSize+batchSize]\n",
    "                    labeler = batchY[i*batchSize:i*batchSize+batchSize]\n",
    "                else:\n",
    "                    trainer = batchX[i*batchSize:len(dataPoints)]\n",
    "                    labeler = batchY[i*batchSize:len(dataPoints)]\n",
    "                batchLoss = 0.0\n",
    "                dw = np.zeros_like(weights)\n",
    "                db = np.zeros_like(biases)\n",
    "                for data in range(len(trainer)):\n",
    "                    a,h = Algorithms.ForwardProp(weights - v_w*(beta1), biases - v_b*(beta1), activate, output, trainer[data])\n",
    "                    currWeights, currBiases = Algorithms.BackProp(weights - v_w*(beta1), biases - v_b*(beta1), a, h, derivative, trainer[data], labeler[data])\n",
    "                    batchLoss += Functions.crossEntropyLoss(Functions.onehot(labeler[data]), h[-1])\n",
    "                    \n",
    "                    dw += currWeights\n",
    "                    db += currBiases\n",
    "                Loss += batchLoss\n",
    "                batchLoss /= batchSize\n",
    "                dw /= batchSize\n",
    "                db /= batchSize\n",
    "                m_w = beta1*m_w + (1-beta1)*dw\n",
    "                m_b = beta1*m_b + (1-beta1)*db\n",
    "                v_w = v_w*beta2 + (1-beta2)*dw**2\n",
    "                v_b = v_b*beta2 + (1-beta2)*db**2\n",
    "                \n",
    "                m_w_hat = m_w/(1 - np.power(beta1, i+1))\n",
    "                m_b_hat = m_b/(1 - np.power(beta1, i+1))\n",
    "                v_w_hat = v_w/(1 - np.power(beta2, i+1))\n",
    "                v_b_hat = v_b/(1 - np.power(beta2, i+1))\n",
    "                \n",
    "                i+=1\n",
    "                \n",
    "                tempW = np.zeros_like(m_w)\n",
    "                tempB = np.zeros_like(m_b)\n",
    "                for j in range(len(dw)):\n",
    "                    tempW[j] = np.sqrt(v_w_hat[j])\n",
    "                    tempB[j] = np.sqrt(v_b_hat[j])\n",
    "                weights = weights - ((learnerRateW*(dw + decay*weights))/(tempW + epsilon))\n",
    "                biases = biases - (learnerRateB*(db + decay*biases))/(tempB+epsilon)\n",
    "                lossTrack.append(batchLoss)\n",
    "            #wandb.log({'valAcc':(Algorithms.evaluateNetwork(weights, biases, valTest_x, valTest_y)), 'epoch':epoch})\n",
    "            #validation.append(Algorithms.evaluateNetwork(weights, biases, activate, output, valTest_x, valTest_y))\n",
    "            Loss /= len(dataPoints)\n",
    "            loss.append(Loss)\n",
    "            validation.append(Algorithms.evaluateNetwork(weights, biases, activate, output, valTest_x, valTest_y))\n",
    "            validationLoss.append((Algorithms.uniLoss(weights, biases, activate, output, valTest_x, valTest_y)))\n",
    "            trainAccuracy.append(Algorithms.evaluateNetwork(weights, biases, activate, output, trainx, train_y))\n",
    "            wandb.log({\"valAcc\":validation[-1], \"epoch\":epoch, \"loss\":loss[-1], \"valLoss\": validationLoss[-1], \"train Accuracy\":trainAccuracy[-1]})\n",
    "            \n",
    "            if validation[-1] <= prev_val:\n",
    "                patience -= 1\n",
    "            else:\n",
    "                patience = 3\n",
    "                prev_weights = weights\n",
    "                prev_biases = biases\n",
    "                prev_val = validation[-1]\n",
    "            \n",
    "            if patience <= 0:\n",
    "                weights = prev_weights\n",
    "                biases = prev_biases\n",
    "                #learningRate /= 2\n",
    "            \n",
    "            \n",
    "        return prev_weights, prev_biases\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def miniBatchNAG(weights, biases, batchSize, learningRate, activate, output, derivative, dataPoints, dataLabels, valTest_x, valTest_y, decay, epochs):\n",
    "        '''\n",
    "            args -> weights,biases :: The model on which loss is to be calculated\n",
    "            args -> a,h :: The preactivation and activation lists for every layer of the model.\n",
    "            args -> activate :: The activation Function to be used\n",
    "            args -> output :: usually the softmax function\n",
    "            args -> derivative :: the derivative of the function used in corresponding Forward Prop\n",
    "            args -> dataPoint :: The image upon which to BackwardProp\n",
    "            args -> dataLabel :: The label of the corresponding image\n",
    "            args -> lossFunc :: The loss function on which to operate on\n",
    "            args -> valTestx :: The validation image set\n",
    "            args -> valTesty :: The validation labels\n",
    "            args -> batchSize :: batch size \n",
    "            args -> learningRate :: the lower the better, but too low and you have no training at all.\n",
    "            args -> decay :: the amount of L2 regularization (small is good)\n",
    "            args -> epochs :: The number of epochs for which this will run \n",
    "            return -> weights,biases :: The updated model\n",
    "        '''\n",
    "        validation = []\n",
    "        validationLoss = []\n",
    "        trainAccuracy = []\n",
    "        loss = []\n",
    "        lossTrack = []\n",
    "        beta = 0.9\n",
    "        prevWeights = np.zeros_like(weights)\n",
    "        prevBiases = np.zeros_like(biases)\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            indices = np.arange(len(dataPoints))\n",
    "            np.random.shuffle(indices)\n",
    "            batchX = dataPoints[indices]\n",
    "            batchY = dataLabels[indices]\n",
    "            Loss = 0.0\n",
    "            for i in range(math.ceil(len(dataPoints)/batchSize)):\n",
    "                if (i+1)*batchSize < len(dataPoints):\n",
    "                    trainer = batchX[i*batchSize:i*batchSize+batchSize]\n",
    "                    labeler = batchY[i*batchSize:i*batchSize+batchSize]\n",
    "                else:\n",
    "                    trainer = batchX[i*batchSize:len(dataPoints)]\n",
    "                    labeler = batchY[i*batchSize:len(dataPoints)]\n",
    "                batchLoss = 0.0\n",
    "                tempWeights = np.zeros_like(weights)\n",
    "                tempBiases = np.zeros_like(biases)\n",
    "                for data in range(len(trainer)):\n",
    "                    a,h = Algorithms.ForwardProp(weights-prevWeights, biases-prevBiases, activate, output, trainer[data])\n",
    "                    currWeights, currBiases = Algorithms.BackProp(weights-prevWeights, biases-prevBiases, a, h, derivative, trainer[data], labeler[data])\n",
    "                    batchLoss += Functions.crossEntropyLoss(Functions.onehot(labeler[data]), h[-1])\n",
    "                    tempWeights += currWeights\n",
    "                    tempBiases += currBiases\n",
    "                Loss += batchLoss\n",
    "                batchLoss /= batchSize\n",
    "                tempWeights /= batchSize\n",
    "                tempBiases /= batchSize\n",
    "                momentumWeights = beta*prevWeights + tempWeights*1.0\n",
    "                momentumBiases = beta*prevBiases + tempBiases*1.0\n",
    "                weights = weights - learningRate*(momentumWeights + decay*weights) \n",
    "                biases = biases - learningRate*(momentumBiases + decay*biases)\n",
    "                prevWeights = momentumWeights\n",
    "                prevBiases = momentumBiases\n",
    "                lossTrack.append(batchLoss)\n",
    "            Loss /= len(dataPoints)\n",
    "            loss.append(Loss)\n",
    "            validation.append(Algorithms.evaluateNetwork(weights, biases, activate, output, valTest_x, valTest_y))\n",
    "            validationLoss.append((Algorithms.uniLoss(weights, biases, activate, output, valTest_x, valTest_y)))\n",
    "            trainAccuracy.append(Algorithms.evaluateNetwork(weights, biases, activate, output, trainx, train_y))\n",
    "            wandb.log({\"valAcc\":validation[-1], \"epoch\":epoch, \"loss\":loss[-1], \"valLoss\": validationLoss[-1], \"train Accuracy\":trainAccuracy[-1]})\n",
    "            \n",
    "        return weights, biases\n",
    "    \n",
    "    @staticmethod\n",
    "    def RMSProp(weights, biases, batchSize, learningRate, activate, output, derivative, dataPoints, dataLabels, valTest_x, valTest_y, decay, epochs):\n",
    "        '''\n",
    "            args -> weights,biases :: The model on which loss is to be calculated\n",
    "            args -> a,h :: The preactivation and activation lists for every layer of the model.\n",
    "            args -> activate :: The activation Function to be used\n",
    "            args -> output :: usually the softmax function\n",
    "            args -> derivative :: the derivative of the function used in corresponding Forward Prop\n",
    "            args -> dataPoint :: The image upon which to BackwardProp\n",
    "            args -> dataLabel :: The label of the corresponding image\n",
    "            args -> lossFunc :: The loss function on which to operate on\n",
    "            args -> valTestx :: The validation image set\n",
    "            args -> valTesty :: The validation labels\n",
    "            args -> batchSize :: batch size \n",
    "            args -> learningRate :: the lower the better, but too low and you have no training at all.\n",
    "            args -> decay :: the amount of L2 regularization (small is good)\n",
    "            args -> epochs :: The number of epochs for which this will run \n",
    "            return -> weights,biases :: The updated model\n",
    "        '''\n",
    "        validation = []\n",
    "        validationLoss = []\n",
    "        trainAccuracy = []\n",
    "        loss = []\n",
    "        lossTrack = []\n",
    "        beta = 0.5\n",
    "        epsilon = 0.000001\n",
    "        momentumWeights = np.zeros_like(weights)\n",
    "        momentumBiases = np.zeros_like(biases)\n",
    "        learnerRateW = np.full_like(weights, learningRate)\n",
    "        learnerRateB = np.full_like(biases, learningRate)\n",
    "        \n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            indices = np.arange(len(dataPoints))\n",
    "            np.random.shuffle(indices)\n",
    "            batchX = dataPoints[indices]\n",
    "            batchY = dataLabels[indices]\n",
    "            Loss = 0.0\n",
    "            for i in range(math.ceil(len(dataPoints)/batchSize)):\n",
    "                if (i+1)*batchSize < len(dataPoints):\n",
    "                    trainer = batchX[i*batchSize:i*batchSize+batchSize]\n",
    "                    labeler = batchY[i*batchSize:i*batchSize+batchSize]\n",
    "                else:\n",
    "                    trainer = batchX[i*batchSize:len(dataPoints)]\n",
    "                    labeler = batchY[i*batchSize:len(dataPoints)]\n",
    "                batchLoss = 0.0\n",
    "                dw = np.zeros_like(weights)\n",
    "                db = np.zeros_like(biases)\n",
    "                for data in range(len(trainer)):\n",
    "                    a,h = Algorithms.ForwardProp(weights, biases, activate, output, trainer[data])\n",
    "                    currWeights, currBiases = Algorithms.BackProp(weights, biases , a, h, derivative, trainer[data], labeler[data])\n",
    "                    batchLoss += Functions.crossEntropyLoss(Functions.onehot(labeler[data]), h[-1])\n",
    "                    dw += currWeights\n",
    "                    db += currBiases\n",
    "                Loss += batchLoss\n",
    "                batchLoss /= batchSize\n",
    "                dw /= batchSize\n",
    "                db /= batchSize\n",
    "                momentumWeights = momentumWeights*beta + (1-beta)*dw**2\n",
    "                momentumBiases = momentumBiases*beta + (1-beta)*db**2\n",
    "                tempW = np.zeros_like(momentumWeights)\n",
    "                tempB = np.zeros_like(momentumBiases)\n",
    "                for i in range(len(dw)):\n",
    "                    tempW[i] = np.sqrt(momentumWeights[i])\n",
    "                    tempB[i] = np.sqrt(momentumBiases[i])\n",
    "                weights = weights - ((learnerRateW)*(dw + decay*weights)/(tempW + epsilon))\n",
    "                biases = biases - (learnerRateB*(db + decay*biases))/(tempB+epsilon)\n",
    "                lossTrack.append(batchLoss)\n",
    "            Loss /= len(dataPoints)\n",
    "            loss.append(Loss)\n",
    "            validation.append(Algorithms.evaluateNetwork(weights, biases, activate, output, valTest_x, valTest_y))\n",
    "            validationLoss.append((Algorithms.uniLoss(weights, biases, activate, output, valTest_x, valTest_y)))\n",
    "            trainAccuracy.append(Algorithms.evaluateNetwork(weights, biases, activate, output, trainx, train_y))\n",
    "            wandb.log({\"valAcc\":validation[-1], \"epoch\":epoch, \"loss\":loss[-1], \"valLoss\": validationLoss[-1], \"train Accuracy\":trainAccuracy[-1]})\n",
    "            \n",
    "        return weights, biases\n",
    "\n",
    "    @staticmethod\n",
    "    def miniBatchGD(weights, biases, batchSize, learningRate, activate, output, derivative, dataPoints, dataLabels, valTest_x, valTest_y, decay, epochs):\n",
    "        '''\n",
    "            args -> weights,biases :: The model on which loss is to be calculated\n",
    "            args -> a,h :: The preactivation and activation lists for every layer of the model.\n",
    "            args -> activate :: The activation Function to be used\n",
    "            args -> output :: usually the softmax function\n",
    "            args -> derivative :: the derivative of the function used in corresponding Forward Prop\n",
    "            args -> dataPoint :: The image upon which to BackwardProp\n",
    "            args -> dataLabel :: The label of the corresponding image\n",
    "            args -> lossFunc :: The loss function on which to operate on\n",
    "            args -> valTestx :: The validation image set\n",
    "            args -> valTesty :: The validation labels\n",
    "            args -> batchSize :: batch size \n",
    "            args -> learningRate :: the lower the better, but too low and you have no training at all.\n",
    "            args -> decay :: the amount of L2 regularization (small is good)\n",
    "            args -> epochs :: The number of epochs for which this will run \n",
    "            return -> weights,biases :: The updated model\n",
    "        '''\n",
    "        validation = []\n",
    "        validationLoss = []\n",
    "        trainAccuracy = []\n",
    "        loss = []\n",
    "        lossTrack = []\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            indices = np.arange(len(dataPoints))\n",
    "            np.random.shuffle(indices)\n",
    "            batchX = dataPoints[indices]\n",
    "            batchY = dataLabels[indices]\n",
    "            Loss = 0.0\n",
    "            for i in range(math.ceil(len(dataPoints)/batchSize)):\n",
    "                if (i+1)*batchSize < len(dataPoints):\n",
    "                    trainer = batchX[i*batchSize:i*batchSize+batchSize]\n",
    "                    labeler = batchY[i*batchSize:i*batchSize+batchSize]\n",
    "                else:\n",
    "                    trainer = batchX[i*batchSize:len(dataPoints)]\n",
    "                    labeler = batchY[i*batchSize:len(dataPoints)]\n",
    "                batchLoss = 0.0\n",
    "                dw = np.zeros_like(weights)\n",
    "                db = np.zeros_like(biases)\n",
    "                for data in range(len(trainer)):\n",
    "                    a,h = Algorithms.ForwardProp(weights, biases , activate, output, trainer[data])\n",
    "                    tempw,tempb = Algorithms.BackProp(weights, biases, a, h, derivative, trainer[data], labeler[data])\n",
    "                    dw+=tempw\n",
    "                    db+=tempb\n",
    "                    batchLoss += Functions.crossEntropyLoss(Functions.onehot(labeler[data]), h[-1])\n",
    "                Loss += batchLoss\n",
    "                batchLoss /= batchSize\n",
    "                dw /= batchSize\n",
    "                db /= batchSize\n",
    "                weights -= learningRate*(dw + decay*weights)\n",
    "                biases -= learningRate*(db + decay*biases)\n",
    "                lossTrack.append(batchLoss)\n",
    "            Loss /= len(dataPoints)\n",
    "            loss.append(Loss)\n",
    "            validation.append(Algorithms.evaluateNetwork(weights, biases, activate, output, valTest_x, valTest_y))\n",
    "            validationLoss.append((Algorithms.uniLoss(weights, biases, activate, output, valTest_x, valTest_y)))\n",
    "            trainAccuracy.append(Algorithms.evaluateNetwork(weights, biases, activate, output, trainx, train_y))\n",
    "            wandb.log({\"valAcc\":validation[-1], \"epoch\":epoch, \"loss\":loss[-1], \"valLoss\": validationLoss[-1], \"train Accuracy\":trainAccuracy[-1]})\n",
    "            \n",
    "        return weights, biases\n",
    "    \n",
    "    @staticmethod\n",
    "    def evaluateNetwork(weights, biases,activate, output, test_x, test_y):\n",
    "        num_acc = 0\n",
    "        for i in range(len(test_x)):\n",
    "            a,h = Algorithms.ForwardProp(weights, biases, activate, output, test_x[i])\n",
    "            h = np.array(h, dtype = object)\n",
    "            predY =   np.argmax(h[len(h)-1])\n",
    "            if test_y[i] == predY:\n",
    "                num_acc+=1\n",
    "        return (num_acc/len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qHfSx0jHSqtk",
    "outputId": "5e2dae53-035e-468c-98d7-d29b08d821f1"
   },
   "outputs": [],
   "source": [
    "#The class of FeedForwardNeuralNetwor\n",
    "\n",
    "class FFNet:\n",
    "    ''' The neural Network class/library, has functions crucial to implementing the neural Network\n",
    "        constructor initializes the network to adapt to the input layer size and also initializaes the output layer size\n",
    "        \n",
    "    '''\n",
    "    #constructor\n",
    "    def __init__(self,number_of_hidden_layers, number_of_inputs, number_of_outputs):\n",
    "        self.number_of_inputs = number_of_inputs\n",
    "        self.number_of_hidden_layers = number_of_hidden_layers\n",
    "        self.number_of_outputs = number_of_outputs\n",
    "        self.input = [0 for i in range(number_of_inputs)]\n",
    "        self.output = [0 for i in range(10)]\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        #self.hidden.append(np.random.random((number_of_inputs+1)))\n",
    "    \n",
    "    #Method for creating layers\n",
    "    def addHiddenLayer(self,number_of_neurons, initialization):\n",
    "        ''' args -> number_of_neurons :: The number of neurons to be added for this layer of the network\n",
    "            args -> initialization :: The type of initialization used\n",
    "            \n",
    "            return -> null\n",
    "        '''\n",
    "        if(len(self.weights) == 0):\n",
    "            temp_weights = np.random.randn(number_of_neurons, self.number_of_inputs)*0.01\n",
    "            temp_biases = np.full((number_of_neurons), 0.01)\n",
    "            if initialization == \"xavier\":\n",
    "                temp_weights = np.random.randn(number_of_neurons, self.number_of_inputs)/np.sqrt((self.number_of_inputs)/2)\n",
    "            elif initialization == \"he\":\n",
    "                temp_weights = np.random.randn(number_of_neurons, self.number_of_inputs)/np.sqrt((self.number_of_inputs)/6)\n",
    "        else:\n",
    "            prev_neurons = len(self.weights[-1])\n",
    "            temp_weights = np.random.randn(number_of_neurons, prev_neurons)*0.01\n",
    "            temp_biases = np.full((number_of_neurons), 0.01)\n",
    "            if initialization == \"xavier\":\n",
    "                temp_weights = np.random.randn(number_of_neurons, prev_neurons)/np.sqrt((prev_neurons)/2)\n",
    "            elif initialization == \"he\":\n",
    "                temp_weights = np.random.randn(number_of_neurons, prev_neurons)/np.sqrt((prev_neurons)/6)\n",
    "                \n",
    "\n",
    "        self.weights.append(temp_weights)\n",
    "        self.biases.append(temp_biases)\n",
    "    \n",
    "    def addOutputLayer(self, number_of_outputs, initialization):\n",
    "        ''' To add the output layer\n",
    "            args -> number_of_outputs :: The number of neurons in the output layer of the network\n",
    "            args -> initialization :: The type of initialization used for this network layer\n",
    "        '''\n",
    "        if(len(self.weights) == 0):\n",
    "            #print(\"number of inputs: \"+str(self.number_of_inputs))\n",
    "            temp_weights = np.random.randn(number_of_outputs, self.number_of_inputs)*0.01\n",
    "            temp_biases = np.full((number_of_outputs), 0.01)\n",
    "            if initialization == \"xavier\":\n",
    "                temp_weights = np.random.randn(number_of_outputs, self.number_of_inputs)/np.sqrt((prev_neurons)/2)\n",
    "        else:\n",
    "            prev_neurons = len(self.weights[-1])\n",
    "            temp_weights = np.random.randn(number_of_outputs, prev_neurons)*0.01\n",
    "            temp_biases = np.full((number_of_outputs), 0.01)\n",
    "            if initialization == \"xavier\":\n",
    "                temp_weights = np.random.randn(number_of_outputs, prev_neurons)/np.sqrt((prev_neurons)/2)\n",
    "                \n",
    "        \n",
    "        self.weights.append(temp_weights)\n",
    "        self.biases.append(temp_biases)\n",
    "\n",
    "    def solidify(self):\n",
    "        ''' convert the entire list into a numpy array'''\n",
    "        self.weights = np.array(self.weights, dtype = object)\n",
    "        self.biases = np.array(self.biases, dtype = object)\n",
    "\n",
    "    def getNetwork(self):\n",
    "        ''' returns the weights, biases of the network'''\n",
    "        return self.weights,self.biases\n",
    "    \n",
    "    def ForwardProp(self, activate, output, inputLayer):\n",
    "        ''' Forward Propagate the network on the given activation function, output function, and input layer'''\n",
    "        return Algorithms.ForwardProp(self.network, activate, output, inputLayer)\n",
    "    \n",
    "    def lossCalc(self, lossFunction, Y):\n",
    "        ''' calculate the loss function '''\n",
    "        predY = self.historyA[(len(self.historyA)-1)]\n",
    "        return lossFunction(Y,self.predY)\n",
    "\n",
    "    def BackProp(self, a, h, dataPoint, dataLabel):\n",
    "        return Algorithms.BackProp(self.network, a, h, dataPoint, dataLabel)\n",
    "    \n",
    "    def fit(self, optimizer, batchSize, learningRate, activation, trainx, train_y, decay, epochs):\n",
    "        '''train the network'''\n",
    "        '''\n",
    "            args -> weights,biases :: The model on which loss is to be calculated\n",
    "            args -> a,h :: The preactivation and activation lists for every layer of the model.\n",
    "            args -> activate :: The activation Function to be used\n",
    "            args -> output :: usually the softmax function\n",
    "            args -> derivative :: the derivative of the function used in corresponding Forward Prop\n",
    "            args -> dataPoint :: The image upon which to BackwardProp\n",
    "            args -> dataLabel :: The label of the corresponding image\n",
    "            args -> lossFunc :: The loss function on which to operate on\n",
    "            args -> valTestx :: The validation image set\n",
    "            args -> valTesty :: The validation labels\n",
    "            args -> batchSize :: batch size \n",
    "            args -> learningRate :: the lower the better, but too low and you have no training at all.\n",
    "            args -> decay :: the amount of L2 regularization (small is good)\n",
    "            args -> epochs :: The number of epochs for which this will run \n",
    "            return -> weights,biases :: The updated model\n",
    "        '''\n",
    "        #break data into training and validation\n",
    "        indices = np.arange(len(trainx))\n",
    "        np.random.shuffle(indices)\n",
    "        trainx = trainx[indices]\n",
    "        train_y = train_y[indices]\n",
    "        \n",
    "        valTest_x = trainx[int(0.9*len(trainx)):]\n",
    "        valTest_y = train_y[int(0.9*len(train_y)):]\n",
    "        \n",
    "        trainx = trainx[:int(0.9*len(trainx))]\n",
    "        train_y = train_y[:int(0.9*len(train_y))]\n",
    "        \n",
    "        ''' if else selector block for selecting activation function'''\n",
    "        if activation == \"relu\":\n",
    "            activate = Functions.reLU\n",
    "            derivative = Functions.derivative_reLU\n",
    "            output = Functions.softmax\n",
    "        elif activation == \"tanh\":\n",
    "            activate = Functions.tanh\n",
    "            derivative = Functions.derivative_tanh\n",
    "            output = Functions.softmax\n",
    "        elif activation == \"identity\":\n",
    "            activate = Functions.identity\n",
    "            derivative = Functions.derivative_identity\n",
    "            output = Functions.softmax\n",
    "        else:\n",
    "            activate = Functions.sigmoid\n",
    "            derivative = Functions.derivative_sigmoid\n",
    "            output = Functions.softmax\n",
    "        \n",
    "        #print(optimizer)\n",
    "        ''' if else selection block for optimizer'''\n",
    "        if optimizer == \"momentum\":\n",
    "            self.weights, self.biases = Algorithms.miniBatchMGD(self.weights,self.biases , batchSize, learningRate, activate, output, derivative,  trainx, train_y, valTest_x, valTest_y, decay, epochs)\n",
    "        elif optimizer == \"nag\":\n",
    "            self.weights, self.biases = Algorithms.miniBatchNAG(self.weights,self.biases , batchSize, learningRate,activate, output, derivative , trainx, train_y, valTest_x, valTest_y, decay, epochs)\n",
    "        elif optimizer == \"rmsprop\":\n",
    "            self.weights, self.biases = Algorithms.RMSProp(self.weights,self.biases , batchSize, learningRate, activate, output, derivative,  trainx, train_y, valTest_x, valTest_y, decay, epochs)\n",
    "        elif optimizer == \"adam\":\n",
    "            self.weights, self.biases = Algorithms.ADAM(self.weights,self.biases , batchSize, learningRate,activate, output, derivative,  trainx, train_y, valTest_x, valTest_y, decay, epochs)\n",
    "        elif optimizer == \"nadam\":\n",
    "            self.weights, self.biases = Algorithms.NADAM(self.weights,self.biases , batchSize, learningRate, activate, output, derivative,  trainx, train_y, valTest_x, valTest_y, decay, epochs)\n",
    "        else:\n",
    "            self.weights, self.biases = Algorithms.miniBatchGD(self.weights,self.biases , batchSize, learningRate, activate, output, derivative , trainx, train_y, valTest_x, valTest_y, decay, epochs)\n",
    "        print(Algorithms.evaluateNetwork(self.weights, self.biases, activate, output, testx, test_y))       \n",
    "        \n",
    "        return self.weights,self.biases\n",
    "            \n",
    "    def evaluateNetwork(self, testx, tes_ty):\n",
    "        '''to evaluate the network'''\n",
    "        Algorithms.evaluateNetwork(self.weights, self.biases, testx, test_y)        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs22m028\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: b79isutt\n",
      "Sweep URL: https://wandb.ai/cs22m028/cs6910-cs22m028FinalSet/sweeps/b79isutt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: nfxh6bsp with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchSize: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfc_layer_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitialization: xavier\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearningRate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/barenya/Documents/GitHub/CS6910Assignment1/wandb/run-20230319_173246-nfxh6bsp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs22m028/cs6910-cs22m028FinalSet/runs/nfxh6bsp' target=\"_blank\">spring-sweep-1</a></strong> to <a href='https://wandb.ai/cs22m028/cs6910-cs22m028FinalSet' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cs22m028/cs6910-cs22m028FinalSet/sweeps/b79isutt' target=\"_blank\">https://wandb.ai/cs22m028/cs6910-cs22m028FinalSet/sweeps/b79isutt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs22m028/cs6910-cs22m028FinalSet' target=\"_blank\">https://wandb.ai/cs22m028/cs6910-cs22m028FinalSet</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs22m028/cs6910-cs22m028FinalSet/sweeps/b79isutt' target=\"_blank\">https://wandb.ai/cs22m028/cs6910-cs22m028FinalSet/sweeps/b79isutt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs22m028/cs6910-cs22m028FinalSet/runs/nfxh6bsp' target=\"_blank\">https://wandb.ai/cs22m028/cs6910-cs22m028FinalSet/runs/nfxh6bsp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hm/9gm9jdm90q5fz1jnmjxxsxh40000gn/T/ipykernel_52073/2211684043.py:553: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for epoch in tqdm(range(epochs)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "389d9ac3388a490d891f202d797da74d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    data = PreProc()\n",
    "    train_x, test_x = data.flattenAndCentralize()\n",
    "    trainx = train_x/255.0\n",
    "    testx = test_x/255.0\n",
    "    train_y, test_y = data.getLabels()    \n",
    "\n",
    "import wandb\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "sweep_config = {\n",
    "    'method': 'bayes'\n",
    "    }\n",
    "\n",
    "metric = {\n",
    "    'name': 'valAcc',\n",
    "    'goal': 'maximize'   \n",
    "    }\n",
    "\n",
    "sweep_config['metric'] = metric\n",
    "\n",
    "parameters_dict = {\n",
    "    'optimizer': {\n",
    "        'values': ['nadam', 'momentum', 'nag', 'rmsprop', 'adam', 'sgd']\n",
    "        },\n",
    "    'fc_layer_size': {\n",
    "        'values': [32, 64, 128]\n",
    "        },\n",
    "    'number_of_layers': {\n",
    "        'values' : [3,4,5]\n",
    "        },\n",
    "    'epochs':{\n",
    "        'values' : [5,10]\n",
    "        },\n",
    "    'decay' : {\n",
    "        'values' : [0 ,0.0005, 0.5]\n",
    "        },\n",
    "    'learningRate' : {\n",
    "        'values' : [1e-1, 1e-3, 1e-4]\n",
    "        },\n",
    "    'batchSize' : {\n",
    "        'values' : [16, 32, 64]\n",
    "        },\n",
    "    'initialization' : {\n",
    "        'values' : ['he', 'xavier']\n",
    "        },\n",
    "    'activation' : {\n",
    "        'values' : ['sigmoid', 'tanh', 'relu']\n",
    "        },\n",
    "    }\n",
    "\n",
    "sweep_config['parameters'] = parameters_dict\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"cs6910-cs22m028FinalSet\")\n",
    "wandb.agent(sweep_id, project= \"cs6910-cs22m028FinalSet\", function = main)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
