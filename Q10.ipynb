{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.datasets import fashion_mnist\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import math\n",
    "import cv2 as cv\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    '''This function is used to log values into wandb and train the neural Network with the configuration\n",
    "        that wandb decides.\n",
    "    '''\n",
    "    #initializat wandb\n",
    "    wandb.init()\n",
    "    \n",
    "    #get current config\n",
    "    config = wandb.config\n",
    "    \n",
    "    #alter name of the wandb run to something meaningful\n",
    "    wandb.run.name = \"config_\"+ str(config.loss)\n",
    "    \n",
    "    #make a neuralNetwork from the configurations specified\n",
    "    neuralNet = FFNet(0, len(trainx[0]), 10)\n",
    "    for layer in range(config.number_of_layers):\n",
    "        neuralNet.addHiddenLayer(config.fc_layer_size, config.initialization)\n",
    "    neuralNet.addOutputLayer(10, config.initialization)\n",
    "    #compile the network\n",
    "    neuralNet.solidify()\n",
    "    \n",
    "    #train and log the values\n",
    "    weights, biases = neuralNet.fit(config.optimizer,config.batchSize, config.learningRate, config.activation, trainx, train_y, config.decay, config.epochs, config.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TfO2B6v6BVWj"
   },
   "outputs": [],
   "source": [
    "class PreProc:\n",
    "    '''Class used for preprocessing all images. \n",
    "        making a constructor of this class immediately loads in desired dataset\n",
    "        \n",
    "        visualize(n) logs into wandb 10 images each belonging to a separate class.\n",
    "        \n",
    "        flattenAndCentralize() makes the mean of the image arrays 0. This helps increasing the \n",
    "        training accuracy quicker per epoch\n",
    "        \n",
    "        getLabels() return labels in corresponding index fashion\n",
    "        \n",
    "        getInputsize returns the number of images present in the training sample\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        (self.trainx,self.trainy),(self.testx, self.testy) = fashion_mnist.load_data()\n",
    "        \n",
    "    def visualize(self,n):\n",
    "        ''' args -> n :: The number of images desired to be visualized\n",
    "            returns-> null\n",
    "            \n",
    "            shows the images via matplotlib\n",
    "        '''\n",
    "        for i in range(n):\n",
    "            plt.subplot(330+1+i) # ask someone why??\n",
    "            plt.imshow(self.trainx[i], cmap = plt.get_cmap('gray'))\n",
    "        plt.show()\n",
    "            \n",
    "        \n",
    "    def flattenAndCentralize(self):\n",
    "        ''' args -> none\n",
    "            returns -> trainx_flattened :: The training images, mean centered and flattened\n",
    "                        into a 1 dimensional array\n",
    "                    -> testx_flattened :: The testing images, mean centered and flattened\n",
    "                        into a 1 dimensional array\n",
    "        '''\n",
    "        trainx_flattened = np.copy(self.trainx).astype('float64')\n",
    "        testx_flattened = np.copy(self.testx).astype('float64')\n",
    "        trainx_flattened -= np.mean(trainx_flattened, axis = 0)\n",
    "        testx_flattened -= np.mean(testx_flattened, axis = 0)\n",
    "        for image in trainx_flattened:\n",
    "            image = cv.GaussianBlur(image,(3,3),cv.BORDER_DEFAULT)\n",
    "        trainx_flattened.shape = (len(trainx_flattened),784)\n",
    "        testx_flattened.shape = (10000,784)\n",
    "        return trainx_flattened,testx_flattened\n",
    "    \n",
    "\n",
    "    \n",
    "    def getLabels(self):\n",
    "        ''' args -> none\n",
    "            returns -> self.trainy :: The labels of the training data\n",
    "                    -> self.testy :: The labels of the testing data\n",
    "        '''\n",
    "        return self.trainy, self.testy\n",
    "    \n",
    "    def getInputSize(self):\n",
    "        return len(self.trainx[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7BwoVy0bPFA0"
   },
   "outputs": [],
   "source": [
    "class Functions:\n",
    "    ''' The Functions class/ Library stores static methods corresponding to all the functions\n",
    "        To be used in the program/training/testing.\n",
    "        The correct implementation of these is vital to the correct working of the neural net\n",
    "        model\n",
    "    '''\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(input):\n",
    "        ''' args -> input :: the input value, a numpy array type to the sigmoid function\n",
    "            return -> np.array :: the np array containing calculated sigmoid values (per input[i])\n",
    "        '''\n",
    "        input = np.clip(input, -100,100)\n",
    "        return  1.0/(1.0+np.exp(-input))\n",
    "    \n",
    "    @staticmethod\n",
    "    def reLU(input):\n",
    "        ''' args -> input :: the input value, a numpy array type to the reLU function\n",
    "            return -> np.array :: the np array containing calculated relu values (per input[i])\n",
    "        '''\n",
    "        return np.maximum(0.01*input,input)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tanh(input):\n",
    "        ''' args -> input :: the input value, a numpy array type to the tanh function\n",
    "            return -> np.array :: the np array containing calculated tanh values (per input[i])\n",
    "        '''\n",
    "        return np.tanh(input)\n",
    "    \n",
    "    @staticmethod\n",
    "    def identity(input):\n",
    "        ''' args -> input :: the input value, a numpy array type to the identity function\n",
    "            return -> np.array :: the np array containing calculated same values (per input[i])\n",
    "        '''\n",
    "        return input\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax(input):\n",
    "        ''' args -> input :: the input value, a numpy array type to the softmax function\n",
    "            return -> np.array :: the np array containing calculated softmax values (per input[i])\n",
    "        '''\n",
    "        input = np.clip(input, -100,100)\n",
    "        return np.exp(input)/(np.sum(np.exp(input)))\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative_softmax(input):\n",
    "        ''' args -> input :: the input value, a numpy array type to the derivative of the softmax function\n",
    "            return -> np.array :: the np array containing calculated derivative of softmax values (per input[i])\n",
    "        '''\n",
    "        return Functions.softmax(input)*(1-Functions.softmax(input))\n",
    "    \n",
    "    @staticmethod\n",
    "    def onehot(input):\n",
    "        \n",
    "        result = np.zeros(10)\n",
    "        result[input] = 1\n",
    "        return result\n",
    "    \n",
    "    @staticmethod\n",
    "    def crossEntropyLoss(y,yHat):\n",
    "        loss = (-1/10.0) * np.sum(np.multiply(y, np.log(yHat+1e-10)) + np.multiply((1 - y), np.log(1 - (yHat+1e-10))))\n",
    "        return loss\n",
    "    \n",
    "    @staticmethod\n",
    "    def mse(y,yHat):\n",
    "        return np.mean(np.dot((y - yHat).T, (y - yHat)))\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative_sigmoid(input):\n",
    "        ''' args -> input :: the input value, a numpy array type to the derivative of the sigmoid function\n",
    "            return -> np.array :: the np array containing calculated derivative of sigmoid values (per input[i])\n",
    "        '''\n",
    "        return Functions.sigmoid(input)*(1-Functions.sigmoid(input))\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative_tanh(input):\n",
    "        ''' args -> input :: the input value, a numpy array type to the derivative of the tanh function\n",
    "            return -> np.array :: the np array containing calculated derivative of tanh values (per input[i])\n",
    "        '''\n",
    "        return (1 - (np.tanh(input)**2))\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative_reLU(input):\n",
    "        ''' args -> input :: the input value, a numpy array type to the derivative of the reLU function\n",
    "            return -> np.array :: the np array containing calculated derivative of reLU values (per input[i])\n",
    "        '''\n",
    "        return np.where(input > 0, 1, 0.01)\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative_identity(input):\n",
    "        ''' args -> input :: the input value, a numpy array type to the derivative of the identity function\n",
    "            return -> np.array :: the np array containing calculated derivative of identity values (per input[i])\n",
    "        '''\n",
    "        return 1\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot(input):\n",
    "        ''' args -> input :: the loss list to be plotted\n",
    "            return -> null \n",
    "            Just show the matplotlib plots for the loss\n",
    "        '''\n",
    "        plt.plot(input)\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Loss over iterations\")\n",
    "        plt.show()\n",
    "    \n",
    "    @staticmethod\n",
    "    def plotAccuracy(input):\n",
    "        ''' args -> input :: the accuracy list to be plotted\n",
    "            return -> null \n",
    "            Just show the matplotlib plots for the accuracy\n",
    "        '''\n",
    "        plt.plot(input)\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"val accuracy\")\n",
    "        plt.title(\"Train over iterations\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bu5XtsgmjyaH"
   },
   "outputs": [],
   "source": [
    "class Algorithms:\n",
    "    ''' The Algorithms class/ libarary contains several functions and optimizers crucial for \n",
    "        the implementation of training and testing of the neural networks\n",
    "        \n",
    "        All these functions are static methods and therefore creation of an object instance\n",
    "        of algorithms is unnecessary\n",
    "    '''\n",
    "    @staticmethod\n",
    "    def uniLoss(weights, biases, activate, output, t1, t2, lossFunc):\n",
    "        '''\n",
    "            args -> weights,biases :: The model on which loss is to be calculated\n",
    "            args -> activate :: The activation Function to be used\n",
    "            args -> t1,t2 :: images and labels respectively\n",
    "            args -> lossFunc :: which loss function to be used for the purpose of evaluation\n",
    "            \n",
    "            return -> float :: the loss calculated on the given data by the model.\n",
    "        '''\n",
    "        Loss = 0.0\n",
    "        if lossFunc == \"mse\":\n",
    "            for index in range(len(t1)):\n",
    "                a,h = Algorithms.ForwardProp(weights, biases, activate, output, t1[index])\n",
    "                pred = h[-1]\n",
    "                true = t2[index]\n",
    "                Loss += (Functions.mse(Functions.onehot(true), pred))\n",
    "        else:\n",
    "            for index in range(len(t1)):\n",
    "                a,h = Algorithms.ForwardProp(weights, biases, activate, output, t1[index])\n",
    "                pred = h[-1]\n",
    "                true = t2[index]\n",
    "                Loss += (Functions.crossEntropyLoss(Functions.onehot(true), pred))\n",
    "        return (Loss/len(t1))\n",
    "    \n",
    "    @staticmethod\n",
    "    def ForwardProp(weights, bias, activate, output, inputLayer):\n",
    "        '''\n",
    "            args -> weights,biases :: The model on which loss is to be calculated\n",
    "            args -> activate :: The activation Function to be used\n",
    "            args -> output :: usually the softmax function\n",
    "            args -> inputLayer :: The image upon which to Forward Prop\n",
    "            \n",
    "            return -> a,h :: The preactivation and activation lists for every layer of the model.\n",
    "        '''\n",
    "        L = len(weights)-1\n",
    "        a = []\n",
    "        h = []\n",
    "        a.append(np.matmul(weights[0],inputLayer)+bias[0])\n",
    "        h.append(activate(a[0]))\n",
    "        for k in range(1,L):\n",
    "            a.append(np.matmul(weights[k],h[k-1].T)+bias[k])\n",
    "            h.append(activate(a[k]))\n",
    "        a.append(np.matmul(weights[L],h[L-1].T)+bias[L])\n",
    "        h.append(output(a[L]))\n",
    "        return a,h\n",
    "    @staticmethod\n",
    "    def BackProp(weights, biases, a, h, derivative, dataPoint, dataLabel, lossFunc):\n",
    "        '''\n",
    "            args -> weights,biases :: The model on which loss is to be calculated\n",
    "            args -> a,h :: The preactivation and activation lists for every layer of the model.\n",
    "            args -> derivative :: the derivative of the function used in corresponding Forward Prop\n",
    "            args -> dataPoint :: The image upon which to BackwardProp\n",
    "            args -> dataLabel :: The label of the corresponding image\n",
    "            args -> lossFunc :: The loss function on which to operate on\n",
    "            \n",
    "            return -> weights,biases :: The updated model\n",
    "        '''\n",
    "        L = len(weights)-1\n",
    "        if lossFunc == \"mse\":\n",
    "            gradaL = -((Functions.onehot(dataLabel) - h[-1])*(Functions.derivative_softmax(a[-1])))\n",
    "        else:\n",
    "            gradaL = -(Functions.onehot(dataLabel)-h[len(h)-1])\n",
    "        dw = np.zeros_like(weights)\n",
    "        db = np.zeros_like(biases)\n",
    "        for k in range(L,0,-1):\n",
    "            gradW = np.outer(gradaL, h[k-1].T)\n",
    "            gradB = gradaL\n",
    "            dw[k] = gradW\n",
    "            db[k] = gradB\n",
    "\n",
    "            gradhL_1 = np.matmul(np.transpose(weights[k]),gradaL)\n",
    "            gradaL_1 = np.multiply(gradhL_1, derivative(a[k-1]))\n",
    "            gradaL = gradaL_1\n",
    "        dw[0] = np.outer(gradaL,dataPoint.T)\n",
    "        db[0] = gradaL\n",
    "        return dw, db\n",
    "\n",
    "    @staticmethod\n",
    "    def miniBatchMGD(weights, biases, batchSize, learningRate, activate, output, derivative, dataPoints, dataLabels, valTest_x, valTest_y, decay, epochs, lossFunc):\n",
    "        '''\n",
    "            args -> weights,biases :: The model on which loss is to be calculated\n",
    "            args -> a,h :: The preactivation and activation lists for every layer of the model.\n",
    "            args -> activate :: The activation Function to be used\n",
    "            args -> output :: usually the softmax function\n",
    "            args -> derivative :: the derivative of the function used in corresponding Forward Prop\n",
    "            args -> dataPoint :: The image upon which to BackwardProp\n",
    "            args -> dataLabel :: The label of the corresponding image\n",
    "            args -> lossFunc :: The loss function on which to operate on\n",
    "            args -> valTestx :: The validation image set\n",
    "            args -> valTesty :: The validation labels\n",
    "            \n",
    "            return -> weights,biases :: The updated model\n",
    "        '''\n",
    "        validation = []\n",
    "        validationLoss = []\n",
    "        trainAccuracy = []\n",
    "        loss = []\n",
    "        lossTrack = []\n",
    "        LOSS = 0.0\n",
    "        beta = 0.9\n",
    "        prevWeights = np.zeros_like(weights)\n",
    "        prevBiases = np.zeros_like(biases)\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            indices = np.arange(len(dataPoints))\n",
    "            np.random.shuffle(indices)\n",
    "            batchX = dataPoints[indices]\n",
    "            batchY = dataLabels[indices]\n",
    "            Loss = 0.0\n",
    "            for i in range(math.ceil(len(dataPoints)/batchSize)):\n",
    "                if (i+1)*batchSize < len(dataPoints):\n",
    "                    trainer = batchX[i*batchSize:i*batchSize+batchSize]\n",
    "                    labeler = batchY[i*batchSize:i*batchSize+batchSize]\n",
    "                else:\n",
    "                    trainer = batchX[i*batchSize:len(dataPoints)]\n",
    "                    labeler = batchY[i*batchSize:len(dataPoints)]\n",
    "                batchLoss = 0.0\n",
    "                dw = np.zeros_like(weights)\n",
    "                db = np.zeros_like(biases)\n",
    "                for data in range(len(trainer)):\n",
    "                    a,h = Algorithms.ForwardProp(weights, biases , activate, output, trainer[data])\n",
    "                    currWeights, currBiases = Algorithms.BackProp(weights, biases, a, h, derivative, trainer[data], labeler[data], lossFunc)\n",
    "                    if lossFunc == \"mse\":\n",
    "                        batchLoss += Functions.mse(Functions.onehot(labeler[data]), h[-1])\n",
    "                    else:\n",
    "                        batchLoss += Functions.crossEntropyLoss(Functions.onehot(labeler[data]), h[-1])\n",
    "                    dw += currWeights\n",
    "                    db += currBiases\n",
    "                Loss+=batchLoss\n",
    "                batchLoss /= batchSize\n",
    "                dw /= batchSize\n",
    "                db /= batchSize\n",
    "                momentumWeights = prevWeights*beta + dw*1.0\n",
    "                momentumBiases = prevBiases*beta + db*1.0\n",
    "                weights -= learningRate*(momentumWeights + decay*weights)\n",
    "                biases -= learningRate*(momentumBiases + decay*biases)\n",
    "                prevWeights = momentumWeights\n",
    "                prevBiases = momentumBiases\n",
    "                lossTrack.append(batchLoss)\n",
    "            \n",
    "            loss.append((Loss/len(dataPoints)))\n",
    "            validation.append(Algorithms.evaluateNetwork(weights, biases, activate, output, valTest_x, valTest_y))\n",
    "            validationLoss.append((Algorithms.uniLoss(weights, biases, activate, output, valTest_x, valTest_y, lossFunc)))\n",
    "            trainAccuracy.append(Algorithms.evaluateNetwork(weights, biases, activate, output, testx, test_y))\n",
    "            wandb.log({\"Train accuracy\": trainAccuracy[-1], \"Validation Accuracy\":validation[-1], \"Validation Loss\":validationLoss[-1], \"loss\":loss[-1]})\n",
    "            #print(validationLoss)\n",
    "            \n",
    "        return weights, biases\n",
    "\n",
    "    @staticmethod\n",
    "    def ADAM(weights, biases, batchSize, learningRate, activate, output, derivative, dataPoints, dataLabels, valTest_x, valTest_y, decay, epochs, lossFunc, beta1, beta2, epsilon):\n",
    "        '''\n",
    "            args -> weights,biases :: The model on which loss is to be calculated\n",
    "            args -> a,h :: The preactivation and activation lists for every layer of the model.\n",
    "            args -> activate :: The activation Function to be used\n",
    "            args -> output :: usually the softmax function\n",
    "            args -> derivative :: the derivative of the function used in corresponding Forward Prop\n",
    "            args -> dataPoint :: The image upon which to BackwardProp\n",
    "            args -> dataLabel :: The label of the corresponding image\n",
    "            args -> lossFunc :: The loss function on which to operate on\n",
    "            args -> valTestx :: The validation image set\n",
    "            args -> valTesty :: The validation labels\n",
    "            args -> batchSize :: batch size \n",
    "            args -> learningRate :: the lower the better, but too low and you have no training at all.\n",
    "            args -> decay :: the amount of L2 regularization (small is good)\n",
    "            args -> epochs :: The number of epochs for which this will run \n",
    "            \n",
    "            updated :: \n",
    "            args -> beta1 :: value for momentum calculation\n",
    "            args -> beta2 :: value for velocity calculation\n",
    "            args -> epsilon :: small noise to avoid nans and other runtime errors/warnings\n",
    "            \n",
    "            return -> weights,biases :: The updated model\n",
    "        '''\n",
    "        validation = []\n",
    "        validationLoss = []\n",
    "        trainAccuracy = []\n",
    "        loss = []\n",
    "        lossTrack = []\n",
    "        prev_val = 0.0\n",
    "        beta1 = 0.9\n",
    "        beta2 = 0.99\n",
    "        epsilon = 0.000001\n",
    "        patience = 3\n",
    "        m_w = np.zeros_like(weights)\n",
    "        v_w = np.zeros_like(weights)\n",
    "        m_b = np.zeros_like(biases)\n",
    "        v_b = np.zeros_like(biases)\n",
    "        learnerRateW = np.full_like(weights, learningRate)\n",
    "        learnerRateB = np.full_like(biases, learningRate)\n",
    "        i = 0\n",
    "        \n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            indices = np.arange(len(dataPoints))\n",
    "            np.random.shuffle(indices)\n",
    "            batchX = dataPoints[indices]\n",
    "            batchY = dataLabels[indices]\n",
    "            Loss = 0.0\n",
    "            for i in range(math.ceil(len(dataPoints)/batchSize)):\n",
    "                if (i+1)*batchSize < len(dataPoints):\n",
    "                    trainer = batchX[i*batchSize:i*batchSize+batchSize]\n",
    "                    labeler = batchY[i*batchSize:i*batchSize+batchSize]\n",
    "                else:\n",
    "                    trainer = batchX[i*batchSize:len(dataPoints)]\n",
    "                    labeler = batchY[i*batchSize:len(dataPoints)]\n",
    "                batchLoss = 0.0\n",
    "                dw = np.zeros_like(weights)\n",
    "                db = np.zeros_like(biases)\n",
    "                for data in range(len(trainer)):\n",
    "                    a,h = Algorithms.ForwardProp(weights, biases,activate, output, trainer[data])\n",
    "                    currWeights, currBiases = Algorithms.BackProp(weights, biases , a, h, derivative, trainer[data], labeler[data], lossFunc)\n",
    "                    if lossFunc == \"mse\":\n",
    "                        batchLoss += Functions.mse(Functions.onehot(labeler[data]), h[-1])\n",
    "                    else:\n",
    "                        batchLoss += Functions.crossEntropyLoss(Functions.onehot(labeler[data]), h[-1])\n",
    "                    \n",
    "                    dw += currWeights\n",
    "                    db += currBiases\n",
    "                batchLoss /= batchSize\n",
    "                dw /= batchSize\n",
    "                db /= batchSize\n",
    "                m_w = beta1*m_w + (1-beta1)*dw\n",
    "                m_b = beta1*m_b + (1-beta1)*db\n",
    "                v_w = v_w*beta2 + (1-beta2)*dw**2\n",
    "                v_b = v_b*beta2 + (1-beta2)*db**2\n",
    "                \n",
    "                m_w_hat = m_w/(1 - np.power(beta1, i+1))\n",
    "                m_b_hat = m_b/(1 - np.power(beta1, i+1))\n",
    "                v_w_hat = v_w/(1 - np.power(beta2, i+1))\n",
    "                v_b_hat = v_b/(1 - np.power(beta2, i+1))\n",
    "                \n",
    "                i+=1\n",
    "                \n",
    "                tempW = np.zeros_like(m_w)\n",
    "                tempB = np.zeros_like(m_b)\n",
    "                for i in range(len(dw)):\n",
    "                    tempW[i] = np.sqrt(v_w_hat[i])\n",
    "                    tempB[i] = np.sqrt(v_b_hat[i])\n",
    "                weights = weights - ((learnerRateW*(dw + decay*weights))/(tempW + epsilon))\n",
    "                biases = biases - (learnerRateB*(db + decay*biases))/(tempB+epsilon)\n",
    "\n",
    "                lossTrack.append(batchLoss)\n",
    "                \n",
    "            Loss /= len(dataPoints)\n",
    "            loss.append(Loss)\n",
    "            validation.append(Algorithms.evaluateNetwork(weights, biases, activate, output, valTest_x, valTest_y))\n",
    "            validationLoss.append((Algorithms.uniLoss(weights, biases, activate, output, valTest_x, valTest_y, lossFunc)))\n",
    "            trainAccuracy.append(Algorithms.evaluateNetwork(weights, biases, activate, output, trainx, train_y))\n",
    "            wandb.log({\"Train accuracy\": trainAccuracy[-1], \"Validation Accuracy\":validation[-1], \"Validation Loss\":validationLoss[-1], \"loss\":loss[-1]})\n",
    "            #print(validationLoss[-1])\n",
    "                \n",
    "            if validation[-1] <= prev_val+1e-2:\n",
    "                patience -= 1\n",
    "            else:\n",
    "                patience = 3\n",
    "                prev_weights = weights\n",
    "                prev_biases = biases\n",
    "                prev_val = validation[-1]\n",
    "            \n",
    "            if patience == 0:\n",
    "                weights = prev_weights\n",
    "                biases = prev_biases\n",
    "            Loss /= len(dataPoints)\n",
    "            loss.append(Loss)\n",
    "            \n",
    "        if prev_val > validation[-1]:\n",
    "            return prev_weights, prev_biases\n",
    "            \n",
    "        return weights,biases\n",
    "\n",
    "    @staticmethod\n",
    "    def NADAM(weights, biases, batchSize, learningRate, activate, output, derivative, dataPoints, dataLabels, valTest_x, valTest_y, decay, epochs, lossFunc, beta1, beta2, epsilon):\n",
    "        '''\n",
    "            args -> weights,biases :: The model on which loss is to be calculated\n",
    "            args -> a,h :: The preactivation and activation lists for every layer of the model.\n",
    "            args -> activate :: The activation Function to be used\n",
    "            args -> output :: usually the softmax function\n",
    "            args -> derivative :: the derivative of the function used in corresponding Forward Prop\n",
    "            args -> dataPoint :: The image upon which to BackwardProp\n",
    "            args -> dataLabel :: The label of the corresponding image\n",
    "            args -> lossFunc :: The loss function on which to operate on\n",
    "            args -> valTestx :: The validation image set\n",
    "            args -> valTesty :: The validation labels\n",
    "            args -> batchSize :: batch size \n",
    "            args -> learningRate :: the lower the better, but too low and you have no training at all.\n",
    "            args -> decay :: the amount of L2 regularization (small is good)\n",
    "            args -> epochs :: The number of epochs for which this will run \n",
    "            \n",
    "            updated :: \n",
    "            args -> beta1 :: value for momentum calculation\n",
    "            args -> beta2 :: value for velocity calculation\n",
    "            args -> epsilon :: small noise to avoid nans and other runtime errors/warnings\n",
    "            \n",
    "            return -> weights,biases :: The updated model\n",
    "        '''\n",
    "        patience = 3\n",
    "        validation = []\n",
    "        validationLoss = []\n",
    "        trainAccuracy = []\n",
    "        loss = []\n",
    "        lossTrack = []\n",
    "        prev_val = 0.0\n",
    "        prevWeights = np.zeros_like(weights)\n",
    "        prevBiases = np.zeros_like(biases)\n",
    "        beta1 = 0.9\n",
    "        beta2 = 0.99\n",
    "        epsilon = 0.000001\n",
    "        prev_weights = np.zeros_like(weights)\n",
    "        prev_biases = np.zeros_like(biases)\n",
    "        m_w = np.zeros_like(weights)\n",
    "        v_w = np.zeros_like(weights)\n",
    "        m_b = np.zeros_like(biases)\n",
    "        v_b = np.zeros_like(biases)\n",
    "        learnerRateW = np.full_like(weights, learningRate)\n",
    "        learnerRateB = np.full_like(biases, learningRate)\n",
    "        i = 0\n",
    "        \n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            indices = np.arange(len(dataPoints))\n",
    "            np.random.shuffle(indices)\n",
    "            batchX = dataPoints[indices]\n",
    "            batchY = dataLabels[indices]\n",
    "            Loss = 0.0\n",
    "            for i in range(math.ceil(len(dataPoints)/batchSize)):\n",
    "                if (i+1)*batchSize < len(dataPoints):\n",
    "                    trainer = batchX[i*batchSize:i*batchSize+batchSize]\n",
    "                    labeler = batchY[i*batchSize:i*batchSize+batchSize]\n",
    "                else:\n",
    "                    trainer = batchX[i*batchSize:len(dataPoints)]\n",
    "                    labeler = batchY[i*batchSize:len(dataPoints)]\n",
    "                batchLoss = 0.0\n",
    "                dw = np.zeros_like(weights)\n",
    "                db = np.zeros_like(biases)\n",
    "                for data in range(len(trainer)):\n",
    "                    a,h = Algorithms.ForwardProp(weights - v_w*(beta1), biases - v_b*(beta1), activate, output, trainer[data])\n",
    "                    currWeights, currBiases = Algorithms.BackProp(weights - v_w*(beta1), biases - v_b*(beta1), a, h, derivative, trainer[data], labeler[data], lossFunc)\n",
    "                    if lossFunc == \"mse\":\n",
    "                        batchLoss += Functions.mse(Functions.onehot(labeler[data]), h[-1])\n",
    "                    else:\n",
    "                        batchLoss += Functions.crossEntropyLoss(Functions.onehot(labeler[data]), h[-1])\n",
    "                    \n",
    "                    dw += currWeights\n",
    "                    db += currBiases\n",
    "                Loss += batchLoss\n",
    "                batchLoss /= batchSize\n",
    "                dw /= batchSize\n",
    "                db /= batchSize\n",
    "                m_w = beta1*m_w + (1-beta1)*dw\n",
    "                m_b = beta1*m_b + (1-beta1)*db\n",
    "                v_w = v_w*beta2 + (1-beta2)*dw**2\n",
    "                v_b = v_b*beta2 + (1-beta2)*db**2\n",
    "                \n",
    "                m_w_hat = m_w/(1 - np.power(beta1, i+1))\n",
    "                m_b_hat = m_b/(1 - np.power(beta1, i+1))\n",
    "                v_w_hat = v_w/(1 - np.power(beta2, i+1))\n",
    "                v_b_hat = v_b/(1 - np.power(beta2, i+1))\n",
    "                \n",
    "                i+=1\n",
    "                \n",
    "                tempW = np.zeros_like(m_w)\n",
    "                tempB = np.zeros_like(m_b)\n",
    "                for j in range(len(dw)):\n",
    "                    tempW[j] = np.sqrt(v_w_hat[j])\n",
    "                    tempB[j] = np.sqrt(v_b_hat[j])\n",
    "                weights = weights - ((learnerRateW*(dw + decay*weights))/(tempW + epsilon))\n",
    "                biases = biases - (learnerRateB*(db + decay*biases))/(tempB+epsilon)\n",
    "                lossTrack.append(batchLoss)\n",
    "            validation.append(Algorithms.evaluateNetwork(weights, biases, activate, output, valTest_x, valTest_y))\n",
    "            if validation[-1] <= prev_val - 2*1e-2:\n",
    "                patience -= 1\n",
    "            else:\n",
    "                patience = 3\n",
    "                prev_weights = weights\n",
    "                prev_biases = biases\n",
    "                prev_val = validation[-1]\n",
    "            \n",
    "            Loss /= len(dataPoints)\n",
    "            print(Loss)\n",
    "            loss.append(Loss)\n",
    "            validationLoss.append((Algorithms.uniLoss(weights, biases, activate, output, valTest_x, valTest_y, lossFunc)))\n",
    "            trainAccuracy.append(Algorithms.evaluateNetwork(weights, biases, activate, output, trainx, train_y))\n",
    "            wandb.log({\"Train accuracy\": trainAccuracy[-1], \"Validation Accuracy\":validation[-1], \"Validation Loss\":validationLoss[-1], \"loss\":Loss})\n",
    "            #print(validationLoss[-1])\n",
    "            \n",
    "            if patience <= 0:\n",
    "                return prev_weights, prev_biases\n",
    "            \n",
    "        return prev_weights, prev_biases\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def miniBatchNAG(weights, biases, batchSize, learningRate, activate, output, derivative, dataPoints, dataLabels, valTest_x, valTest_y, decay, epochs, lossFunc):\n",
    "        '''\n",
    "            args -> weights,biases :: The model on which loss is to be calculated\n",
    "            args -> a,h :: The preactivation and activation lists for every layer of the model.\n",
    "            args -> activate :: The activation Function to be used\n",
    "            args -> output :: usually the softmax function\n",
    "            args -> derivative :: the derivative of the function used in corresponding Forward Prop\n",
    "            args -> dataPoint :: The image upon which to BackwardProp\n",
    "            args -> dataLabel :: The label of the corresponding image\n",
    "            args -> lossFunc :: The loss function on which to operate on\n",
    "            args -> valTestx :: The validation image set\n",
    "            args -> valTesty :: The validation labels\n",
    "            args -> batchSize :: batch size \n",
    "            args -> learningRate :: the lower the better, but too low and you have no training at all.\n",
    "            args -> decay :: the amount of L2 regularization (small is good)\n",
    "            args -> epochs :: The number of epochs for which this will run \n",
    "            return -> weights,biases :: The updated model\n",
    "        '''\n",
    "        validation = []\n",
    "        validationLoss = []\n",
    "        trainAccuracy = []\n",
    "        loss = []\n",
    "        lossTrack = []\n",
    "        beta = 0.9\n",
    "        prevWeights = np.zeros_like(weights)\n",
    "        prevBiases = np.zeros_like(biases)\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            indices = np.arange(len(dataPoints))\n",
    "            np.random.shuffle(indices)\n",
    "            batchX = dataPoints[indices]\n",
    "            batchY = dataLabels[indices]\n",
    "            Loss = 0.0\n",
    "            for i in range(math.ceil(len(dataPoints)/batchSize)):\n",
    "                if (i+1)*batchSize < len(dataPoints):\n",
    "                    trainer = batchX[i*batchSize:i*batchSize+batchSize]\n",
    "                    labeler = batchY[i*batchSize:i*batchSize+batchSize]\n",
    "                else:\n",
    "                    trainer = batchX[i*batchSize:len(dataPoints)]\n",
    "                    labeler = batchY[i*batchSize:len(dataPoints)]\n",
    "                batchLoss = 0.0\n",
    "                tempWeights = np.zeros_like(weights)\n",
    "                tempBiases = np.zeros_like(biases)\n",
    "                for data in range(len(trainer)):\n",
    "                    a,h = Algorithms.ForwardProp(weights-prevWeights, biases-prevBiases, activate, output, trainer[data])\n",
    "                    currWeights, currBiases = Algorithms.BackProp(weights-prevWeights, biases-prevBiases, a, h, derivative, trainer[data], labeler[data], lossFunc)\n",
    "                    if lossFunc == \"mse\":\n",
    "                        batchLoss += Functions.mse(Functions.onehot(labeler[data]), h[-1])\n",
    "                    else:\n",
    "                        batchLoss += Functions.crossEntropyLoss(Functions.onehot(labeler[data]), h[-1])\n",
    "                    tempWeights += currWeights\n",
    "                    tempBiases += currBiases\n",
    "                batchLoss /= batchSize\n",
    "                tempWeights /= batchSize\n",
    "                tempBiases /= batchSize\n",
    "                momentumWeights = beta*prevWeights + tempWeights*1.0\n",
    "                momentumBiases = beta*prevBiases + tempBiases*1.0\n",
    "                weights = weights - learningRate*(momentumWeights + decay*weights) \n",
    "                biases = biases - learningRate*(momentumBiases + decay*biases)\n",
    "                prevWeights = momentumWeights\n",
    "                prevBiases = momentumBiases\n",
    "                lossTrack.append(batchLoss)\n",
    "            Loss /= len(dataPoints)\n",
    "            loss.append(Loss)\n",
    "            validation.append(Algorithms.evaluateNetwork(weights, biases, activate, output, valTest_x, valTest_y))\n",
    "            validationLoss.append((Algorithms.uniLoss(weights, biases, activate, output, valTest_x, valTest_y, lossFunc)))\n",
    "            trainAccuracy.append(Algorithms.evaluateNetwork(weights, biases, activate, output, trainx, train_y))\n",
    "            wandb.log({\"Train accuracy\": trainAccuracy[-1], \"Validation Accuracy\":validation[-1], \"Validation Loss\":validationLoss[-1]})\n",
    "            #print(trainAccuracy[-1])\n",
    "            \n",
    "        return weights, biases\n",
    "    \n",
    "    @staticmethod\n",
    "    def RMSProp(weights, biases, batchSize, learningRate, activate, output, derivative, dataPoints, dataLabels, valTest_x, valTest_y, decay, epochs, lossFunc):\n",
    "        '''\n",
    "            args -> weights,biases :: The model on which loss is to be calculated\n",
    "            args -> a,h :: The preactivation and activation lists for every layer of the model.\n",
    "            args -> activate :: The activation Function to be used\n",
    "            args -> output :: usually the softmax function\n",
    "            args -> derivative :: the derivative of the function used in corresponding Forward Prop\n",
    "            args -> dataPoint :: The image upon which to BackwardProp\n",
    "            args -> dataLabel :: The label of the corresponding image\n",
    "            args -> lossFunc :: The loss function on which to operate on\n",
    "            args -> valTestx :: The validation image set\n",
    "            args -> valTesty :: The validation labels\n",
    "            args -> batchSize :: batch size \n",
    "            args -> learningRate :: the lower the better, but too low and you have no training at all.\n",
    "            args -> decay :: the amount of L2 regularization (small is good)\n",
    "            args -> epochs :: The number of epochs for which this will run \n",
    "            return -> weights,biases :: The updated model\n",
    "        '''\n",
    "        validation = []\n",
    "        validationLoss = []\n",
    "        trainAccuracy = []\n",
    "        loss = []\n",
    "        lossTrack = []\n",
    "        beta = 0.5\n",
    "        epsilon = 0.000001\n",
    "        momentumWeights = np.zeros_like(weights)\n",
    "        momentumBiases = np.zeros_like(biases)\n",
    "        learnerRateW = np.full_like(weights, learningRate)\n",
    "        learnerRateB = np.full_like(biases, learningRate)\n",
    "        \n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            indices = np.arange(len(dataPoints))\n",
    "            np.random.shuffle(indices)\n",
    "            batchX = dataPoints[indices]\n",
    "            batchY = dataLabels[indices]\n",
    "            Loss = 0.0\n",
    "            for i in range(math.ceil(len(dataPoints)/batchSize)):\n",
    "                if (i+1)*batchSize < len(dataPoints):\n",
    "                    trainer = batchX[i*batchSize:i*batchSize+batchSize]\n",
    "                    labeler = batchY[i*batchSize:i*batchSize+batchSize]\n",
    "                else:\n",
    "                    trainer = batchX[i*batchSize:len(dataPoints)]\n",
    "                    labeler = batchY[i*batchSize:len(dataPoints)]\n",
    "                batchLoss = 0.0\n",
    "                dw = np.zeros_like(weights)\n",
    "                db = np.zeros_like(biases)\n",
    "                for data in range(len(trainer)):\n",
    "                    a,h = Algorithms.ForwardProp(weights, biases, activate, output, trainer[data])\n",
    "                    currWeights, currBiases = Algorithms.BackProp(weights, biases , a, h, derivative, trainer[data], labeler[data], lossFunc)\n",
    "                    if lossFunc == \"mse\":\n",
    "                        batchLoss += Functions.mse(Functions.onehot(labeler[data]), h[-1])\n",
    "                    else:\n",
    "                        batchLoss += Functions.crossEntropyLoss(Functions.onehot(labeler[data]), h[-1])\n",
    "                    dw += currWeights\n",
    "                    db += currBiases\n",
    "                batchLoss /= batchSize\n",
    "                dw /= batchSize\n",
    "                db /= batchSize\n",
    "                momentumWeights = momentumWeights*beta + (1-beta)*dw**2\n",
    "                momentumBiases = momentumBiases*beta + (1-beta)*db**2\n",
    "                tempW = np.zeros_like(momentumWeights)\n",
    "                tempB = np.zeros_like(momentumBiases)\n",
    "                for i in range(len(dw)):\n",
    "                    tempW[i] = np.sqrt(momentumWeights[i])\n",
    "                    tempB[i] = np.sqrt(momentumBiases[i])\n",
    "                weights = weights - ((learnerRateW)*(dw + decay*weights)/(tempW + epsilon))\n",
    "                biases = biases - (learnerRateB*(db + decay*biases))/(tempB+epsilon)\n",
    "                lossTrack.append(batchLoss)\n",
    "            Loss /= len(dataPoints)\n",
    "            loss.append(Loss)\n",
    "            validation.append(Algorithms.evaluateNetwork(weights, biases, activate, output, valTest_x, valTest_y))\n",
    "            validationLoss.append((Algorithms.uniLoss(weights, biases, activate, output, valTest_x, valTest_y, lossFunc)))\n",
    "            trainAccuracy.append(Algorithms.evaluateNetwork(weights, biases, activate, output, trainx, train_y))\n",
    "            wandb.log({\"Train accuracy\": trainAccuracy[-1], \"Validation Accuracy\":validation[-1], \"Validation Loss\":validationLoss[-1]})\n",
    "            print(trainAccuracy[-1])\n",
    "        return weights, biases\n",
    "\n",
    "    @staticmethod\n",
    "    def miniBatchGD(weights, biases, batchSize, learningRate, activate, output, derivative, dataPoints, dataLabels, valTest_x, valTest_y, decay, epochs, lossFunc):\n",
    "        validation = []\n",
    "        validationLoss = []\n",
    "        trainAccuracy = []\n",
    "        loss = []\n",
    "        lossTrack = []\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            indices = np.arange(len(dataPoints))\n",
    "            np.random.shuffle(indices)\n",
    "            batchX = dataPoints[indices]\n",
    "            batchY = dataLabels[indices]\n",
    "            Loss = 0.0\n",
    "            for i in range(math.ceil(len(dataPoints)/batchSize)):\n",
    "                if (i+1)*batchSize < len(dataPoints):\n",
    "                    trainer = batchX[i*batchSize:i*batchSize+batchSize]\n",
    "                    labeler = batchY[i*batchSize:i*batchSize+batchSize]\n",
    "                else:\n",
    "                    trainer = batchX[i*batchSize:len(dataPoints)]\n",
    "                    labeler = batchY[i*batchSize:len(dataPoints)]\n",
    "                batchLoss = 0.0\n",
    "                dw = np.zeros_like(weights)\n",
    "                db = np.zeros_like(biases)\n",
    "                for data in range(len(trainer)):\n",
    "                    a,h = Algorithms.ForwardProp(weights, biases , activate, output, trainer[data])\n",
    "                    tempw,tempb = Algorithms.BackProp(weights, biases, a, h, derivative, trainer[data], labeler[data], lossFunc)\n",
    "                    dw+=tempw\n",
    "                    db+=tempb\n",
    "                    if lossFunc == \"mse\":\n",
    "                        batchLoss += Functions.mse(Functions.onehot(labeler[data]), h[-1])\n",
    "                    else:\n",
    "                        batchLoss += Functions.crossEntropyLoss(Functions.onehot(labeler[data]), h[-1])\n",
    "                batchLoss /= batchSize\n",
    "                dw /= batchSize\n",
    "                db /= batchSize\n",
    "                weights -= learningRate*(dw + decay*weights)\n",
    "                biases -= learningRate*(db + decay*biases)\n",
    "                lossTrack.append(batchLoss)\n",
    "            Loss /= len(dataPoints)\n",
    "            loss.append(Loss)\n",
    "            validation.append(Algorithms.evaluateNetwork(weights, biases, activate, output, valTest_x, valTest_y))\n",
    "            validationLoss.append((Algorithms.uniLoss(weights, biases, activate, output, valTest_x, valTest_y, lossFunc)))\n",
    "            trainAccuracy.append(Algorithms.evaluateNetwork(weights, biases, activate, output, trainx, train_y))\n",
    "            wandb.log({\"Train accuracy\": trainAccuracy[-1], \"Validation Accuracy\":validation[-1], \"Validation Loss\":validationLoss[-1]})\n",
    "            print(trainAccuracy[-1])\n",
    "        return weights, biases\n",
    "    \n",
    "    @staticmethod\n",
    "    def evaluateNetwork(weights, biases,activate, output, test_x, test_y):\n",
    "        num_acc = 0\n",
    "        for i in range(len(test_x)):\n",
    "            a,h = Algorithms.ForwardProp(weights, biases, activate, output, test_x[i])\n",
    "            h = np.array(h, dtype = object)\n",
    "            predY =   np.argmax(h[len(h)-1])\n",
    "            if test_y[i] == predY:\n",
    "                num_acc+=1\n",
    "        return (num_acc/len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qHfSx0jHSqtk",
    "outputId": "5e2dae53-035e-468c-98d7-d29b08d821f1"
   },
   "outputs": [],
   "source": [
    "#The class of FeedForwardNeuralNetwor\n",
    "\n",
    "class FFNet:\n",
    "    ''' The neural Network class/library, has functions crucial to implementing the neural Network\n",
    "        constructor initializes the network to adapt to the input layer size and also initializaes the output layer size\n",
    "        \n",
    "    '''\n",
    "    #constructor\n",
    "    def __init__(self,number_of_hidden_layers, number_of_inputs, number_of_outputs):\n",
    "        self.number_of_inputs = number_of_inputs\n",
    "        self.number_of_hidden_layers = number_of_hidden_layers\n",
    "        self.number_of_outputs = number_of_outputs\n",
    "        self.input = [0 for i in range(number_of_inputs)]\n",
    "        self.output = [0 for i in range(10)]\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "    \n",
    "    #Method for creating layers\n",
    "    def addHiddenLayer(self,number_of_neurons, initialization):\n",
    "        ''' args -> number_of_neurons :: The number of neurons to be added for this layer of the network\n",
    "            args -> initialization :: The type of initialization used\n",
    "            \n",
    "            return -> null\n",
    "        '''\n",
    "        if(len(self.weights) == 0):\n",
    "            temp_weights = np.random.randn(number_of_neurons, self.number_of_inputs)*0.01\n",
    "            temp_biases = np.full((number_of_neurons), 0.01)\n",
    "            if initialization == \"xavier\":\n",
    "                temp_weights = np.random.randn(number_of_neurons, self.number_of_inputs)/np.sqrt((self.number_of_inputs)/2)\n",
    "                #temp_biases = np.random.randn(number_of_neurons)*np.sqrt(1/(number_of_neurons))\n",
    "        else:\n",
    "            prev_neurons = len(self.weights[-1])\n",
    "            temp_weights = np.random.randn(number_of_neurons, prev_neurons)*0.01\n",
    "            temp_biases = np.full((number_of_neurons), 0.01)\n",
    "            if initialization == \"xavier\":\n",
    "                temp_weights = np.random.randn(number_of_neurons, prev_neurons)/np.sqrt((prev_neurons)/2)\n",
    "                #temp_biases = np.random.randn(number_of_neurons)*np.sqrt(1/(number_of_neurons))\n",
    "\n",
    "        self.weights.append(temp_weights)\n",
    "        self.biases.append(temp_biases)\n",
    "    \n",
    "    def addOutputLayer(self, number_of_outputs, initialization):\n",
    "        ''' To add the output layer\n",
    "            args -> number_of_outputs :: The number of neurons in the output layer of the network\n",
    "            args -> initialization :: The type of initialization used for this network layer\n",
    "        '''\n",
    "        if(len(self.weights) == 0):\n",
    "            #print(\"number of inputs: \"+str(self.number_of_inputs))\n",
    "            temp_weights = np.random.randn(number_of_outputs, self.number_of_inputs)*0.01\n",
    "            temp_biases = np.full((number_of_outputs), 0.01)\n",
    "            if initialization == \"xavier\":\n",
    "                temp_weights = np.random.randn(number_of_outputs, self.number_of_inputs)/np.sqrt((prev_neurons)/2)\n",
    "        else:\n",
    "            prev_neurons = len(self.weights[-1])\n",
    "            temp_weights = np.random.randn(number_of_outputs, prev_neurons)*0.01\n",
    "            temp_biases = np.full((number_of_outputs), 0.01)\n",
    "            if initialization == \"xavier\":\n",
    "                temp_weights = np.random.randn(number_of_outputs, prev_neurons)/np.sqrt((prev_neurons)/2)\n",
    "                \n",
    "        \n",
    "        self.weights.append(temp_weights)\n",
    "        self.biases.append(temp_biases)\n",
    "\n",
    "    def solidify(self):\n",
    "        ''' convert the entire list into a numpy array'''\n",
    "        self.weights = np.array(self.weights, dtype = object)\n",
    "        self.biases = np.array(self.biases, dtype = object)\n",
    "\n",
    "    def getNetwork(self):\n",
    "        ''' returns the weights, biases of the network'''\n",
    "        return self.weights,self.biases\n",
    "    \n",
    "    def ForwardProp(self, activate, output, inputLayer):\n",
    "        ''' Forward Propagate the network on the given activation function, output function, and input layer'''\n",
    "        return Algorithms.ForwardProp(self.network, activate, output, inputLayer)\n",
    "    \n",
    "    def lossCalc(self, lossFunction, Y):\n",
    "        ''' calulate the loss fucntion'''\n",
    "        predY = self.historyA[(len(self.historyA)-1)]\n",
    "        return lossFunction(Y,self.predY)\n",
    "\n",
    "    def BackProp(self, a, h, dataPoint, dataLabel):\n",
    "        '''call the back propagation'''\n",
    "        return Algorithms.BackProp(self.network, a, h, dataPoint, dataLabel)\n",
    "    \n",
    "    def fit(self, optimizer, batchSize, learningRate, activation, trainx, train_y, decay, epochs, lossFunc):\n",
    "        ''' the fit method basically trains the model for the given configuration'''\n",
    "        #break data into training and validation\n",
    "        indices = np.arange(len(trainx))\n",
    "        np.random.shuffle(indices)\n",
    "        trainx = trainx[indices]\n",
    "        train_y = train_y[indices]\n",
    "        \n",
    "        valTest_x = trainx[int(0.9*len(trainx)):]\n",
    "        valTest_y = train_y[int(0.9*len(train_y)):]\n",
    "        \n",
    "        trainx = trainx[:int(0.9*len(trainx))]\n",
    "        train_y = train_y[:int(0.9*len(train_y))]\n",
    "        \n",
    "        ''' the selector if else blocks to choose the activation function and output function'''\n",
    "        if activation == \"relu\":\n",
    "            activate = Functions.reLU\n",
    "            derivative = Functions.derivative_reLU\n",
    "            output = Functions.softmax\n",
    "        elif activation == \"tanh\":\n",
    "            activate = Functions.tanh\n",
    "            derivative = Functions.derivative_tanh\n",
    "            output = Functions.softmax\n",
    "        elif activation == \"identity\":\n",
    "            activate = Functions.identity\n",
    "            derivative = Functions.derivative_identity\n",
    "            output = Functions.softmax\n",
    "        else:\n",
    "            activate = Functions.sigmoid\n",
    "            derivative = Functions.derivative_sigmoid\n",
    "            output = Functions.softmax\n",
    "        \n",
    "        #print(optimizer)\n",
    "        ''' The if else block for selecting the appropriate optimizer'''\n",
    "        if optimizer == \"momentum\":\n",
    "            self.weights, self.biases = Algorithms.miniBatchMGD(self.weights,self.biases , batchSize, learningRate, activate, output, derivative,  trainx, train_y, valTest_x, valTest_y, decay, epochs, lossFunc)\n",
    "        elif optimizer == \"nag\":\n",
    "            self.weights, self.biases = Algorithms.miniBatchNAG(self.weights,self.biases , batchSize, learningRate,activate, output, derivative , trainx, train_y, valTest_x, valTest_y, decay, epochs, lossFunc)\n",
    "        elif optimizer == \"rmsprop\":\n",
    "            self.weights, self.biases = Algorithms.RMSProp(self.weights,self.biases , batchSize, learningRate, activate, output, derivative,  trainx, train_y, valTest_x, valTest_y, decay, epochs, lossFunc)\n",
    "        elif optimizer == \"adam\":\n",
    "            self.weights, self.biases = Algorithms.ADAM(self.weights,self.biases , batchSize, learningRate,activate, output, derivative,  trainx, train_y, valTest_x, valTest_y, decay, epochs, lossFunc, 0.9, 0.99, 1e-5)\n",
    "        elif optimizer == \"nadam\":\n",
    "            self.weights, self.biases = Algorithms.NADAM(self.weights,self.biases , batchSize, learningRate, activate, output, derivative,  trainx, train_y, valTest_x, valTest_y, decay, epochs, lossFunc, 0.9, 0.99, 1e-5)\n",
    "        else:\n",
    "            self.weights, self.biases = Algorithms.miniBatchGD(self.weights,self.biases , batchSize, learningRate, activate, output, derivative , trainx, train_y, valTest_x, valTest_y, decay, epochs, lossFunc)\n",
    "        print(Algorithms.evaluateNetwork(self.weights, self.biases, activate, output, testx, test_y))       \n",
    "        \n",
    "        return self.weights,self.biases\n",
    "            \n",
    "    def evaluateNetwork(self, testx, tes_ty):\n",
    "        ''' To evaluate Network on the given images and labels set.'''\n",
    "        Algorithms.evaluateNetwork(self.weights, self.biases, testx, test_y)        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "wpFxAmhE9t2C",
    "outputId": "fdd02dd5-6147-4d7a-9d86-9cabebea2f81",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    data = PreProc()\n",
    "    #data.visualize(5)\n",
    "    train_x, test_x = data.flattenAndCentralize()\n",
    "    trainx = train_x/255.0\n",
    "    testx = test_x/255.0\n",
    "    train_y, test_y = data.getLabels()\n",
    "\n",
    "    wandb.login()\n",
    "    sweep_config = {\n",
    "    'method': 'grid'\n",
    "    }\n",
    "    metric = {\n",
    "    'name': 'valAcc',\n",
    "    'goal': 'maximize'   \n",
    "    }\n",
    "    parameters_dict = {\n",
    "    'optimizer': {\n",
    "        'values': ['nadam']\n",
    "        },\n",
    "    'fc_layer_size': {\n",
    "        'values': [128]\n",
    "        },\n",
    "    'number_of_layers': {\n",
    "        'values' : [5]\n",
    "        },\n",
    "    'epochs':{\n",
    "        'values' : [10]\n",
    "        },\n",
    "    'decay' : {\n",
    "        'values' : [0.0005]\n",
    "        },\n",
    "    'learningRate' : {\n",
    "        'values' : [1e-4]\n",
    "        },\n",
    "    'batchSize' : {\n",
    "        'values' : [32]\n",
    "        },\n",
    "    'initialization' : {\n",
    "        'values' : ['xavier']\n",
    "        },\n",
    "    'activation' : {\n",
    "        'values' : ['tanh']\n",
    "        }\n",
    "}\n",
    "\n",
    "sweep_config['parameters'] = parameters_dict\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"cs6910.cs22m028.q10\")\n",
    "wandb.agent(sweep_id, train, count =1)\n",
    "\n",
    "parameters_dict = {\n",
    "    'optimizer': {\n",
    "        'values': ['nadam']\n",
    "        },\n",
    "    'fc_layer_size': {\n",
    "        'values': [128]\n",
    "        },\n",
    "    'number_of_layers': {\n",
    "        'values' : [5]\n",
    "        },\n",
    "    'epochs':{\n",
    "        'values' : [10]\n",
    "        },\n",
    "    'decay' : {\n",
    "        'values' : [0]\n",
    "        },\n",
    "    'learningRate' : {\n",
    "        'values' : [1e-4]\n",
    "        },\n",
    "    'batchSize' : {\n",
    "        'values' : [32]\n",
    "        },\n",
    "    'initialization' : {\n",
    "        'values' : ['xavier']\n",
    "        },\n",
    "    'activation' : {\n",
    "        'values' : ['relu']\n",
    "        }\n",
    "}\n",
    "\n",
    "sweep_config['parameters'] = parameters_dict\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"cs6910.cs22m028.q10\")\n",
    "wandb.agent(sweep_id, train, count =1)\n",
    "\n",
    "parameters_dict = {\n",
    "    'optimizer': {\n",
    "        'values': ['sgd']\n",
    "        },\n",
    "    'fc_layer_size': {\n",
    "        'values': [128]\n",
    "        },\n",
    "    'number_of_layers': {\n",
    "        'values' : [5]\n",
    "        },\n",
    "    'epochs':{\n",
    "        'values' : [10]\n",
    "        },\n",
    "    'decay' : {\n",
    "        'values' : [0]\n",
    "        },\n",
    "    'learningRate' : {\n",
    "        'values' : [1e-1]\n",
    "        },\n",
    "    'batchSize' : {\n",
    "        'values' : [32]\n",
    "        },\n",
    "    'initialization' : {\n",
    "        'values' : ['he']\n",
    "        },\n",
    "    'activation' : {\n",
    "        'values' : ['relu']\n",
    "        }\n",
    "}\n",
    "\n",
    "sweep_config['parameters'] = parameters_dict\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"cs6910.cs22m028.q10\")\n",
    "wandb.agent(sweep_id, train, count =1)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
