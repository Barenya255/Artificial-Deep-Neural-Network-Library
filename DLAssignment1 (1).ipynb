{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vlcWrCZGiPRM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from matplotlib import pyplot as plt\n",
        "from keras.datasets import fashion_mnist"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PreProc:\n",
        "    def __init__(self):\n",
        "        (self.trainx,self.trainy),(self.testx, self.testy) = fashion_mnist.load_data()\n",
        "    def visualize(self,n):\n",
        "        for i in range(n):\n",
        "            plt.subplot(330+1+i) # ask someone why??\n",
        "            plt.imshow(self.trainx[i], cmap = plt.get_cmap('gray'))\n",
        "        plt.show()\n",
        "    def flatten(self):\n",
        "        trainx_flattened = self.trainx\n",
        "        testx_flattened = self.testx\n",
        "        trainx_flattened.shape = (60000,784)\n",
        "        testx_flattened.shape = (10000,784)\n",
        "        return trainx_flattened,testx_flattened\n",
        "    def getLabels(self):\n",
        "        return self.trainy, self.testy\n",
        "    def getInputSize(self):\n",
        "        return len(self.trainx[0])"
      ],
      "metadata": {
        "id": "TfO2B6v6BVWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.getInputSize()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HhzRi0xhOLc",
        "outputId": "c3c442f0-9d09-4d9a-a9dd-d6be5c4c41d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(784,)"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Functions:\n",
        "    @staticmethod\n",
        "    def sigmoid(input):\n",
        "        return  1/(1+np.exp(-input))\n",
        "    @staticmethod\n",
        "    def softmax(input):\n",
        "        return np.exp(input)/(np.sum(np.exp(input)))\n",
        "    @staticmethod\n",
        "    def onehot(input):\n",
        "        result = np.zeros(10)\n",
        "        result[input] = 1\n",
        "        return result\n",
        "    @staticmethod\n",
        "    def crossEntropyLoss(y,yHat):\n",
        "        result = np.dot(y,np.log(yHat))\n",
        "        return result\n",
        "    @staticmethod\n",
        "    def derivative_sigmoid(input):\n",
        "        return ((np.exp(-input))/(1+np.exp(-input)**2))"
      ],
      "metadata": {
        "id": "7BwoVy0bPFA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Algorithms:\n",
        "    @staticmethod\n",
        "    def ForwardProp(net, activate, output, inputLayer):\n",
        "        L = len(net)-1\n",
        "        a = []\n",
        "        weights = net[0][:,:len(net[0][0])-1]\n",
        "        bias = net[0][:,len(net[0][0])-1]\n",
        "        temp = np.matmul(weights,inputLayer)+bias\n",
        "        temp = temp/np.linalg.norm(temp)\n",
        "        a.append(temp)\n",
        "        h = []\n",
        "        h.append(activate(a[0]))\n",
        "        for k in range(1,L-1):\n",
        "            weights = net[k][:,:len(net[k][0])-1]\n",
        "            bias = net[k][:,len(net[k][0])-1]\n",
        "            temp = np.matmul(weights,h[k-1])+bias\n",
        "            temp = temp/np.linalg.norm(temp)\n",
        "            a.append(bias + np.matmul(weights,h[k-1]))\n",
        "            h.append(activate(a[k]))\n",
        "        weights = net[L][:,:len(net[L][0])-1]\n",
        "        bias = net[L][:,len(net[L][0])-1]\n",
        "        temp = np.matmul(weights,h[0])+bias\n",
        "        temp = temp/np.linalg.norm(temp)\n",
        "        a.append(temp)\n",
        "        h.append(output(a[L]))\n",
        "        return a,h\n",
        "    @staticmethod\n",
        "    def BackProp(net, a, h, dataPoint):\n",
        "        L = len(net)-1\n",
        "        gradaL = -(Functions.onehot(train_y[1])-h[len(h)-1])\n",
        "        gradient = np.zeros_like(net)\n",
        "        for k in range(L,0,-1):\n",
        "            gradW = np.outer(gradaL,h[k-1].T)\n",
        "            gradB = gradaL\n",
        "            gradB.resize((len(gradB),1))\n",
        "            np.append(gradW,gradB,axis=1)\n",
        "            gradient[k] = gradW\n",
        "\n",
        "            gradhL_1 = np.matmul(np.transpose(net[k][:,len(net[k])-1]),a[k])\n",
        "            gradaL_1 = np.multiply(gradhL_1, Functions.derivative_sigmoid(a[k-1]))\n",
        "            print(net[k][:,:len(net[k][0])-1].shape)\n",
        "            gradaL = gradaL_1\n",
        "        gradW = np.outer(gradaL,dataPoint.T)\n",
        "        gradB = gradaL\n",
        "        gradB.resize((len(gradB),1))\n",
        "        np.append(gradW,gradB,axis=1)\n",
        "        gradient[0] = gradW\n",
        "        return gradient"
      ],
      "metadata": {
        "id": "Bu5XtsgmjyaH"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The class of FeedForwardNeuralNetwor\n",
        "\n",
        "class FFNet:\n",
        "    #constructor\n",
        "    def __init__(self,number_of_hidden_layers, number_of_inputs, number_of_outputs):\n",
        "        self.number_of_inputs = number_of_inputs\n",
        "        self.number_of_hidden_layers = number_of_hidden_layers\n",
        "        self.number_of_outputs = number_of_outputs\n",
        "        self.input = [0 for i in range(number_of_inputs)]\n",
        "        self.output = [0 for i in range(10)]\n",
        "        self.hidden = []\n",
        "        #self.hidden.append(np.random.random((number_of_inputs+1)))\n",
        "    \n",
        "    #Method for creating layers\n",
        "    def addHiddenLayer(self,number_of_neurons):\n",
        "        if(len(self.hidden) == 0):\n",
        "            temp_weights = np.random.random((number_of_neurons, self.number_of_inputs+1)) #The +1 is for biases.\n",
        "        else:\n",
        "            prev_neurons = len(self.hidden[len(self.hidden) - 1])\n",
        "            temp_weights = np.random.random((number_of_neurons, prev_neurons + 1)) # The +1 is for biases.\n",
        "        temp_weights = temp_weights/np.linalg.norm(temp_weights)\n",
        "        self.hidden.append(temp_weights)\n",
        "    \n",
        "    def addOutputLayer(self, number_of_outputs):\n",
        "        if(len(self.hidden) == 0):\n",
        "            #print(\"number of inputs: \"+str(self.number_of_inputs))\n",
        "            temp_weights = np.random.random((number_of_outputs, self.number_of_inputs)) #Bias not needed for the output layer.\n",
        "        else:\n",
        "            prev_neurons = len(self.hidden[len(self.hidden) - 1])\n",
        "            temp_weights = np.random.random((number_of_outputs, prev_neurons + 1)) #Bias not needed for the output layer.\n",
        "        temp_weights = temp_weights/np.linalg.norm(temp_weights)\n",
        "        self.hidden.append(temp_weights)\n",
        "\n",
        "    def solidify(self):\n",
        "        self.network = np.array(self.hidden)\n",
        "\n",
        "    def getNetwork(self):\n",
        "        return self.network\n",
        "    \n",
        "    def ForwardProp(self, activate, output, inputLayer):\n",
        "        net = self.network\n",
        "        L = len(self.network)-1\n",
        "        a = []\n",
        "        weights = net[0][:,:len(net[0][0])-1]\n",
        "        bias = net[0][:,len(net[0][0])-1]\n",
        "        temp = np.matmul(weights,inputLayer)+bias\n",
        "        temp = temp/np.linalg.norm(temp)\n",
        "        a.append(temp)\n",
        "        h = []\n",
        "        h.append(activate(a[0]))\n",
        "        for k in range(1,L-1):\n",
        "            weights = net[k][:,:len(net[k][0])-1]\n",
        "            bias = net[k][:,len(net[k][0])-1]\n",
        "            temp = np.matmul(weights,h[k-1])+bias\n",
        "            temp = temp/np.linalg.norm(temp)\n",
        "            a.append(bias + np.matmul(weights,h[k-1]))\n",
        "            h.append(activate(a[k]))\n",
        "        weights = net[L][:,:len(net[L][0])-1]\n",
        "        bias = net[L][:,len(net[L][0])-1]\n",
        "        temp = np.matmul(weights,h[0])+bias\n",
        "        temp = temp/np.linalg.norm(temp)\n",
        "        a.append(temp)\n",
        "        h.append(output(a[L]))\n",
        "        self.historyA = a\n",
        "        self.historyh = h\n",
        "        return a,h\n",
        "    \n",
        "    def loss(self, lossFunction, Y):\n",
        "        self.predY = self.historyA[(len(self.historyA)-1)]\n",
        "        lossFunction(Y,self.predY)\n",
        "\n",
        "    def BackProp(self, a, h, dataPoint):\n",
        "        net = self.network\n",
        "        L = len(net)-1\n",
        "        gradaL = -(Functions.onehot(train_y[1])-h[len(h)-1])\n",
        "        gradient = np.zeros_like(net)\n",
        "        for k in range(L,0,-1):\n",
        "            gradW = np.outer(gradaL,h[k-1].T)\n",
        "            gradB = gradaL\n",
        "            gradB.resize((len(gradB),1))\n",
        "            np.append(gradW,gradB,axis=1)\n",
        "            gradient[k] = gradW\n",
        "\n",
        "            gradhL_1 = np.matmul(np.transpose(net[k][:,len(net[k])-1]),a[k])\n",
        "            gradaL_1 = np.multiply(gradhL_1, Functions.derivative_sigmoid(a[k-1]))\n",
        "            print(net[k][:,:len(net[k][0])-1].shape)\n",
        "            gradaL = gradaL_1\n",
        "        gradW = np.outer(gradaL,dataPoint.T)\n",
        "        print(\"gradaL: \", end = \" \")\n",
        "        print(gradaL)\n",
        "        print(\"gradW: \", end = \" \")\n",
        "        print(gradW)\n",
        "        gradB = gradaL\n",
        "        gradB.resize((len(gradB),1))\n",
        "        np.append(gradW,gradB,axis=1)\n",
        "        gradient[0] = gradW\n",
        "        return gradient"
      ],
      "metadata": {
        "id": "FpxCgrhAinMc"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10,0,-1):\n",
        "    print(i, end = \" \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ew6hJr2qWgsd",
        "outputId": "f456da30-c10f-4a53-9108-f7cb10ff100b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 9 8 7 6 5 4 3 2 1 "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    data = PreProc()\n",
        "    #data.visualize(5)\n",
        "    train_x, test_x = data.flatten()\n",
        "    train_y, test_y = data.getLabels()\n",
        "    neuralNet = FFNet(0,data.getInputSize(), 10)\n",
        "    neuralNet.addHiddenLayer(20)\n",
        "    neuralNet.addOutputLayer(10)\n",
        "    neuralNet.solidify()\n",
        "    net = neuralNet.getNetwork()\n",
        "    a,h = neuralNet.ForwardProp(Functions.sigmoid, Functions.softmax, train_x[1])\n",
        "    gradient = neuralNet.BackProp(a,h, train_x[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpFxAmhE9t2C",
        "outputId": "f1482b52-5772-4800-96c6-3457d1fa9b2e"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10, 20)\n",
            "gradaL:  [0.0920129  0.09167097 0.09201342 0.09192534 0.09223669 0.0919\n",
            " 0.09186913 0.09184943 0.09193042 0.09194273 0.09194843 0.09179016\n",
            " 0.09181748 0.09185127 0.09205699 0.09172587 0.09193367 0.09194057\n",
            " 0.09189407 0.0919297 ]\n",
            "gradW:  [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-41-28e6341a5206>:35: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  self.network = np.array(self.hidden)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in gradient[0]:\n",
        "    print(i, end = \" \")"
      ],
      "metadata": {
        "id": "OY0Zy9psDtk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gradient[1] = gradW"
      ],
      "metadata": {
        "id": "I3YdLFqZU8Ci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gradaL = -(Functions.onehot(train_y[1])-h[len(h)-1])"
      ],
      "metadata": {
        "id": "kPDcvGye0lF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gradhL_1 = np.matmul(np.transpose(net[(len(net)-1)]),aL)"
      ],
      "metadata": {
        "id": "PcCtxLxzFKVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gradaL_1 = np.multiply(net[len(net)-1][:,:len(net[len(net)-1][0])-1], Functions.derivative_sigmoid(a[len(net)-2]))"
      ],
      "metadata": {
        "id": "W9xEUE6XKesg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gradW = np.outer(gradaL,h[len(net)-2].T)\n",
        "gradB = gradaL\n"
      ],
      "metadata": {
        "id": "K72jzJOiJSR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gradB.resize((len(gradB),1))"
      ],
      "metadata": {
        "id": "XDbZT58hUOWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gradB.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3-e7uWUUYd_",
        "outputId": "2dccdc5f-1f4f-4de9-ef20-bdc5f9e31910"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.append(gradW,gradB.resize((10,1)),axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdZHmwrRR2rx",
        "outputId": "df26a5dc-df87-4147-c06a-0d3df7c1d70d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.49974422, -0.49987732, -0.50045317, -0.49836884, -0.50235192,\n",
              "        -0.50235109, -0.49928474, -0.49920566, -0.49775972, -0.49974985,\n",
              "        -0.49777699, -0.50029926, -0.49888012, -0.50086685, -0.50025429,\n",
              "        -0.49987588, -0.49888532, -0.49982542, -0.50135421, -0.50049566,\n",
              "        -0.89963249],\n",
              "       [ 0.05646266,  0.0564777 ,  0.05654276,  0.05630726,  0.05675728,\n",
              "         0.05675719,  0.05641074,  0.05640181,  0.05623844,  0.05646329,\n",
              "         0.05624039,  0.05652537,  0.05636503,  0.0565895 ,  0.05652029,\n",
              "         0.05647753,  0.05636562,  0.05647183,  0.05664456,  0.05654756,\n",
              "         0.10164328],\n",
              "       [ 0.05479879,  0.05481338,  0.05487653,  0.05464797,  0.05508473,\n",
              "         0.05508464,  0.05474841,  0.05473974,  0.05458118,  0.05479941,\n",
              "         0.05458308,  0.05485965,  0.05470404,  0.05492189,  0.05485472,\n",
              "         0.05481323,  0.05470461,  0.05480769,  0.05497533,  0.05488119,\n",
              "         0.09864801],\n",
              "       [ 0.05828847,  0.058304  ,  0.05837116,  0.05812805,  0.05859263,\n",
              "         0.05859253,  0.05823488,  0.05822566,  0.05805701,  0.05828913,\n",
              "         0.05805902,  0.05835321,  0.05818769,  0.05841941,  0.05834797,\n",
              "         0.05830383,  0.0581883 ,  0.05829794,  0.05847626,  0.05837612,\n",
              "         0.10493009],\n",
              "       [ 0.05363827,  0.05365255,  0.05371436,  0.05349065,  0.05391816,\n",
              "         0.05391807,  0.05358895,  0.05358046,  0.05342527,  0.05363887,\n",
              "         0.05342712,  0.05369784,  0.05354552,  0.05375876,  0.05369302,\n",
              "         0.0536524 ,  0.05354608,  0.05364698,  0.05381107,  0.05371892,\n",
              "         0.09655885],\n",
              "       [ 0.05534094,  0.05535568,  0.05541945,  0.05518863,  0.05562971,\n",
              "         0.05562962,  0.05529006,  0.0552813 ,  0.05512118,  0.05534156,\n",
              "         0.05512309,  0.05540241,  0.05524525,  0.05546526,  0.05539743,\n",
              "         0.05535552,  0.05524583,  0.05534993,  0.05551923,  0.05542416,\n",
              "         0.09962398],\n",
              "       [ 0.05590346,  0.05591835,  0.05598276,  0.0557496 ,  0.05619517,\n",
              "         0.05619507,  0.05585206,  0.05584321,  0.05568146,  0.05590409,\n",
              "         0.05568339,  0.05596555,  0.0558068 ,  0.05602904,  0.05596052,\n",
              "         0.05591818,  0.05580738,  0.05591254,  0.05608356,  0.05598752,\n",
              "         0.10063661],\n",
              "       [ 0.05581826,  0.05583313,  0.05589745,  0.05566464,  0.05610953,\n",
              "         0.05610943,  0.05576694,  0.05575811,  0.05559661,  0.05581889,\n",
              "         0.05559854,  0.05588026,  0.05572175,  0.05594365,  0.05587524,\n",
              "         0.05583297,  0.05572233,  0.05582733,  0.05599809,  0.0559022 ,\n",
              "         0.10048325],\n",
              "       [ 0.05537143,  0.05538617,  0.05544998,  0.05521904,  0.05566036,\n",
              "         0.05566027,  0.05532052,  0.05531175,  0.05515155,  0.05537205,\n",
              "         0.05515346,  0.05543293,  0.05527569,  0.05549581,  0.05542794,\n",
              "         0.05538601,  0.05527626,  0.05538042,  0.05554981,  0.05545469,\n",
              "         0.09967886],\n",
              "       [ 0.05412194,  0.05413635,  0.05419872,  0.05397299,  0.05440435,\n",
              "         0.05440426,  0.05407218,  0.05406361,  0.05390702,  0.05412255,\n",
              "         0.05390889,  0.05418205,  0.05402836,  0.05424352,  0.05417718,\n",
              "         0.0541362 ,  0.05402892,  0.05413073,  0.0542963 ,  0.05420332,\n",
              "         0.09742955]])"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gradaL.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDIXmbKHNo25",
        "outputId": "6d44560f-8280-4146-b7bd-dedf9486be76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10,)"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a[len(net)-2].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7_71o1X06hg",
        "outputId": "5899546a-1304-47a4-e96b-f0f8edbce0ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20,)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weights = net[0][:,:len(net[0][0])-1]\n",
        "bias = net[0][:,len(net[0][0])-1]\n",
        "temp = np.matmul(weights,train_x[0])+bias\n",
        "temp = temp/np.linalg.norm(temp)\n",
        "a = []\n",
        "a.append(temp)\n",
        "h = []\n",
        "h.append(Functions.sigmoid(a[0]))"
      ],
      "metadata": {
        "id": "k65CaJIefUTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights = net[L][:,:len(net[L][0])-1]\n",
        "bias = net[L][:,len(net[L][0])-1]\n",
        "temp = np.matmul(weights,h[0])+bias\n",
        "temp = temp/np.linalg.norm(temp)"
      ],
      "metadata": {
        "id": "ZafDhQ7LmcUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "L = len(net)-1"
      ],
      "metadata": {
        "id": "epu4dvEji2mG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tg0dnhLfheDu",
        "outputId": "93e6fdc7-9cf7-4236-b55e-550c4459f101"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 20)"
            ]
          },
          "metadata": {},
          "execution_count": 222
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#The class of FeedForwardNeuralNetwor\n",
        "\n",
        "class FFNet:\n",
        "    #constructor\n",
        "    hidden = []\n",
        "    input = []\n",
        "    output = []\n",
        "    def __init__(self,number_of_hidden_layers, number_of_inputs, number_of_outputs):\n",
        "        self.number_of_inputs = number_of_inputs\n",
        "        self.number_of_hidden_layers = number_of_hidden_layers\n",
        "        self.number_of_outputs = number_of_outputs\n",
        "        #At the same time, the layers input layers mus also be initialized.\n",
        "\n",
        "        input = [0 for i in range(number_of_inputs)]\n",
        "        output = [0 for i in range(number_of_outputs)]\n",
        "        hidden = [[]]\n",
        "\n",
        "        #input and output layers are nothing but simple lists\n",
        "    \n",
        "    #Method for creating layers\n",
        "    def add_hidden_layer(number_of_neurons):\n",
        "        temp_weights = [0 for i in range(number_of_neurons+1)] #The +1 is for bias values\n",
        "        hidden.append(temp_weights)\n",
        "    \n",
        "    def backward_propagate(a,h, pred_y):\n",
        "        delthet[L] = -(exp(y) - pred_y) #with respect to output layer\n",
        "        for k in range(0,L-1,-1):\n",
        "            delthetw = np.matmul(delthet[k], h[k-1].T)\n",
        "            delthetb = delthet[k]\n",
        "            deltheth = np.matmul(weights[k].T, delthet[k])\n",
        "            delthet[k-1] = hadamard(deltheth, preac(a)) \n",
        "\n",
        "    def forward_propagate():\n",
        "        #here, we are calculating the preactivations and activations.\n",
        "        #we then store them in an array and return it.\n",
        "        \n",
        "        for k in range(number_of_levels-1):\n",
        "            a[k] = biases[k] + np.matmul(weights[k], h[k-1])\n",
        "            h[k] = g(a[k])\n",
        "        a[number_of_levels-1] = biases[number_of_levels] + np.matmul(weights[number_of_levels],h[number_of_levels-1])\n",
        "        pred_y = output(a[number_of_levels-1])\n",
        "        return a,h, pred_y\n",
        "\n",
        "\n",
        "    def gradient_descent():\n",
        "        a,h, pred_y = forward_propagate()\n",
        "        delthet = backward_propagate(a,h, pred_y)\n",
        "        thet += delthet\n",
        "\n",
        "    def fit(dataset):\n",
        "        for x,y in dataset:\n",
        "            loss = forward(x,y)\n",
        "            delthet = backward(loss)\n",
        "            thet += learn_rate*delthet\n",
        "    "
      ],
      "metadata": {
        "id": "qohv21-jxL88"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}