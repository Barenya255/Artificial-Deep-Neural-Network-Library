{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vlcWrCZGiPRM"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "x= "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#The class of FeedForwardNeuralNetwor\n",
        "\n",
        "class FFNet:\n",
        "    #constructor\n",
        "    hidden = [[]]\n",
        "    def __init__(self,number_of_hidden_layers, number_of_inputs, number_of_outputs):\n",
        "        self.number_of_inputs = number_of_inputs\n",
        "        self.number_of_hidden_layers = number_of_hidden_layers\n",
        "        self.number_of_outputs = number_of_outputs\n",
        "        #At the same time, the layers input layers mus also be initialized.\n",
        "\n",
        "        input = [0 for i in range(number_of_inputs)]\n",
        "        output = [0 for i in range(number_of_outputs)]\n",
        "        hidden = [[]]\n",
        "\n",
        "        #input and output layers are nothing but simple lists\n",
        "    \n",
        "    #Method for creating layers\n",
        "    def add_hidden_layer(number_of_neurons):\n",
        "        temp_weights = [0 for i in range(number_of_neurons+1)] #The +1 is for bias values\n",
        "        hidden.append(temp_weights)\n",
        "    \n",
        "    def backward_propagate(a,h, pred_y):\n",
        "        delthet[L] = -(exp(y) - pred_y) #with respect to output layer\n",
        "        for k in range(0,L-1,-1):\n",
        "            delthetw = np.matmul(delthet[k], h[k-1].T)\n",
        "            delthetb = delthet[k]\n",
        "            deltheth = np.matmul(weights[k].T, delthet[k])\n",
        "            delthet[k-1] = hadamard(deltheth, preac(a)) \n",
        "\n",
        "    def forward_propagate():\n",
        "        #here, we are calculating the preactivations and activations.\n",
        "        #we then store them in an array and return it.\n",
        "        \n",
        "        for k in range(number_of_levels-1):\n",
        "            a[k] = biases[k] + np.matmul(weights[k], h[k-1])\n",
        "            h[k] = g(a[k])\n",
        "        a[number_of_levels-1] = biases[number_of_levels] + np.matmul(weights[number_of_levels],h[number_of_levels-1])\n",
        "        pred_y = output(a[number_of_levels-1])\n",
        "        return a,h, pred_y\n",
        "\n",
        "\n",
        "    def backward_propagate():\n",
        "\n",
        "\n",
        "    def gradient_descent():\n",
        "        a,h, pred_y = forward_propagate()\n",
        "        delthet = backward_propagate(a,h, pred_y)\n",
        "        thet += delthet\n",
        "\n",
        "    def fit(dataset):\n",
        "        for x,y in dataset:\n",
        "            loss = forward(x,y)\n",
        "            delthet = backward(loss)\n",
        "            thet += learn_rate*delthet\n",
        "    "
      ],
      "metadata": {
        "id": "FpxCgrhAinMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qohv21-jxL88"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}